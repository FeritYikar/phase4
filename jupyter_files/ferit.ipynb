{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Metro</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>SizeRank</th>\n",
       "      <th>1996-04</th>\n",
       "      <th>1996-05</th>\n",
       "      <th>1996-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-07</th>\n",
       "      <th>2017-08</th>\n",
       "      <th>2017-09</th>\n",
       "      <th>2017-10</th>\n",
       "      <th>2017-11</th>\n",
       "      <th>2017-12</th>\n",
       "      <th>2018-01</th>\n",
       "      <th>2018-02</th>\n",
       "      <th>2018-03</th>\n",
       "      <th>2018-04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84654</td>\n",
       "      <td>60657</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>1</td>\n",
       "      <td>334200.0</td>\n",
       "      <td>335400.0</td>\n",
       "      <td>336500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1005500</td>\n",
       "      <td>1007500</td>\n",
       "      <td>1007800</td>\n",
       "      <td>1009600</td>\n",
       "      <td>1013300</td>\n",
       "      <td>1018700</td>\n",
       "      <td>1024400</td>\n",
       "      <td>1030700</td>\n",
       "      <td>1033800</td>\n",
       "      <td>1030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90668</td>\n",
       "      <td>75070</td>\n",
       "      <td>McKinney</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas-Fort Worth</td>\n",
       "      <td>Collin</td>\n",
       "      <td>2</td>\n",
       "      <td>235700.0</td>\n",
       "      <td>236900.0</td>\n",
       "      <td>236700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>308000</td>\n",
       "      <td>310000</td>\n",
       "      <td>312500</td>\n",
       "      <td>314100</td>\n",
       "      <td>315000</td>\n",
       "      <td>316600</td>\n",
       "      <td>318100</td>\n",
       "      <td>319600</td>\n",
       "      <td>321100</td>\n",
       "      <td>321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91982</td>\n",
       "      <td>77494</td>\n",
       "      <td>Katy</td>\n",
       "      <td>TX</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Harris</td>\n",
       "      <td>3</td>\n",
       "      <td>210400.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>321000</td>\n",
       "      <td>320600</td>\n",
       "      <td>320200</td>\n",
       "      <td>320400</td>\n",
       "      <td>320800</td>\n",
       "      <td>321200</td>\n",
       "      <td>321200</td>\n",
       "      <td>323000</td>\n",
       "      <td>326900</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84616</td>\n",
       "      <td>60614</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>4</td>\n",
       "      <td>498100.0</td>\n",
       "      <td>500900.0</td>\n",
       "      <td>503100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1289800</td>\n",
       "      <td>1287700</td>\n",
       "      <td>1287400</td>\n",
       "      <td>1291500</td>\n",
       "      <td>1296600</td>\n",
       "      <td>1299000</td>\n",
       "      <td>1302700</td>\n",
       "      <td>1306400</td>\n",
       "      <td>1308500</td>\n",
       "      <td>1307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93144</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>TX</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>5</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>119100</td>\n",
       "      <td>119400</td>\n",
       "      <td>120000</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120500</td>\n",
       "      <td>121000</td>\n",
       "      <td>121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14718</th>\n",
       "      <td>58333</td>\n",
       "      <td>1338</td>\n",
       "      <td>Ashfield</td>\n",
       "      <td>MA</td>\n",
       "      <td>Greenfield Town</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>14719</td>\n",
       "      <td>94600.0</td>\n",
       "      <td>94300.0</td>\n",
       "      <td>94000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>216800</td>\n",
       "      <td>217700</td>\n",
       "      <td>218600</td>\n",
       "      <td>218500</td>\n",
       "      <td>218100</td>\n",
       "      <td>216400</td>\n",
       "      <td>213100</td>\n",
       "      <td>209800</td>\n",
       "      <td>209200</td>\n",
       "      <td>209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14719</th>\n",
       "      <td>59107</td>\n",
       "      <td>3293</td>\n",
       "      <td>Woodstock</td>\n",
       "      <td>NH</td>\n",
       "      <td>Claremont</td>\n",
       "      <td>Grafton</td>\n",
       "      <td>14720</td>\n",
       "      <td>92700.0</td>\n",
       "      <td>92500.0</td>\n",
       "      <td>92400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>202100</td>\n",
       "      <td>208400</td>\n",
       "      <td>212200</td>\n",
       "      <td>215200</td>\n",
       "      <td>214300</td>\n",
       "      <td>213100</td>\n",
       "      <td>213700</td>\n",
       "      <td>218300</td>\n",
       "      <td>222700</td>\n",
       "      <td>225800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14720</th>\n",
       "      <td>75672</td>\n",
       "      <td>40404</td>\n",
       "      <td>Berea</td>\n",
       "      <td>KY</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Madison</td>\n",
       "      <td>14721</td>\n",
       "      <td>57100.0</td>\n",
       "      <td>57300.0</td>\n",
       "      <td>57500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121800</td>\n",
       "      <td>122800</td>\n",
       "      <td>124600</td>\n",
       "      <td>126700</td>\n",
       "      <td>128800</td>\n",
       "      <td>130600</td>\n",
       "      <td>131700</td>\n",
       "      <td>132500</td>\n",
       "      <td>133000</td>\n",
       "      <td>133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14721</th>\n",
       "      <td>93733</td>\n",
       "      <td>81225</td>\n",
       "      <td>Mount Crested Butte</td>\n",
       "      <td>CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gunnison</td>\n",
       "      <td>14722</td>\n",
       "      <td>191100.0</td>\n",
       "      <td>192400.0</td>\n",
       "      <td>193700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>662800</td>\n",
       "      <td>671200</td>\n",
       "      <td>682400</td>\n",
       "      <td>695600</td>\n",
       "      <td>695500</td>\n",
       "      <td>694700</td>\n",
       "      <td>706400</td>\n",
       "      <td>705300</td>\n",
       "      <td>681500</td>\n",
       "      <td>664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14722</th>\n",
       "      <td>95851</td>\n",
       "      <td>89155</td>\n",
       "      <td>Mesquite</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Clark</td>\n",
       "      <td>14723</td>\n",
       "      <td>176400.0</td>\n",
       "      <td>176300.0</td>\n",
       "      <td>176100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>333800</td>\n",
       "      <td>336400</td>\n",
       "      <td>339700</td>\n",
       "      <td>343800</td>\n",
       "      <td>346800</td>\n",
       "      <td>348900</td>\n",
       "      <td>350400</td>\n",
       "      <td>353000</td>\n",
       "      <td>356000</td>\n",
       "      <td>357200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14723 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RegionID  RegionName                 City State              Metro  \\\n",
       "0         84654       60657              Chicago    IL            Chicago   \n",
       "1         90668       75070             McKinney    TX  Dallas-Fort Worth   \n",
       "2         91982       77494                 Katy    TX            Houston   \n",
       "3         84616       60614              Chicago    IL            Chicago   \n",
       "4         93144       79936              El Paso    TX            El Paso   \n",
       "...         ...         ...                  ...   ...                ...   \n",
       "14718     58333        1338             Ashfield    MA    Greenfield Town   \n",
       "14719     59107        3293            Woodstock    NH          Claremont   \n",
       "14720     75672       40404                Berea    KY           Richmond   \n",
       "14721     93733       81225  Mount Crested Butte    CO                NaN   \n",
       "14722     95851       89155             Mesquite    NV          Las Vegas   \n",
       "\n",
       "      CountyName  SizeRank   1996-04   1996-05   1996-06  ...  2017-07  \\\n",
       "0           Cook         1  334200.0  335400.0  336500.0  ...  1005500   \n",
       "1         Collin         2  235700.0  236900.0  236700.0  ...   308000   \n",
       "2         Harris         3  210400.0  212200.0  212200.0  ...   321000   \n",
       "3           Cook         4  498100.0  500900.0  503100.0  ...  1289800   \n",
       "4        El Paso         5   77300.0   77300.0   77300.0  ...   119100   \n",
       "...          ...       ...       ...       ...       ...  ...      ...   \n",
       "14718   Franklin     14719   94600.0   94300.0   94000.0  ...   216800   \n",
       "14719    Grafton     14720   92700.0   92500.0   92400.0  ...   202100   \n",
       "14720    Madison     14721   57100.0   57300.0   57500.0  ...   121800   \n",
       "14721   Gunnison     14722  191100.0  192400.0  193700.0  ...   662800   \n",
       "14722      Clark     14723  176400.0  176300.0  176100.0  ...   333800   \n",
       "\n",
       "       2017-08  2017-09  2017-10  2017-11  2017-12  2018-01  2018-02  2018-03  \\\n",
       "0      1007500  1007800  1009600  1013300  1018700  1024400  1030700  1033800   \n",
       "1       310000   312500   314100   315000   316600   318100   319600   321100   \n",
       "2       320600   320200   320400   320800   321200   321200   323000   326900   \n",
       "3      1287700  1287400  1291500  1296600  1299000  1302700  1306400  1308500   \n",
       "4       119400   120000   120300   120300   120300   120300   120500   121000   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "14718   217700   218600   218500   218100   216400   213100   209800   209200   \n",
       "14719   208400   212200   215200   214300   213100   213700   218300   222700   \n",
       "14720   122800   124600   126700   128800   130600   131700   132500   133000   \n",
       "14721   671200   682400   695600   695500   694700   706400   705300   681500   \n",
       "14722   336400   339700   343800   346800   348900   350400   353000   356000   \n",
       "\n",
       "       2018-04  \n",
       "0      1030600  \n",
       "1       321800  \n",
       "2       329900  \n",
       "3      1307000  \n",
       "4       121500  \n",
       "...        ...  \n",
       "14718   209300  \n",
       "14719   225800  \n",
       "14720   133400  \n",
       "14721   664400  \n",
       "14722   357200  \n",
       "\n",
       "[14723 rows x 272 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/zillow_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegionID\n",
       "58196     1\n",
       "58197     1\n",
       "58200     1\n",
       "58201     1\n",
       "58202     1\n",
       "         ..\n",
       "677175    1\n",
       "698721    1\n",
       "738092    1\n",
       "753843    1\n",
       "753844    1\n",
       "Name: State, Length: 14723, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('RegionID')['State'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95804"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nv = df[df['State'] == 'NV']\n",
    "nv_zipcodes = list(df_nv.RegionID)\n",
    "nv_zipcodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>84654</th>\n",
       "      <th>90668</th>\n",
       "      <th>91982</th>\n",
       "      <th>84616</th>\n",
       "      <th>93144</th>\n",
       "      <th>91733</th>\n",
       "      <th>61807</th>\n",
       "      <th>84640</th>\n",
       "      <th>91940</th>\n",
       "      <th>97564</th>\n",
       "      <th>...</th>\n",
       "      <th>59187</th>\n",
       "      <th>94711</th>\n",
       "      <th>62556</th>\n",
       "      <th>99032</th>\n",
       "      <th>62697</th>\n",
       "      <th>58333</th>\n",
       "      <th>59107</th>\n",
       "      <th>75672</th>\n",
       "      <th>93733</th>\n",
       "      <th>95851</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-04-01</th>\n",
       "      <td>334200</td>\n",
       "      <td>235700</td>\n",
       "      <td>210400</td>\n",
       "      <td>498100</td>\n",
       "      <td>77300</td>\n",
       "      <td>95000</td>\n",
       "      <td>152900</td>\n",
       "      <td>216500</td>\n",
       "      <td>95400</td>\n",
       "      <td>766000</td>\n",
       "      <td>...</td>\n",
       "      <td>80800</td>\n",
       "      <td>135900</td>\n",
       "      <td>78300</td>\n",
       "      <td>136200</td>\n",
       "      <td>62500</td>\n",
       "      <td>94600</td>\n",
       "      <td>92700</td>\n",
       "      <td>57100</td>\n",
       "      <td>191100</td>\n",
       "      <td>176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-01</th>\n",
       "      <td>335400</td>\n",
       "      <td>236900</td>\n",
       "      <td>212200</td>\n",
       "      <td>500900</td>\n",
       "      <td>77300</td>\n",
       "      <td>95200</td>\n",
       "      <td>152700</td>\n",
       "      <td>216700</td>\n",
       "      <td>95600</td>\n",
       "      <td>771100</td>\n",
       "      <td>...</td>\n",
       "      <td>80100</td>\n",
       "      <td>136300</td>\n",
       "      <td>78300</td>\n",
       "      <td>136600</td>\n",
       "      <td>62600</td>\n",
       "      <td>94300</td>\n",
       "      <td>92500</td>\n",
       "      <td>57300</td>\n",
       "      <td>192400</td>\n",
       "      <td>176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-06-01</th>\n",
       "      <td>336500</td>\n",
       "      <td>236700</td>\n",
       "      <td>212200</td>\n",
       "      <td>503100</td>\n",
       "      <td>77300</td>\n",
       "      <td>95400</td>\n",
       "      <td>152600</td>\n",
       "      <td>216900</td>\n",
       "      <td>95800</td>\n",
       "      <td>776500</td>\n",
       "      <td>...</td>\n",
       "      <td>79400</td>\n",
       "      <td>136600</td>\n",
       "      <td>78200</td>\n",
       "      <td>136800</td>\n",
       "      <td>62700</td>\n",
       "      <td>94000</td>\n",
       "      <td>92400</td>\n",
       "      <td>57500</td>\n",
       "      <td>193700</td>\n",
       "      <td>176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-07-01</th>\n",
       "      <td>337600</td>\n",
       "      <td>235400</td>\n",
       "      <td>210700</td>\n",
       "      <td>504600</td>\n",
       "      <td>77300</td>\n",
       "      <td>95700</td>\n",
       "      <td>152400</td>\n",
       "      <td>217000</td>\n",
       "      <td>96100</td>\n",
       "      <td>781900</td>\n",
       "      <td>...</td>\n",
       "      <td>78600</td>\n",
       "      <td>136900</td>\n",
       "      <td>78200</td>\n",
       "      <td>136800</td>\n",
       "      <td>62700</td>\n",
       "      <td>93700</td>\n",
       "      <td>92200</td>\n",
       "      <td>57700</td>\n",
       "      <td>195000</td>\n",
       "      <td>176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-08-01</th>\n",
       "      <td>338500</td>\n",
       "      <td>233300</td>\n",
       "      <td>208300</td>\n",
       "      <td>505500</td>\n",
       "      <td>77400</td>\n",
       "      <td>95900</td>\n",
       "      <td>152300</td>\n",
       "      <td>217100</td>\n",
       "      <td>96400</td>\n",
       "      <td>787300</td>\n",
       "      <td>...</td>\n",
       "      <td>77900</td>\n",
       "      <td>137100</td>\n",
       "      <td>78100</td>\n",
       "      <td>136700</td>\n",
       "      <td>62700</td>\n",
       "      <td>93400</td>\n",
       "      <td>92100</td>\n",
       "      <td>58000</td>\n",
       "      <td>196300</td>\n",
       "      <td>175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1018700</td>\n",
       "      <td>316600</td>\n",
       "      <td>321200</td>\n",
       "      <td>1299000</td>\n",
       "      <td>120300</td>\n",
       "      <td>162800</td>\n",
       "      <td>414300</td>\n",
       "      <td>777900</td>\n",
       "      <td>172300</td>\n",
       "      <td>3778700</td>\n",
       "      <td>...</td>\n",
       "      <td>123400</td>\n",
       "      <td>257600</td>\n",
       "      <td>171300</td>\n",
       "      <td>341000</td>\n",
       "      <td>122800</td>\n",
       "      <td>216400</td>\n",
       "      <td>213100</td>\n",
       "      <td>130600</td>\n",
       "      <td>694700</td>\n",
       "      <td>348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1024400</td>\n",
       "      <td>318100</td>\n",
       "      <td>321200</td>\n",
       "      <td>1302700</td>\n",
       "      <td>120300</td>\n",
       "      <td>162800</td>\n",
       "      <td>413900</td>\n",
       "      <td>778500</td>\n",
       "      <td>173300</td>\n",
       "      <td>3770800</td>\n",
       "      <td>...</td>\n",
       "      <td>124400</td>\n",
       "      <td>258000</td>\n",
       "      <td>172400</td>\n",
       "      <td>342300</td>\n",
       "      <td>123200</td>\n",
       "      <td>213100</td>\n",
       "      <td>213700</td>\n",
       "      <td>131700</td>\n",
       "      <td>706400</td>\n",
       "      <td>350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>1030700</td>\n",
       "      <td>319600</td>\n",
       "      <td>323000</td>\n",
       "      <td>1306400</td>\n",
       "      <td>120500</td>\n",
       "      <td>162900</td>\n",
       "      <td>411400</td>\n",
       "      <td>780500</td>\n",
       "      <td>174200</td>\n",
       "      <td>3763100</td>\n",
       "      <td>...</td>\n",
       "      <td>125500</td>\n",
       "      <td>260600</td>\n",
       "      <td>173600</td>\n",
       "      <td>345000</td>\n",
       "      <td>123200</td>\n",
       "      <td>209800</td>\n",
       "      <td>218300</td>\n",
       "      <td>132500</td>\n",
       "      <td>705300</td>\n",
       "      <td>353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>1033800</td>\n",
       "      <td>321100</td>\n",
       "      <td>326900</td>\n",
       "      <td>1308500</td>\n",
       "      <td>121000</td>\n",
       "      <td>163500</td>\n",
       "      <td>413200</td>\n",
       "      <td>782800</td>\n",
       "      <td>175400</td>\n",
       "      <td>3779800</td>\n",
       "      <td>...</td>\n",
       "      <td>126600</td>\n",
       "      <td>264700</td>\n",
       "      <td>175800</td>\n",
       "      <td>348000</td>\n",
       "      <td>120700</td>\n",
       "      <td>209200</td>\n",
       "      <td>222700</td>\n",
       "      <td>133000</td>\n",
       "      <td>681500</td>\n",
       "      <td>356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>1030600</td>\n",
       "      <td>321800</td>\n",
       "      <td>329900</td>\n",
       "      <td>1307000</td>\n",
       "      <td>121500</td>\n",
       "      <td>164300</td>\n",
       "      <td>417900</td>\n",
       "      <td>782800</td>\n",
       "      <td>176200</td>\n",
       "      <td>3813500</td>\n",
       "      <td>...</td>\n",
       "      <td>127500</td>\n",
       "      <td>266800</td>\n",
       "      <td>177500</td>\n",
       "      <td>349300</td>\n",
       "      <td>117700</td>\n",
       "      <td>209300</td>\n",
       "      <td>225800</td>\n",
       "      <td>133400</td>\n",
       "      <td>664400</td>\n",
       "      <td>357200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 14723 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              84654   90668   91982    84616   93144   91733   61807   84640  \\\n",
       "1996-04-01   334200  235700  210400   498100   77300   95000  152900  216500   \n",
       "1996-05-01   335400  236900  212200   500900   77300   95200  152700  216700   \n",
       "1996-06-01   336500  236700  212200   503100   77300   95400  152600  216900   \n",
       "1996-07-01   337600  235400  210700   504600   77300   95700  152400  217000   \n",
       "1996-08-01   338500  233300  208300   505500   77400   95900  152300  217100   \n",
       "...             ...     ...     ...      ...     ...     ...     ...     ...   \n",
       "2017-12-01  1018700  316600  321200  1299000  120300  162800  414300  777900   \n",
       "2018-01-01  1024400  318100  321200  1302700  120300  162800  413900  778500   \n",
       "2018-02-01  1030700  319600  323000  1306400  120500  162900  411400  780500   \n",
       "2018-03-01  1033800  321100  326900  1308500  121000  163500  413200  782800   \n",
       "2018-04-01  1030600  321800  329900  1307000  121500  164300  417900  782800   \n",
       "\n",
       "             91940    97564  ...   59187   94711   62556   99032   62697  \\\n",
       "1996-04-01   95400   766000  ...   80800  135900   78300  136200   62500   \n",
       "1996-05-01   95600   771100  ...   80100  136300   78300  136600   62600   \n",
       "1996-06-01   95800   776500  ...   79400  136600   78200  136800   62700   \n",
       "1996-07-01   96100   781900  ...   78600  136900   78200  136800   62700   \n",
       "1996-08-01   96400   787300  ...   77900  137100   78100  136700   62700   \n",
       "...            ...      ...  ...     ...     ...     ...     ...     ...   \n",
       "2017-12-01  172300  3778700  ...  123400  257600  171300  341000  122800   \n",
       "2018-01-01  173300  3770800  ...  124400  258000  172400  342300  123200   \n",
       "2018-02-01  174200  3763100  ...  125500  260600  173600  345000  123200   \n",
       "2018-03-01  175400  3779800  ...  126600  264700  175800  348000  120700   \n",
       "2018-04-01  176200  3813500  ...  127500  266800  177500  349300  117700   \n",
       "\n",
       "             58333   59107   75672   93733   95851  \n",
       "1996-04-01   94600   92700   57100  191100  176400  \n",
       "1996-05-01   94300   92500   57300  192400  176300  \n",
       "1996-06-01   94000   92400   57500  193700  176100  \n",
       "1996-07-01   93700   92200   57700  195000  176000  \n",
       "1996-08-01   93400   92100   58000  196300  175900  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "2017-12-01  216400  213100  130600  694700  348900  \n",
       "2018-01-01  213100  213700  131700  706400  350400  \n",
       "2018-02-01  209800  218300  132500  705300  353000  \n",
       "2018-03-01  209200  222700  133000  681500  356000  \n",
       "2018-04-01  209300  225800  133400  664400  357200  \n",
       "\n",
       "[265 rows x 14723 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series = pd.DataFrame(index=pd.to_datetime(df.columns[7:]), data=np.ones(len(df.columns)-7))\n",
    "for i in range(df.shape[0]):\n",
    "    df_time_series[df['RegionID'][i]] = df.iloc[i,7:]\n",
    "df_time_series.drop(df_time_series.columns[0],axis=1, inplace=True)\n",
    "df_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_time_series.iloc[:,0].plot(figsize=(16,9))\n",
    "df_time_series.iloc[:,1].plot()\n",
    "df_time_series.iloc[:,2].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use p value from df test on skylar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Los Angeles    264\n",
       "Jefferson      175\n",
       "Orange         166\n",
       "Washington     164\n",
       "Montgomery     159\n",
       "Cook           140\n",
       "Harris         127\n",
       "Jackson        118\n",
       "Middlesex      117\n",
       "Maricopa       111\n",
       "Franklin       110\n",
       "Suffolk        110\n",
       "Clark           94\n",
       "Marion          92\n",
       "Monroe          90\n",
       "San Diego       87\n",
       "Madison         85\n",
       "Douglas         83\n",
       "Lake            79\n",
       "Dallas          77\n",
       "Name: CountyName, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CountyName'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Metro</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>SizeRank</th>\n",
       "      <th>1996-04</th>\n",
       "      <th>1996-05</th>\n",
       "      <th>1996-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-07</th>\n",
       "      <th>2017-08</th>\n",
       "      <th>2017-09</th>\n",
       "      <th>2017-10</th>\n",
       "      <th>2017-11</th>\n",
       "      <th>2017-12</th>\n",
       "      <th>2018-01</th>\n",
       "      <th>2018-02</th>\n",
       "      <th>2018-03</th>\n",
       "      <th>2018-04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84654</td>\n",
       "      <td>60657</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>1</td>\n",
       "      <td>334200.0</td>\n",
       "      <td>335400.0</td>\n",
       "      <td>336500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1005500</td>\n",
       "      <td>1007500</td>\n",
       "      <td>1007800</td>\n",
       "      <td>1009600</td>\n",
       "      <td>1013300</td>\n",
       "      <td>1018700</td>\n",
       "      <td>1024400</td>\n",
       "      <td>1030700</td>\n",
       "      <td>1033800</td>\n",
       "      <td>1030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90668</td>\n",
       "      <td>75070</td>\n",
       "      <td>McKinney</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas-Fort Worth</td>\n",
       "      <td>Collin</td>\n",
       "      <td>2</td>\n",
       "      <td>235700.0</td>\n",
       "      <td>236900.0</td>\n",
       "      <td>236700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>308000</td>\n",
       "      <td>310000</td>\n",
       "      <td>312500</td>\n",
       "      <td>314100</td>\n",
       "      <td>315000</td>\n",
       "      <td>316600</td>\n",
       "      <td>318100</td>\n",
       "      <td>319600</td>\n",
       "      <td>321100</td>\n",
       "      <td>321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91982</td>\n",
       "      <td>77494</td>\n",
       "      <td>Katy</td>\n",
       "      <td>TX</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Harris</td>\n",
       "      <td>3</td>\n",
       "      <td>210400.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>321000</td>\n",
       "      <td>320600</td>\n",
       "      <td>320200</td>\n",
       "      <td>320400</td>\n",
       "      <td>320800</td>\n",
       "      <td>321200</td>\n",
       "      <td>321200</td>\n",
       "      <td>323000</td>\n",
       "      <td>326900</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84616</td>\n",
       "      <td>60614</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>4</td>\n",
       "      <td>498100.0</td>\n",
       "      <td>500900.0</td>\n",
       "      <td>503100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1289800</td>\n",
       "      <td>1287700</td>\n",
       "      <td>1287400</td>\n",
       "      <td>1291500</td>\n",
       "      <td>1296600</td>\n",
       "      <td>1299000</td>\n",
       "      <td>1302700</td>\n",
       "      <td>1306400</td>\n",
       "      <td>1308500</td>\n",
       "      <td>1307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93144</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>TX</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>5</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>119100</td>\n",
       "      <td>119400</td>\n",
       "      <td>120000</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120500</td>\n",
       "      <td>121000</td>\n",
       "      <td>121500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RegionID  RegionName      City State              Metro CountyName  \\\n",
       "0     84654       60657   Chicago    IL            Chicago       Cook   \n",
       "1     90668       75070  McKinney    TX  Dallas-Fort Worth     Collin   \n",
       "2     91982       77494      Katy    TX            Houston     Harris   \n",
       "3     84616       60614   Chicago    IL            Chicago       Cook   \n",
       "4     93144       79936   El Paso    TX            El Paso    El Paso   \n",
       "\n",
       "   SizeRank   1996-04   1996-05   1996-06  ...  2017-07  2017-08  2017-09  \\\n",
       "0         1  334200.0  335400.0  336500.0  ...  1005500  1007500  1007800   \n",
       "1         2  235700.0  236900.0  236700.0  ...   308000   310000   312500   \n",
       "2         3  210400.0  212200.0  212200.0  ...   321000   320600   320200   \n",
       "3         4  498100.0  500900.0  503100.0  ...  1289800  1287700  1287400   \n",
       "4         5   77300.0   77300.0   77300.0  ...   119100   119400   120000   \n",
       "\n",
       "   2017-10  2017-11  2017-12  2018-01  2018-02  2018-03  2018-04  \n",
       "0  1009600  1013300  1018700  1024400  1030700  1033800  1030600  \n",
       "1   314100   315000   316600   318100   319600   321100   321800  \n",
       "2   320400   320800   321200   321200   323000   326900   329900  \n",
       "3  1291500  1296600  1299000  1302700  1306400  1308500  1307000  \n",
       "4   120300   120300   120300   120300   120500   121000   121500  \n",
       "\n",
       "[5 rows x 272 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/zillow_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>84654</th>\n",
       "      <th>90668</th>\n",
       "      <th>91982</th>\n",
       "      <th>84616</th>\n",
       "      <th>93144</th>\n",
       "      <th>91733</th>\n",
       "      <th>61807</th>\n",
       "      <th>84640</th>\n",
       "      <th>91940</th>\n",
       "      <th>97564</th>\n",
       "      <th>...</th>\n",
       "      <th>59187</th>\n",
       "      <th>94711</th>\n",
       "      <th>62556</th>\n",
       "      <th>99032</th>\n",
       "      <th>62697</th>\n",
       "      <th>58333</th>\n",
       "      <th>59107</th>\n",
       "      <th>75672</th>\n",
       "      <th>93733</th>\n",
       "      <th>95851</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-04-01</th>\n",
       "      <td>334200</td>\n",
       "      <td>235700</td>\n",
       "      <td>210400</td>\n",
       "      <td>498100</td>\n",
       "      <td>77300</td>\n",
       "      <td>95000</td>\n",
       "      <td>152900</td>\n",
       "      <td>216500</td>\n",
       "      <td>95400</td>\n",
       "      <td>766000</td>\n",
       "      <td>...</td>\n",
       "      <td>80800</td>\n",
       "      <td>135900</td>\n",
       "      <td>78300</td>\n",
       "      <td>136200</td>\n",
       "      <td>62500</td>\n",
       "      <td>94600</td>\n",
       "      <td>92700</td>\n",
       "      <td>57100</td>\n",
       "      <td>191100</td>\n",
       "      <td>176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-01</th>\n",
       "      <td>335400</td>\n",
       "      <td>236900</td>\n",
       "      <td>212200</td>\n",
       "      <td>500900</td>\n",
       "      <td>77300</td>\n",
       "      <td>95200</td>\n",
       "      <td>152700</td>\n",
       "      <td>216700</td>\n",
       "      <td>95600</td>\n",
       "      <td>771100</td>\n",
       "      <td>...</td>\n",
       "      <td>80100</td>\n",
       "      <td>136300</td>\n",
       "      <td>78300</td>\n",
       "      <td>136600</td>\n",
       "      <td>62600</td>\n",
       "      <td>94300</td>\n",
       "      <td>92500</td>\n",
       "      <td>57300</td>\n",
       "      <td>192400</td>\n",
       "      <td>176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-06-01</th>\n",
       "      <td>336500</td>\n",
       "      <td>236700</td>\n",
       "      <td>212200</td>\n",
       "      <td>503100</td>\n",
       "      <td>77300</td>\n",
       "      <td>95400</td>\n",
       "      <td>152600</td>\n",
       "      <td>216900</td>\n",
       "      <td>95800</td>\n",
       "      <td>776500</td>\n",
       "      <td>...</td>\n",
       "      <td>79400</td>\n",
       "      <td>136600</td>\n",
       "      <td>78200</td>\n",
       "      <td>136800</td>\n",
       "      <td>62700</td>\n",
       "      <td>94000</td>\n",
       "      <td>92400</td>\n",
       "      <td>57500</td>\n",
       "      <td>193700</td>\n",
       "      <td>176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-07-01</th>\n",
       "      <td>337600</td>\n",
       "      <td>235400</td>\n",
       "      <td>210700</td>\n",
       "      <td>504600</td>\n",
       "      <td>77300</td>\n",
       "      <td>95700</td>\n",
       "      <td>152400</td>\n",
       "      <td>217000</td>\n",
       "      <td>96100</td>\n",
       "      <td>781900</td>\n",
       "      <td>...</td>\n",
       "      <td>78600</td>\n",
       "      <td>136900</td>\n",
       "      <td>78200</td>\n",
       "      <td>136800</td>\n",
       "      <td>62700</td>\n",
       "      <td>93700</td>\n",
       "      <td>92200</td>\n",
       "      <td>57700</td>\n",
       "      <td>195000</td>\n",
       "      <td>176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-08-01</th>\n",
       "      <td>338500</td>\n",
       "      <td>233300</td>\n",
       "      <td>208300</td>\n",
       "      <td>505500</td>\n",
       "      <td>77400</td>\n",
       "      <td>95900</td>\n",
       "      <td>152300</td>\n",
       "      <td>217100</td>\n",
       "      <td>96400</td>\n",
       "      <td>787300</td>\n",
       "      <td>...</td>\n",
       "      <td>77900</td>\n",
       "      <td>137100</td>\n",
       "      <td>78100</td>\n",
       "      <td>136700</td>\n",
       "      <td>62700</td>\n",
       "      <td>93400</td>\n",
       "      <td>92100</td>\n",
       "      <td>58000</td>\n",
       "      <td>196300</td>\n",
       "      <td>175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1018700</td>\n",
       "      <td>316600</td>\n",
       "      <td>321200</td>\n",
       "      <td>1299000</td>\n",
       "      <td>120300</td>\n",
       "      <td>162800</td>\n",
       "      <td>414300</td>\n",
       "      <td>777900</td>\n",
       "      <td>172300</td>\n",
       "      <td>3778700</td>\n",
       "      <td>...</td>\n",
       "      <td>123400</td>\n",
       "      <td>257600</td>\n",
       "      <td>171300</td>\n",
       "      <td>341000</td>\n",
       "      <td>122800</td>\n",
       "      <td>216400</td>\n",
       "      <td>213100</td>\n",
       "      <td>130600</td>\n",
       "      <td>694700</td>\n",
       "      <td>348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1024400</td>\n",
       "      <td>318100</td>\n",
       "      <td>321200</td>\n",
       "      <td>1302700</td>\n",
       "      <td>120300</td>\n",
       "      <td>162800</td>\n",
       "      <td>413900</td>\n",
       "      <td>778500</td>\n",
       "      <td>173300</td>\n",
       "      <td>3770800</td>\n",
       "      <td>...</td>\n",
       "      <td>124400</td>\n",
       "      <td>258000</td>\n",
       "      <td>172400</td>\n",
       "      <td>342300</td>\n",
       "      <td>123200</td>\n",
       "      <td>213100</td>\n",
       "      <td>213700</td>\n",
       "      <td>131700</td>\n",
       "      <td>706400</td>\n",
       "      <td>350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>1030700</td>\n",
       "      <td>319600</td>\n",
       "      <td>323000</td>\n",
       "      <td>1306400</td>\n",
       "      <td>120500</td>\n",
       "      <td>162900</td>\n",
       "      <td>411400</td>\n",
       "      <td>780500</td>\n",
       "      <td>174200</td>\n",
       "      <td>3763100</td>\n",
       "      <td>...</td>\n",
       "      <td>125500</td>\n",
       "      <td>260600</td>\n",
       "      <td>173600</td>\n",
       "      <td>345000</td>\n",
       "      <td>123200</td>\n",
       "      <td>209800</td>\n",
       "      <td>218300</td>\n",
       "      <td>132500</td>\n",
       "      <td>705300</td>\n",
       "      <td>353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>1033800</td>\n",
       "      <td>321100</td>\n",
       "      <td>326900</td>\n",
       "      <td>1308500</td>\n",
       "      <td>121000</td>\n",
       "      <td>163500</td>\n",
       "      <td>413200</td>\n",
       "      <td>782800</td>\n",
       "      <td>175400</td>\n",
       "      <td>3779800</td>\n",
       "      <td>...</td>\n",
       "      <td>126600</td>\n",
       "      <td>264700</td>\n",
       "      <td>175800</td>\n",
       "      <td>348000</td>\n",
       "      <td>120700</td>\n",
       "      <td>209200</td>\n",
       "      <td>222700</td>\n",
       "      <td>133000</td>\n",
       "      <td>681500</td>\n",
       "      <td>356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>1030600</td>\n",
       "      <td>321800</td>\n",
       "      <td>329900</td>\n",
       "      <td>1307000</td>\n",
       "      <td>121500</td>\n",
       "      <td>164300</td>\n",
       "      <td>417900</td>\n",
       "      <td>782800</td>\n",
       "      <td>176200</td>\n",
       "      <td>3813500</td>\n",
       "      <td>...</td>\n",
       "      <td>127500</td>\n",
       "      <td>266800</td>\n",
       "      <td>177500</td>\n",
       "      <td>349300</td>\n",
       "      <td>117700</td>\n",
       "      <td>209300</td>\n",
       "      <td>225800</td>\n",
       "      <td>133400</td>\n",
       "      <td>664400</td>\n",
       "      <td>357200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 14723 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              84654   90668   91982    84616   93144   91733   61807   84640  \\\n",
       "1996-04-01   334200  235700  210400   498100   77300   95000  152900  216500   \n",
       "1996-05-01   335400  236900  212200   500900   77300   95200  152700  216700   \n",
       "1996-06-01   336500  236700  212200   503100   77300   95400  152600  216900   \n",
       "1996-07-01   337600  235400  210700   504600   77300   95700  152400  217000   \n",
       "1996-08-01   338500  233300  208300   505500   77400   95900  152300  217100   \n",
       "...             ...     ...     ...      ...     ...     ...     ...     ...   \n",
       "2017-12-01  1018700  316600  321200  1299000  120300  162800  414300  777900   \n",
       "2018-01-01  1024400  318100  321200  1302700  120300  162800  413900  778500   \n",
       "2018-02-01  1030700  319600  323000  1306400  120500  162900  411400  780500   \n",
       "2018-03-01  1033800  321100  326900  1308500  121000  163500  413200  782800   \n",
       "2018-04-01  1030600  321800  329900  1307000  121500  164300  417900  782800   \n",
       "\n",
       "             91940    97564  ...   59187   94711   62556   99032   62697  \\\n",
       "1996-04-01   95400   766000  ...   80800  135900   78300  136200   62500   \n",
       "1996-05-01   95600   771100  ...   80100  136300   78300  136600   62600   \n",
       "1996-06-01   95800   776500  ...   79400  136600   78200  136800   62700   \n",
       "1996-07-01   96100   781900  ...   78600  136900   78200  136800   62700   \n",
       "1996-08-01   96400   787300  ...   77900  137100   78100  136700   62700   \n",
       "...            ...      ...  ...     ...     ...     ...     ...     ...   \n",
       "2017-12-01  172300  3778700  ...  123400  257600  171300  341000  122800   \n",
       "2018-01-01  173300  3770800  ...  124400  258000  172400  342300  123200   \n",
       "2018-02-01  174200  3763100  ...  125500  260600  173600  345000  123200   \n",
       "2018-03-01  175400  3779800  ...  126600  264700  175800  348000  120700   \n",
       "2018-04-01  176200  3813500  ...  127500  266800  177500  349300  117700   \n",
       "\n",
       "             58333   59107   75672   93733   95851  \n",
       "1996-04-01   94600   92700   57100  191100  176400  \n",
       "1996-05-01   94300   92500   57300  192400  176300  \n",
       "1996-06-01   94000   92400   57500  193700  176100  \n",
       "1996-07-01   93700   92200   57700  195000  176000  \n",
       "1996-08-01   93400   92100   58000  196300  175900  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "2017-12-01  216400  213100  130600  694700  348900  \n",
       "2018-01-01  213100  213700  131700  706400  350400  \n",
       "2018-02-01  209800  218300  132500  705300  353000  \n",
       "2018-03-01  209200  222700  133000  681500  356000  \n",
       "2018-04-01  209300  225800  133400  664400  357200  \n",
       "\n",
       "[265 rows x 14723 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series = pd.DataFrame(index=pd.to_datetime(df.columns[7:]), data=np.ones(len(df.columns)-7))\n",
    "for i in range(df.shape[0]):\n",
    "    df_time_series[df['RegionID'][i]] = df.iloc[i,7:]\n",
    "df_time_series.drop(df_time_series.columns[0],axis=1, inplace=True)\n",
    "df_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>84654</th>\n",
       "      <th>90668</th>\n",
       "      <th>91982</th>\n",
       "      <th>84616</th>\n",
       "      <th>93144</th>\n",
       "      <th>91733</th>\n",
       "      <th>61807</th>\n",
       "      <th>84640</th>\n",
       "      <th>91940</th>\n",
       "      <th>97564</th>\n",
       "      <th>...</th>\n",
       "      <th>59187</th>\n",
       "      <th>94711</th>\n",
       "      <th>62556</th>\n",
       "      <th>99032</th>\n",
       "      <th>62697</th>\n",
       "      <th>58333</th>\n",
       "      <th>59107</th>\n",
       "      <th>75672</th>\n",
       "      <th>93733</th>\n",
       "      <th>95851</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-04-01</th>\n",
       "      <td>334200.0</td>\n",
       "      <td>235700.0</td>\n",
       "      <td>210400.0</td>\n",
       "      <td>498100.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>152900.0</td>\n",
       "      <td>216500.0</td>\n",
       "      <td>95400.0</td>\n",
       "      <td>766000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80800.0</td>\n",
       "      <td>135900.0</td>\n",
       "      <td>78300.0</td>\n",
       "      <td>136200.0</td>\n",
       "      <td>62500.0</td>\n",
       "      <td>94600.0</td>\n",
       "      <td>92700.0</td>\n",
       "      <td>57100.0</td>\n",
       "      <td>191100.0</td>\n",
       "      <td>176400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-01</th>\n",
       "      <td>335400.0</td>\n",
       "      <td>236900.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>500900.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>95200.0</td>\n",
       "      <td>152700.0</td>\n",
       "      <td>216700.0</td>\n",
       "      <td>95600.0</td>\n",
       "      <td>771100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80100.0</td>\n",
       "      <td>136300.0</td>\n",
       "      <td>78300.0</td>\n",
       "      <td>136600.0</td>\n",
       "      <td>62600.0</td>\n",
       "      <td>94300.0</td>\n",
       "      <td>92500.0</td>\n",
       "      <td>57300.0</td>\n",
       "      <td>192400.0</td>\n",
       "      <td>176300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-06-01</th>\n",
       "      <td>336500.0</td>\n",
       "      <td>236700.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>503100.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>95400.0</td>\n",
       "      <td>152600.0</td>\n",
       "      <td>216900.0</td>\n",
       "      <td>95800.0</td>\n",
       "      <td>776500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79400.0</td>\n",
       "      <td>136600.0</td>\n",
       "      <td>78200.0</td>\n",
       "      <td>136800.0</td>\n",
       "      <td>62700.0</td>\n",
       "      <td>94000.0</td>\n",
       "      <td>92400.0</td>\n",
       "      <td>57500.0</td>\n",
       "      <td>193700.0</td>\n",
       "      <td>176100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-07-01</th>\n",
       "      <td>337600.0</td>\n",
       "      <td>235400.0</td>\n",
       "      <td>210700.0</td>\n",
       "      <td>504600.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>95700.0</td>\n",
       "      <td>152400.0</td>\n",
       "      <td>217000.0</td>\n",
       "      <td>96100.0</td>\n",
       "      <td>781900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78600.0</td>\n",
       "      <td>136900.0</td>\n",
       "      <td>78200.0</td>\n",
       "      <td>136800.0</td>\n",
       "      <td>62700.0</td>\n",
       "      <td>93700.0</td>\n",
       "      <td>92200.0</td>\n",
       "      <td>57700.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>176000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-08-01</th>\n",
       "      <td>338500.0</td>\n",
       "      <td>233300.0</td>\n",
       "      <td>208300.0</td>\n",
       "      <td>505500.0</td>\n",
       "      <td>77400.0</td>\n",
       "      <td>95900.0</td>\n",
       "      <td>152300.0</td>\n",
       "      <td>217100.0</td>\n",
       "      <td>96400.0</td>\n",
       "      <td>787300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77900.0</td>\n",
       "      <td>137100.0</td>\n",
       "      <td>78100.0</td>\n",
       "      <td>136700.0</td>\n",
       "      <td>62700.0</td>\n",
       "      <td>93400.0</td>\n",
       "      <td>92100.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>196300.0</td>\n",
       "      <td>175900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1018700.0</td>\n",
       "      <td>316600.0</td>\n",
       "      <td>321200.0</td>\n",
       "      <td>1299000.0</td>\n",
       "      <td>120300.0</td>\n",
       "      <td>162800.0</td>\n",
       "      <td>414300.0</td>\n",
       "      <td>777900.0</td>\n",
       "      <td>172300.0</td>\n",
       "      <td>3778700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>123400.0</td>\n",
       "      <td>257600.0</td>\n",
       "      <td>171300.0</td>\n",
       "      <td>341000.0</td>\n",
       "      <td>122800.0</td>\n",
       "      <td>216400.0</td>\n",
       "      <td>213100.0</td>\n",
       "      <td>130600.0</td>\n",
       "      <td>694700.0</td>\n",
       "      <td>348900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1024400.0</td>\n",
       "      <td>318100.0</td>\n",
       "      <td>321200.0</td>\n",
       "      <td>1302700.0</td>\n",
       "      <td>120300.0</td>\n",
       "      <td>162800.0</td>\n",
       "      <td>413900.0</td>\n",
       "      <td>778500.0</td>\n",
       "      <td>173300.0</td>\n",
       "      <td>3770800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>124400.0</td>\n",
       "      <td>258000.0</td>\n",
       "      <td>172400.0</td>\n",
       "      <td>342300.0</td>\n",
       "      <td>123200.0</td>\n",
       "      <td>213100.0</td>\n",
       "      <td>213700.0</td>\n",
       "      <td>131700.0</td>\n",
       "      <td>706400.0</td>\n",
       "      <td>350400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>1030700.0</td>\n",
       "      <td>319600.0</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>1306400.0</td>\n",
       "      <td>120500.0</td>\n",
       "      <td>162900.0</td>\n",
       "      <td>411400.0</td>\n",
       "      <td>780500.0</td>\n",
       "      <td>174200.0</td>\n",
       "      <td>3763100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>125500.0</td>\n",
       "      <td>260600.0</td>\n",
       "      <td>173600.0</td>\n",
       "      <td>345000.0</td>\n",
       "      <td>123200.0</td>\n",
       "      <td>209800.0</td>\n",
       "      <td>218300.0</td>\n",
       "      <td>132500.0</td>\n",
       "      <td>705300.0</td>\n",
       "      <td>353000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>1033800.0</td>\n",
       "      <td>321100.0</td>\n",
       "      <td>326900.0</td>\n",
       "      <td>1308500.0</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>163500.0</td>\n",
       "      <td>413200.0</td>\n",
       "      <td>782800.0</td>\n",
       "      <td>175400.0</td>\n",
       "      <td>3779800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>126600.0</td>\n",
       "      <td>264700.0</td>\n",
       "      <td>175800.0</td>\n",
       "      <td>348000.0</td>\n",
       "      <td>120700.0</td>\n",
       "      <td>209200.0</td>\n",
       "      <td>222700.0</td>\n",
       "      <td>133000.0</td>\n",
       "      <td>681500.0</td>\n",
       "      <td>356000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>1030600.0</td>\n",
       "      <td>321800.0</td>\n",
       "      <td>329900.0</td>\n",
       "      <td>1307000.0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>164300.0</td>\n",
       "      <td>417900.0</td>\n",
       "      <td>782800.0</td>\n",
       "      <td>176200.0</td>\n",
       "      <td>3813500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>127500.0</td>\n",
       "      <td>266800.0</td>\n",
       "      <td>177500.0</td>\n",
       "      <td>349300.0</td>\n",
       "      <td>117700.0</td>\n",
       "      <td>209300.0</td>\n",
       "      <td>225800.0</td>\n",
       "      <td>133400.0</td>\n",
       "      <td>664400.0</td>\n",
       "      <td>357200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 14723 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                84654     90668     91982      84616     93144     91733  \\\n",
       "1996-04-01   334200.0  235700.0  210400.0   498100.0   77300.0   95000.0   \n",
       "1996-05-01   335400.0  236900.0  212200.0   500900.0   77300.0   95200.0   \n",
       "1996-06-01   336500.0  236700.0  212200.0   503100.0   77300.0   95400.0   \n",
       "1996-07-01   337600.0  235400.0  210700.0   504600.0   77300.0   95700.0   \n",
       "1996-08-01   338500.0  233300.0  208300.0   505500.0   77400.0   95900.0   \n",
       "...               ...       ...       ...        ...       ...       ...   \n",
       "2017-12-01  1018700.0  316600.0  321200.0  1299000.0  120300.0  162800.0   \n",
       "2018-01-01  1024400.0  318100.0  321200.0  1302700.0  120300.0  162800.0   \n",
       "2018-02-01  1030700.0  319600.0  323000.0  1306400.0  120500.0  162900.0   \n",
       "2018-03-01  1033800.0  321100.0  326900.0  1308500.0  121000.0  163500.0   \n",
       "2018-04-01  1030600.0  321800.0  329900.0  1307000.0  121500.0  164300.0   \n",
       "\n",
       "               61807     84640     91940      97564  ...     59187     94711  \\\n",
       "1996-04-01  152900.0  216500.0   95400.0   766000.0  ...   80800.0  135900.0   \n",
       "1996-05-01  152700.0  216700.0   95600.0   771100.0  ...   80100.0  136300.0   \n",
       "1996-06-01  152600.0  216900.0   95800.0   776500.0  ...   79400.0  136600.0   \n",
       "1996-07-01  152400.0  217000.0   96100.0   781900.0  ...   78600.0  136900.0   \n",
       "1996-08-01  152300.0  217100.0   96400.0   787300.0  ...   77900.0  137100.0   \n",
       "...              ...       ...       ...        ...  ...       ...       ...   \n",
       "2017-12-01  414300.0  777900.0  172300.0  3778700.0  ...  123400.0  257600.0   \n",
       "2018-01-01  413900.0  778500.0  173300.0  3770800.0  ...  124400.0  258000.0   \n",
       "2018-02-01  411400.0  780500.0  174200.0  3763100.0  ...  125500.0  260600.0   \n",
       "2018-03-01  413200.0  782800.0  175400.0  3779800.0  ...  126600.0  264700.0   \n",
       "2018-04-01  417900.0  782800.0  176200.0  3813500.0  ...  127500.0  266800.0   \n",
       "\n",
       "               62556     99032     62697     58333     59107     75672  \\\n",
       "1996-04-01   78300.0  136200.0   62500.0   94600.0   92700.0   57100.0   \n",
       "1996-05-01   78300.0  136600.0   62600.0   94300.0   92500.0   57300.0   \n",
       "1996-06-01   78200.0  136800.0   62700.0   94000.0   92400.0   57500.0   \n",
       "1996-07-01   78200.0  136800.0   62700.0   93700.0   92200.0   57700.0   \n",
       "1996-08-01   78100.0  136700.0   62700.0   93400.0   92100.0   58000.0   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-12-01  171300.0  341000.0  122800.0  216400.0  213100.0  130600.0   \n",
       "2018-01-01  172400.0  342300.0  123200.0  213100.0  213700.0  131700.0   \n",
       "2018-02-01  173600.0  345000.0  123200.0  209800.0  218300.0  132500.0   \n",
       "2018-03-01  175800.0  348000.0  120700.0  209200.0  222700.0  133000.0   \n",
       "2018-04-01  177500.0  349300.0  117700.0  209300.0  225800.0  133400.0   \n",
       "\n",
       "               93733     95851  \n",
       "1996-04-01  191100.0  176400.0  \n",
       "1996-05-01  192400.0  176300.0  \n",
       "1996-06-01  193700.0  176100.0  \n",
       "1996-07-01  195000.0  176000.0  \n",
       "1996-08-01  196300.0  175900.0  \n",
       "...              ...       ...  \n",
       "2017-12-01  694700.0  348900.0  \n",
       "2018-01-01  706400.0  350400.0  \n",
       "2018-02-01  705300.0  353000.0  \n",
       "2018-03-01  681500.0  356000.0  \n",
       "2018-04-01  664400.0  357200.0  \n",
       "\n",
       "[265 rows x 14723 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996-04-01    131200\n",
       "1996-05-01    131000\n",
       "1996-06-01    130800\n",
       "1996-07-01    130600\n",
       "1996-08-01    130400\n",
       "               ...  \n",
       "2017-12-01    178400\n",
       "2018-01-01    179400\n",
       "2018-02-01    179800\n",
       "2018-03-01    180500\n",
       "2018-04-01    181200\n",
       "Name: 95768, Length: 265, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series[95768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(df_time_series)*.8)\n",
    "train = df_time_series.iloc[:size]\n",
    "test = df_time_series.iloc[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[nv_zipcodes]\n",
    "test = test[nv_zipcodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "train_data = train.iloc[:,x:x+1].values.astype(int)\n",
    "test_data = test.iloc[:,x:x+1].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "test_data_scaled = scaler.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset with 60 timesteps (5 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60,len(train_data_scaled)):\n",
    "    X_train.append(train_data_scaled[i-60:i])\n",
    "    y_train.append(train_data_scaled[i])\n",
    "\n",
    "data_total = pd.concat((train.iloc[:,x:x+1], test.iloc[:,x:x+1]),axis=0)\n",
    "inputs = data_total[len(train)-60:].values\n",
    "inputs = scaler.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(60,len(inputs)):\n",
    "    X_test.append(inputs[i-60:i])\n",
    "    y_test.append(inputs[i])\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn data into arrays for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-bfe1b689c08a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    204\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[1;32m    926\u001b[0m                                                 input_list)\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1115\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m               \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# LSTM does not support constants. Ignore it during process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m       \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m       init_state = get_initial_state_fn(\n\u001b[0m\u001b[1;32m    642\u001b[0m           inputs=None, batch_size=batch_size, dtype=dtype)\n\u001b[1;32m    643\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m     return list(_generate_zero_filled_state_for_cell(\n\u001b[0m\u001b[1;32m   2520\u001b[0m         self, inputs, batch_size, dtype))\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m   2962\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2964\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_generate_zero_filled_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_generate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m   2975\u001b[0m     \u001b[0mflat_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2976\u001b[0m     \u001b[0minit_state_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2977\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2979\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2746\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2747\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2748\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2749\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   2792\u001b[0m           \u001b[0;31m# Create a constant if it won't be very big. Otherwise create a fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2793\u001b[0m           \u001b[0;31m# op to prevent serialized GraphDefs from becoming too large.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2794\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2795\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2796\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_constant_if_small\u001b[0;34m(value, shape, dtype, name)\u001b[0m\n\u001b[1;32m   2730\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_if_small\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2731\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2732\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2733\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3086\u001b[0m     \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m     \"\"\"\n\u001b[0;32m-> 3088\u001b[0;31m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0m\u001b[1;32m   3089\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (lstm/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
     ]
    }
   ],
   "source": [
    "rnn_model.add(LSTM(units= 50, return_sequences = True, input_shape=(X_train.shape[1:])))\n",
    "rnn_model.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.add(LSTM(units= 50, return_sequences = True))\n",
    "rnn_model.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.add(LSTM(units= 50, return_sequences = True))\n",
    "rnn_model.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.add(LSTM(units= 50, return_sequences = False))\n",
    "rnn_model.add(Dropout(.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.add(Dense(units=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 60, 50)            10400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 71,051\n",
      "Trainable params: 71,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.4187\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0823\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0310\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0397\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0179\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0214\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0149\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0170\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0144\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0129\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0136\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0150\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0112\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0151\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0147\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0115\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.0125\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0112\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0125\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0106\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0127\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0122\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0094\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0115\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0108\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0095\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0111\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0102\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0111\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0116\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0129\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.0102\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0104\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0098\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0096\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0103\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0113\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0097\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0095\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0091\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0089\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0106\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0090\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0123\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0103\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0090\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0097\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0085\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0087\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0094\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0085\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0087\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0105\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0084\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0083\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0089\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0083\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0106\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0080\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0079\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0085\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0067\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0069\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0069\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0061\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.0064\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.0075\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0068\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0071\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0076\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.0063\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0072\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.0062\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.0053\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 62ms/step - loss: 0.0052\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0065\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.0048\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0065\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.0050\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.0054\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 0.0060\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.0050\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 0.0063\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 0.0056\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.0054\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 0.0052\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0047\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.0054\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0059\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.0054\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.0056\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.0059\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.0062\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.0047\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0052\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.0057\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0051\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0046\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.0051\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.0049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff9caff8cd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(X_train, y_train, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the 20% test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_raw = rnn_model.predict(X_test)\n",
    "y_hat = scaler.inverse_transform(y_hat_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test, color='red', label='Real Prices')\n",
    "plt.plot(y_hat, color='blue', label='Predicted Prices')\n",
    "plt.title('Unseen Data Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes = df_time_series.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156891"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series.fillna(method='bfill',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nv_zipcodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dd955c7509fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzipcodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnv_zipcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdict_mape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdict_next_5_years\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mzipcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipcodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nv_zipcodes' is not defined"
     ]
    }
   ],
   "source": [
    "zipcodes = nv_zipcodes\n",
    "dict_mape = {}\n",
    "dict_next_5_years = {}\n",
    "\n",
    "for zipcode in range(len(zipcodes)):\n",
    "\n",
    "    # init a RMM model\n",
    "    rnn_model = Sequential()\n",
    "    # add 4 layers of RNN and a last layer\n",
    "\n",
    "    # we define shape on first layer, (60,1) because we use 60 inputs per prediction\n",
    "    rnn_model.add(LSTM(units= 30, return_sequences = True, input_shape=((60,1))))\n",
    "    rnn_model.add(Dropout(.2))\n",
    "\n",
    "    # 3 other layers\n",
    "    rnn_model.add(LSTM(units= 30, return_sequences = True))\n",
    "    rnn_model.add(Dropout(.2))\n",
    "\n",
    "    rnn_model.add(LSTM(units= 30, return_sequences = True))\n",
    "    rnn_model.add(Dropout(.2))\n",
    "\n",
    "    # return_sequence is False because we want only 1 output after this layer\n",
    "    rnn_model.add(LSTM(units= 30, return_sequences = False))\n",
    "    rnn_model.add(Dropout(.2))\n",
    "\n",
    "    # last layer \n",
    "\n",
    "    rnn_model.add(Dense(units=1))\n",
    "\n",
    "    # compile - because this is a regression model we want to minimize MSE\n",
    "\n",
    "    rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # We get only the specific column(Zipcode from our train and test datas)\n",
    "    train_data = train.iloc[:,zipcode:zipcode+1].values.astype(int)\n",
    "    test_data = test.iloc[:,zipcode:zipcode+1].values.astype(int)\n",
    "    \n",
    "    # We are using normalizaion rather than standascaler. \n",
    "    # In a upward trending timeseries it is better to not start from negative\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    train_data_scaled = scaler.fit_transform(train_data)\n",
    "    test_data_scaled = scaler.fit_transform(test_data)\n",
    "\n",
    "    # Because we are using 60 previous values to model and predict the next value, \n",
    "    # We set X_train from arrays of 60 for each y_train value\n",
    "    # Same idea for test data sets\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(60,len(train_data_scaled)):\n",
    "        X_train.append(train_data_scaled[i-60:i])\n",
    "        y_train.append(train_data_scaled[i])\n",
    "\n",
    "    data_total = pd.concat((train.iloc[:,zipcode:zipcode+1], test.iloc[:,zipcode:zipcode+1]),axis=0)\n",
    "    inputs = data_total[len(train)-60:].values\n",
    "    inputs = scaler.transform(inputs)\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(60,len(inputs)):\n",
    "        X_test.append(inputs[i-60:i])\n",
    "        y_test.append(inputs[i])\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(test_data)\n",
    "\n",
    "    # We need numpy arrays for our model\n",
    "    X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "    \n",
    "    # We fit our data to our zipcode specific data\n",
    "    \n",
    "    rnn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test,y_test))\n",
    "\n",
    "    # Make predictions on the data\n",
    "\n",
    "    y_hat_raw = rnn_model.predict(X_test)\n",
    "    y_hat = scaler.inverse_transform(y_hat_raw)\n",
    "\n",
    "    # Use the score on unseen test data to calculate the MAPE\n",
    "\n",
    "    dict_mape[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_test)/y_test))      \n",
    "\n",
    "    # We get the last 60 values from our test data which is basically last 60 values in the data set\n",
    "    last_60 = df_time_series.iloc[-60:,zipcode:zipcode+1].values.astype(int)\n",
    "    \n",
    "    # Before we use our data we scale it\n",
    "    last_60 = scaler.transform(last_60)\n",
    "    \n",
    "\n",
    "    predictions = []\n",
    "    # We will make next 60 predictions\n",
    "    for i in range(24):\n",
    "        # Our input should be in (x,60,1) format\n",
    "        x_new_pred = last_60[-60:].reshape(1,60,1)\n",
    "\n",
    "        # make a prediction, add to the last_60 for the next prediction and \n",
    "        y_pred = rnn_model.predict(x_new_pred)\n",
    "\n",
    "        # We add our predition to our list of predictions for zipcode specific predictions list\n",
    "        predictions.append(scaler.inverse_transform(y_pred))\n",
    "\n",
    "        last_60 = np.append(last_60,y_pred)\n",
    "        \n",
    "\n",
    "    dict_next_5_years[zipcodes[zipcode]] = predictions\n",
    "    print(f'Iteration number {zipcode} finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{95804: 0.05467682170463134,\n",
       " 95817: 0.056657457760375556,\n",
       " 95813: 0.10627834888752286,\n",
       " 95785: 0.08986851430738302,\n",
       " 95819: 0.08810381854497155,\n",
       " 95770: 0.057699379562738316,\n",
       " 95806: 0.052305123321462935,\n",
       " 95790: 0.09322087751259556,\n",
       " 95799: 0.05321495274014127,\n",
       " 95844: 0.08012690582607666,\n",
       " 95843: 0.0657379767293326,\n",
       " 95815: 0.04273625591032392,\n",
       " 95825: 0.04603331400112482,\n",
       " 95818: 0.057636989368447376,\n",
       " 95811: 0.0762378069082009,\n",
       " 95931: 0.07170479750360981,\n",
       " 95753: 0.06274389413222704,\n",
       " 95827: 0.11008221868752441,\n",
       " 95937: 0.12199265887721046,\n",
       " 95914: 0.032062208924157336,\n",
       " 95754: 0.037946811621694485,\n",
       " 95824: 0.0618866628751203,\n",
       " 95945: 0.07254765729332553,\n",
       " 95800: 0.08874680092995095,\n",
       " 95751: 0.10093614013515913,\n",
       " 95769: 0.5018527073139741,\n",
       " 95909: 0.0674096424695563,\n",
       " 95771: 0.047990223463525675,\n",
       " 95935: 0.05061276701446624,\n",
       " 95798: 0.03678027439271024,\n",
       " 95835: 0.07395967918286414,\n",
       " 95845: 0.06227044366782798,\n",
       " 95865: 0.06575545014403875,\n",
       " 95809: 0.09784466986900034,\n",
       " 95944: 0.06399021398526092,\n",
       " 399671: 0.08308292260887068,\n",
       " 95831: 0.1570519795727418,\n",
       " 95803: 0.0507544170339617,\n",
       " 95939: 0.08168057851837472,\n",
       " 399665: 0.10013196537780465,\n",
       " 95826: 0.05613356108791204,\n",
       " 95830: 0.05970448923574675,\n",
       " 95932: 0.08091552742461114,\n",
       " 95792: 0.05047161630966746,\n",
       " 95837: 0.09860401462571654,\n",
       " 95750: 0.08483103909302343,\n",
       " 95838: 0.05523844715713683,\n",
       " 95912: 0.05837901657479992,\n",
       " 95940: 0.28106375639936776,\n",
       " 95841: 0.04379003517532087,\n",
       " 95793: 0.10037389745988011,\n",
       " 95952: 0.06833261164408425,\n",
       " 95963: 0.022384694692992122,\n",
       " 95816: 0.04385463097920357,\n",
       " 95779: 0.03556880724460994,\n",
       " 95852: 0.06345761882502478,\n",
       " 95783: 0.0748302279911209,\n",
       " 95814: 0.062317089813476834,\n",
       " 95957: 0.05318991367972557,\n",
       " 95888: 0.1145696565254796,\n",
       " 95861: 0.06796495171038397,\n",
       " 95840: 0.12593989722055582,\n",
       " 95842: 0.05825957629912021,\n",
       " 95766: 0.04301802249315993,\n",
       " 95883: 0.08805657643588617,\n",
       " 95911: 0.040560555476354634,\n",
       " 95744: 0.12293028740711862,\n",
       " 95834: 0.10972665029147707,\n",
       " 95928: 0.06332344687643851,\n",
       " 95901: 0.04990201882416135,\n",
       " 95890: 0.04321327713289124,\n",
       " 95966: 0.01260268684228901,\n",
       " 95768: 0.029168041171359436,\n",
       " 95805: 0.19392406855791938,\n",
       " 399673: 0.047749580990445704,\n",
       " 95954: 0.06769568816125639,\n",
       " 399672: 0.1337288411319514,\n",
       " 95787: 0.058703372565927184,\n",
       " 95839: 0.08235699009904583,\n",
       " 399674: 0.10418162676150697,\n",
       " 95922: 0.1544745164013636,\n",
       " 95866: 0.07700280944195965,\n",
       " 95907: 0.24862849465107761,\n",
       " 95788: 0.061516173982173676,\n",
       " 95926: 0.02350615375760288,\n",
       " 95930: 0.07329738453275561,\n",
       " 95956: 0.056024784736855425,\n",
       " 95938: 0.07492229367261637,\n",
       " 95795: 0.0543474483661082,\n",
       " 95923: 0.040052964292729955,\n",
       " 95955: 0.08156206897896215,\n",
       " 95924: 0.054021902486854664,\n",
       " 95775: 0.04711375007392729,\n",
       " 95919: 0.05587287385189804,\n",
       " 95794: 0.10844929995311278,\n",
       " 399666: 0.025155147685891676,\n",
       " 95760: 0.048191135374906074,\n",
       " 95916: 0.09418005735587347,\n",
       " 95891: 0.046391724224360116,\n",
       " 95820: 0.0592900367823701,\n",
       " 95917: 0.08787709688927876,\n",
       " 95893: 0.08050552913425539,\n",
       " 95851: 0.09178239600397316}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes_list = list(dict_mape.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209300.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip = zipcodes_list[0]\n",
    "df_time_series[zip][-1]                                                                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{95804: [0.3980933427810669,\n",
       "  0.39803287386894226,\n",
       "  0.39821651577949524,\n",
       "  0.3983437418937683,\n",
       "  0.39833417534828186,\n",
       "  0.39809948205947876,\n",
       "  0.3975655436515808,\n",
       "  0.3966774642467499,\n",
       "  0.3953898549079895,\n",
       "  0.39365532994270325,\n",
       "  0.39141708612442017,\n",
       "  0.38860607147216797,\n",
       "  0.3851461708545685,\n",
       "  0.3809601962566376,\n",
       "  0.3759770691394806,\n",
       "  0.3701367676258087,\n",
       "  0.3633931577205658,\n",
       "  0.3557119071483612,\n",
       "  0.3470703065395355,\n",
       "  0.3374517858028412,\n",
       "  0.32684364914894104,\n",
       "  0.3152324855327606,\n",
       "  0.30259838700294495,\n",
       "  0.288912296295166],\n",
       " 95817: [0.03496561944484711,\n",
       "  0.03300313279032707,\n",
       "  0.02763722464442253,\n",
       "  0.017508089542388916,\n",
       "  0.0017974742222577333,\n",
       "  -0.0198071226477623,\n",
       "  -0.047191470861434937,\n",
       "  -0.07989288866519928,\n",
       "  -0.11715865880250931,\n",
       "  -0.15798066556453705,\n",
       "  -0.20114430785179138,\n",
       "  -0.24530665576457977,\n",
       "  -0.2891070544719696,\n",
       "  -0.33129382133483887,\n",
       "  -0.3708391487598419,\n",
       "  -0.4069867432117462,\n",
       "  -0.4392766058444977,\n",
       "  -0.46752238273620605,\n",
       "  -0.4917616844177246,\n",
       "  -0.5122056007385254,\n",
       "  -0.5291852355003357,\n",
       "  -0.5430808067321777,\n",
       "  -0.5542906522750854,\n",
       "  -0.5632075667381287],\n",
       " 95813: [-0.08914592117071152,\n",
       "  -0.09002494812011719,\n",
       "  -0.09061695635318756,\n",
       "  -0.09120641648769379,\n",
       "  -0.09201489388942719,\n",
       "  -0.09318137913942337,\n",
       "  -0.09476763755083084,\n",
       "  -0.09677083045244217,\n",
       "  -0.09914153814315796,\n",
       "  -0.10180126130580902,\n",
       "  -0.1046610027551651,\n",
       "  -0.10763166844844818,\n",
       "  -0.11063249409198761,\n",
       "  -0.11359594017267227,\n",
       "  -0.11647076159715652,\n",
       "  -0.11921939253807068,\n",
       "  -0.12181688845157623,\n",
       "  -0.12424743920564651,\n",
       "  -0.1265016496181488,\n",
       "  -0.12857364118099213,\n",
       "  -0.1304592490196228,\n",
       "  -0.13215488195419312,\n",
       "  -0.13365916907787323,\n",
       "  -0.13497011363506317],\n",
       " 95785: [0.10519571602344513,\n",
       "  0.10512767732143402,\n",
       "  0.10469193011522293,\n",
       "  0.10377313196659088,\n",
       "  0.10231314599514008,\n",
       "  0.10030514001846313,\n",
       "  0.09777990728616714,\n",
       "  0.09479204565286636,\n",
       "  0.09140553325414658,\n",
       "  0.08768580108880997,\n",
       "  0.08369389921426773,\n",
       "  0.07948682457208633,\n",
       "  0.07511603087186813,\n",
       "  0.0706249251961708,\n",
       "  0.06604639440774918,\n",
       "  0.06140042841434479,\n",
       "  0.0566931776702404,\n",
       "  0.05191780999302864,\n",
       "  0.04705667123198509,\n",
       "  0.04208521172404289,\n",
       "  0.03697390854358673,\n",
       "  0.03169167414307594,\n",
       "  0.02620869316160679,\n",
       "  0.020498527213931084],\n",
       " 95819: [-0.6186292171478271,\n",
       "  -0.6183198690414429,\n",
       "  -0.6180351972579956,\n",
       "  -0.617796778678894,\n",
       "  -0.617620587348938,\n",
       "  -0.6175149083137512,\n",
       "  -0.6174807548522949,\n",
       "  -0.6175132393836975,\n",
       "  -0.6176038384437561,\n",
       "  -0.6177417635917664,\n",
       "  -0.6179155111312866,\n",
       "  -0.6181144118309021,\n",
       "  -0.6183281540870667,\n",
       "  -0.618548572063446,\n",
       "  -0.618768572807312,\n",
       "  -0.6189833283424377,\n",
       "  -0.6191890835762024,\n",
       "  -0.6193833947181702,\n",
       "  -0.6195648312568665,\n",
       "  -0.6197330951690674,\n",
       "  -0.6198883652687073,\n",
       "  -0.6200312376022339,\n",
       "  -0.620162308216095,\n",
       "  -0.6202827095985413],\n",
       " 95770: [-0.3185250163078308,\n",
       "  -0.315857857465744,\n",
       "  -0.3131186068058014,\n",
       "  -0.31031423807144165,\n",
       "  -0.3074483275413513,\n",
       "  -0.30452123284339905,\n",
       "  -0.30153071880340576,\n",
       "  -0.29847297072410583,\n",
       "  -0.29534319043159485,\n",
       "  -0.29213589429855347,\n",
       "  -0.28884565830230713,\n",
       "  -0.2854675352573395,\n",
       "  -0.28199681639671326,\n",
       "  -0.2784293591976166,\n",
       "  -0.2747615873813629,\n",
       "  -0.2709900736808777,\n",
       "  -0.2671118676662445,\n",
       "  -0.2631247341632843,\n",
       "  -0.25902703404426575,\n",
       "  -0.25481700897216797,\n",
       "  -0.2504928410053253,\n",
       "  -0.24605338275432587,\n",
       "  -0.24149732291698456,\n",
       "  -0.23682370781898499],\n",
       " 95806: [0.4889841377735138,\n",
       "  0.48995262384414673,\n",
       "  0.4902913570404053,\n",
       "  0.489884614944458,\n",
       "  0.4886349141597748,\n",
       "  0.4864799976348877,\n",
       "  0.4833885431289673,\n",
       "  0.4793635606765747,\n",
       "  0.4744342863559723,\n",
       "  0.4686514437198639,\n",
       "  0.46208497881889343,\n",
       "  0.4548148214817047,\n",
       "  0.4469263553619385,\n",
       "  0.4385071098804474,\n",
       "  0.42964109778404236,\n",
       "  0.4204045236110687,\n",
       "  0.4108629524707794,\n",
       "  0.4010688066482544,\n",
       "  0.39106258749961853,\n",
       "  0.38087186217308044,\n",
       "  0.37050753831863403,\n",
       "  0.3599623143672943,\n",
       "  0.3492101728916168,\n",
       "  0.33821067214012146],\n",
       " 95790: [0.22319597005844116,\n",
       "  0.22291679680347443,\n",
       "  0.22219310700893402,\n",
       "  0.22093799710273743,\n",
       "  0.21905739605426788,\n",
       "  0.2164839208126068,\n",
       "  0.2131904661655426,\n",
       "  0.20918521285057068,\n",
       "  0.20450422167778015,\n",
       "  0.1992027312517166,\n",
       "  0.1933441162109375,\n",
       "  0.18699240684509277,\n",
       "  0.18020589649677277,\n",
       "  0.17303401231765747,\n",
       "  0.16551388800144196,\n",
       "  0.15766946971416473,\n",
       "  0.14951303601264954,\n",
       "  0.14104720950126648,\n",
       "  0.13226647675037384,\n",
       "  0.12316003441810608,\n",
       "  0.11371506750583649,\n",
       "  0.10391785949468613,\n",
       "  0.09375660866498947,\n",
       "  0.08322219550609589],\n",
       " 95799: [-0.2940168082714081,\n",
       "  -0.29256492853164673,\n",
       "  -0.2906266152858734,\n",
       "  -0.28818923234939575,\n",
       "  -0.28527697920799255,\n",
       "  -0.28194040060043335,\n",
       "  -0.2782449722290039,\n",
       "  -0.27426016330718994,\n",
       "  -0.2700512111186981,\n",
       "  -0.265674352645874,\n",
       "  -0.2611734867095947,\n",
       "  -0.2565799951553345,\n",
       "  -0.2519141733646393,\n",
       "  -0.24718669056892395,\n",
       "  -0.2424013465642929,\n",
       "  -0.23755784332752228,\n",
       "  -0.23265382647514343,\n",
       "  -0.2276868224143982,\n",
       "  -0.22265562415122986,\n",
       "  -0.21756082773208618,\n",
       "  -0.21240538358688354,\n",
       "  -0.20719462633132935,\n",
       "  -0.2019367516040802,\n",
       "  -0.19664180278778076],\n",
       " 95844: [0.17561191320419312,\n",
       "  0.17634744942188263,\n",
       "  0.1775473803281784,\n",
       "  0.17904691398143768,\n",
       "  0.1806749701499939,\n",
       "  0.18232868611812592,\n",
       "  0.18394313752651215,\n",
       "  0.18547821044921875,\n",
       "  0.18690824508666992,\n",
       "  0.18820811808109283,\n",
       "  0.18934227526187897,\n",
       "  0.19026128947734833,\n",
       "  0.19089636206626892,\n",
       "  0.19116543233394623,\n",
       "  0.19099630415439606,\n",
       "  0.19034750759601593,\n",
       "  0.18920357525348663,\n",
       "  0.1875651329755783,\n",
       "  0.1854359209537506,\n",
       "  0.18281707167625427,\n",
       "  0.17970329523086548,\n",
       "  0.1760839819908142,\n",
       "  0.17194543778896332,\n",
       "  0.16727590560913086],\n",
       " 95843: [0.22792759537696838,\n",
       "  0.22802241146564484,\n",
       "  0.2276904433965683,\n",
       "  0.22680799663066864,\n",
       "  0.2251618206501007,\n",
       "  0.2225366234779358,\n",
       "  0.2187388688325882,\n",
       "  0.21360789239406586,\n",
       "  0.2070205807685852,\n",
       "  0.19888967275619507,\n",
       "  0.18916009366512299,\n",
       "  0.17780311405658722,\n",
       "  0.16481314599514008,\n",
       "  0.1502060890197754,\n",
       "  0.13401253521442413,\n",
       "  0.1162756159901619,\n",
       "  0.09704864025115967,\n",
       "  0.0763988345861435,\n",
       "  0.05441394820809364,\n",
       "  0.03120351955294609,\n",
       "  0.006899457424879074,\n",
       "  -0.018354391679167747,\n",
       "  -0.04441000521183014,\n",
       "  -0.07112742215394974],\n",
       " 95815: [-0.10471236705780029,\n",
       "  -0.10410464555025101,\n",
       "  -0.10504253953695297,\n",
       "  -0.10815533995628357,\n",
       "  -0.11378462612628937,\n",
       "  -0.12200354784727097,\n",
       "  -0.13268117606639862,\n",
       "  -0.14553286135196686,\n",
       "  -0.16018038988113403,\n",
       "  -0.17619453370571136,\n",
       "  -0.19313852488994598,\n",
       "  -0.21060112118721008,\n",
       "  -0.22822202742099762,\n",
       "  -0.24568824470043182,\n",
       "  -0.2627427279949188,\n",
       "  -0.27918463945388794,\n",
       "  -0.294868141412735,\n",
       "  -0.30968788266181946,\n",
       "  -0.32358208298683167,\n",
       "  -0.3365287184715271,\n",
       "  -0.34853604435920715,\n",
       "  -0.35963156819343567,\n",
       "  -0.36985522508621216,\n",
       "  -0.37924671173095703],\n",
       " 95825: [0.2395038604736328,\n",
       "  0.23986661434173584,\n",
       "  0.24022890627384186,\n",
       "  0.24015790224075317,\n",
       "  0.2394232600927353,\n",
       "  0.2378731220960617,\n",
       "  0.23538394272327423,\n",
       "  0.23183181881904602,\n",
       "  0.2270938605070114,\n",
       "  0.22105996310710907,\n",
       "  0.21364498138427734,\n",
       "  0.2047979235649109,\n",
       "  0.19450552761554718,\n",
       "  0.18278996646404266,\n",
       "  0.1697017401456833,\n",
       "  0.15530642867088318,\n",
       "  0.1396682858467102,\n",
       "  0.12284021824598312,\n",
       "  0.10485512018203735,\n",
       "  0.08573056757450104,\n",
       "  0.0654807835817337,\n",
       "  0.04412820190191269,\n",
       "  0.021708259359002113,\n",
       "  -0.001720295986160636],\n",
       " 95818: [0.410080224275589,\n",
       "  0.4096251428127289,\n",
       "  0.4082912802696228,\n",
       "  0.40591320395469666,\n",
       "  0.40248095989227295,\n",
       "  0.3980617821216583,\n",
       "  0.39278438687324524,\n",
       "  0.38681498169898987,\n",
       "  0.38033124804496765,\n",
       "  0.3735039532184601,\n",
       "  0.3664831817150116,\n",
       "  0.3593935966491699,\n",
       "  0.3523312211036682,\n",
       "  0.3453657925128937,\n",
       "  0.3385421633720398,\n",
       "  0.33188509941101074,\n",
       "  0.32540163397789,\n",
       "  0.3190847337245941,\n",
       "  0.31291598081588745,\n",
       "  0.3068670928478241,\n",
       "  0.3009019196033478,\n",
       "  0.294976145029068,\n",
       "  0.28903937339782715,\n",
       "  0.2830349802970886],\n",
       " 95811: [0.04730672761797905,\n",
       "  0.04818412661552429,\n",
       "  0.047348279505968094,\n",
       "  0.044038884341716766,\n",
       "  0.03772363439202309,\n",
       "  0.028118519112467766,\n",
       "  0.015168767422437668,\n",
       "  -0.0010224609868600965,\n",
       "  -0.020245205610990524,\n",
       "  -0.04220552369952202,\n",
       "  -0.06654838472604752,\n",
       "  -0.09290367364883423,\n",
       "  -0.12089239805936813,\n",
       "  -0.15012028813362122,\n",
       "  -0.18017329275608063,\n",
       "  -0.21064461767673492,\n",
       "  -0.24113547801971436,\n",
       "  -0.2712602913379669,\n",
       "  -0.3006499111652374,\n",
       "  -0.3289593756198883,\n",
       "  -0.3558810353279114,\n",
       "  -0.3811582028865814,\n",
       "  -0.40459516644477844,\n",
       "  -0.42605575919151306],\n",
       " 95931: [0.24924997985363007,\n",
       "  0.2531905770301819,\n",
       "  0.2554692029953003,\n",
       "  0.2557324767112732,\n",
       "  0.25371599197387695,\n",
       "  0.2492239773273468,\n",
       "  0.24211570620536804,\n",
       "  0.23231863975524902,\n",
       "  0.21985608339309692,\n",
       "  0.20489108562469482,\n",
       "  0.18775193393230438,\n",
       "  0.16891802847385406,\n",
       "  0.14897437393665314,\n",
       "  0.1285308450460434,\n",
       "  0.10814478248357773,\n",
       "  0.08825380355119705,\n",
       "  0.0691489577293396,\n",
       "  0.050994619727134705,\n",
       "  0.03385590761899948,\n",
       "  0.017723817378282547,\n",
       "  0.0025482308119535446,\n",
       "  -0.01174086518585682,\n",
       "  -0.025223596021533012,\n",
       "  -0.03798019513487816],\n",
       " 95753: [0.10930698364973068,\n",
       "  0.11007986217737198,\n",
       "  0.1093095913529396,\n",
       "  0.10665420442819595,\n",
       "  0.1019020527601242,\n",
       "  0.0949428454041481,\n",
       "  0.085734523832798,\n",
       "  0.07427568733692169,\n",
       "  0.06058794632554054,\n",
       "  0.0447104275226593,\n",
       "  0.026697345077991486,\n",
       "  0.00661889323964715,\n",
       "  -0.015436260960996151,\n",
       "  -0.03935163840651512,\n",
       "  -0.06497489660978317,\n",
       "  -0.09211486577987671,\n",
       "  -0.1205461174249649,\n",
       "  -0.15000799298286438,\n",
       "  -0.18021361529827118,\n",
       "  -0.21088078618049622,\n",
       "  -0.24173977971076965,\n",
       "  -0.2725212574005127,\n",
       "  -0.30295488238334656,\n",
       "  -0.33274802565574646],\n",
       " 95827: [-0.2543237805366516,\n",
       "  -0.23909908533096313,\n",
       "  -0.22482585906982422,\n",
       "  -0.21171501278877258,\n",
       "  -0.19981449842453003,\n",
       "  -0.18902146816253662,\n",
       "  -0.17912445962429047,\n",
       "  -0.16985543072223663,\n",
       "  -0.16093741357326508,\n",
       "  -0.15212245285511017,\n",
       "  -0.1432150900363922,\n",
       "  -0.13408492505550385,\n",
       "  -0.12466675043106079,\n",
       "  -0.11495551466941833,\n",
       "  -0.10499513894319534,\n",
       "  -0.09486646205186844,\n",
       "  -0.08467469364404678,\n",
       "  -0.07453891634941101,\n",
       "  -0.06458264589309692,\n",
       "  -0.054927483201026917,\n",
       "  -0.04568854719400406,\n",
       "  -0.0369706004858017,\n",
       "  -0.028866253793239594,\n",
       "  -0.021454283967614174],\n",
       " 95937: [-0.5698274970054626,\n",
       "  -0.5669133067131042,\n",
       "  -0.5643515586853027,\n",
       "  -0.5623486042022705,\n",
       "  -0.5610527992248535,\n",
       "  -0.5605391263961792,\n",
       "  -0.5608136653900146,\n",
       "  -0.5618273615837097,\n",
       "  -0.5634939074516296,\n",
       "  -0.5657059550285339,\n",
       "  -0.5683491826057434,\n",
       "  -0.5713125467300415,\n",
       "  -0.5744947195053101,\n",
       "  -0.5778084397315979,\n",
       "  -0.5811812281608582,\n",
       "  -0.5845557451248169,\n",
       "  -0.5878883600234985,\n",
       "  -0.5911473035812378,\n",
       "  -0.5943108797073364,\n",
       "  -0.5973655581474304,\n",
       "  -0.6003035306930542,\n",
       "  -0.6031215190887451,\n",
       "  -0.605819821357727,\n",
       "  -0.6084001064300537],\n",
       " 95914: [0.004042044281959534,\n",
       "  0.0017054368508979678,\n",
       "  -0.0017638456774875522,\n",
       "  -0.006686079781502485,\n",
       "  -0.013200990855693817,\n",
       "  -0.021296659484505653,\n",
       "  -0.030851872637867928,\n",
       "  -0.041676104068756104,\n",
       "  -0.05354468151926994,\n",
       "  -0.06622929126024246,\n",
       "  -0.07951852679252625,\n",
       "  -0.0932338684797287,\n",
       "  -0.10723627358675003,\n",
       "  -0.12142480164766312,\n",
       "  -0.13573285937309265,\n",
       "  -0.1501201093196869,\n",
       "  -0.16456493735313416,\n",
       "  -0.179055318236351,\n",
       "  -0.19358178973197937,\n",
       "  -0.20813049376010895,\n",
       "  -0.22267809510231018,\n",
       "  -0.23718750476837158,\n",
       "  -0.251605361700058,\n",
       "  -0.2658626139163971],\n",
       " 95754: [0.03408012539148331,\n",
       "  0.03923213854432106,\n",
       "  0.0531659759581089,\n",
       "  0.07515867799520493,\n",
       "  0.10275682806968689,\n",
       "  0.13305126130580902,\n",
       "  0.1633063405752182,\n",
       "  0.19132572412490845,\n",
       "  0.21584844589233398,\n",
       "  0.23623418807983398,\n",
       "  0.2523486614227295,\n",
       "  0.2644289433956146,\n",
       "  0.27288275957107544,\n",
       "  0.27816733717918396,\n",
       "  0.2807394564151764,\n",
       "  0.2809935510158539,\n",
       "  0.27922379970550537,\n",
       "  0.2756413221359253,\n",
       "  0.27040329575538635,\n",
       "  0.26362618803977966,\n",
       "  0.2554048001766205,\n",
       "  0.24581918120384216,\n",
       "  0.23494867980480194,\n",
       "  0.22288012504577637],\n",
       " 95824: [0.08780914545059204,\n",
       "  0.08899709582328796,\n",
       "  0.09052079916000366,\n",
       "  0.09191815555095673,\n",
       "  0.09303823858499527,\n",
       "  0.09385241568088531,\n",
       "  0.09439557045698166,\n",
       "  0.0947231650352478,\n",
       "  0.09488607197999954,\n",
       "  0.0949227437376976,\n",
       "  0.09485834091901779,\n",
       "  0.0947028174996376,\n",
       "  0.09444241970777512,\n",
       "  0.09404170513153076,\n",
       "  0.09346388280391693,\n",
       "  0.09268895536661148,\n",
       "  0.09171690791845322,\n",
       "  0.09056462347507477,\n",
       "  0.08925805240869522,\n",
       "  0.08782703429460526,\n",
       "  0.08629852533340454,\n",
       "  0.08469197154045105,\n",
       "  0.08301464468240738,\n",
       "  0.08125995099544525],\n",
       " 95945: [-0.36861711740493774,\n",
       "  -0.3651198446750641,\n",
       "  -0.361733615398407,\n",
       "  -0.35856959223747253,\n",
       "  -0.35570546984672546,\n",
       "  -0.3531765639781952,\n",
       "  -0.35097917914390564,\n",
       "  -0.3490794599056244,\n",
       "  -0.34742411971092224,\n",
       "  -0.34595105051994324,\n",
       "  -0.3445979654788971,\n",
       "  -0.3433080017566681,\n",
       "  -0.34203290939331055,\n",
       "  -0.34073498845100403,\n",
       "  -0.33938631415367126,\n",
       "  -0.33796757459640503,\n",
       "  -0.3364664614200592,\n",
       "  -0.33487623929977417,\n",
       "  -0.33319351077079773,\n",
       "  -0.33141738176345825,\n",
       "  -0.32954728603363037,\n",
       "  -0.3275822699069977,\n",
       "  -0.32552123069763184,\n",
       "  -0.3233621120452881],\n",
       " 95800: [-0.05751655995845795,\n",
       "  -0.060836706310510635,\n",
       "  -0.0658070296049118,\n",
       "  -0.07334734499454498,\n",
       "  -0.08405853062868118,\n",
       "  -0.09824392944574356,\n",
       "  -0.11595024168491364,\n",
       "  -0.13700419664382935,\n",
       "  -0.161075621843338,\n",
       "  -0.1877315640449524,\n",
       "  -0.21644806861877441,\n",
       "  -0.24661540985107422,\n",
       "  -0.2775780260562897,\n",
       "  -0.30865925550460815,\n",
       "  -0.3391989767551422,\n",
       "  -0.3686036467552185,\n",
       "  -0.3963666558265686,\n",
       "  -0.42210549116134644,\n",
       "  -0.4455723464488983,\n",
       "  -0.46664920449256897,\n",
       "  -0.48532018065452576,\n",
       "  -0.5016463398933411,\n",
       "  -0.5157473087310791,\n",
       "  -0.5277938842773438],\n",
       " 95751: [-0.668332576751709,\n",
       "  -0.6638051271438599,\n",
       "  -0.6601011157035828,\n",
       "  -0.6578787565231323,\n",
       "  -0.6575507521629333,\n",
       "  -0.6592423319816589,\n",
       "  -0.6628380417823792,\n",
       "  -0.6680604815483093,\n",
       "  -0.6745510101318359,\n",
       "  -0.6819339990615845,\n",
       "  -0.6898585557937622,\n",
       "  -0.6980235576629639,\n",
       "  -0.7061861753463745,\n",
       "  -0.7141625285148621,\n",
       "  -0.721820592880249,\n",
       "  -0.7290738224983215,\n",
       "  -0.7358706593513489,\n",
       "  -0.7421871423721313,\n",
       "  -0.7480191588401794,\n",
       "  -0.7533772587776184,\n",
       "  -0.7582806944847107,\n",
       "  -0.7627553939819336,\n",
       "  -0.7668293118476868,\n",
       "  -0.7705327272415161],\n",
       " 95769: [1.312239170074463,\n",
       "  1.3305752277374268,\n",
       "  1.354735255241394,\n",
       "  1.3787099123001099,\n",
       "  1.399775743484497,\n",
       "  1.4172708988189697,\n",
       "  1.4314044713974,\n",
       "  1.4426437616348267,\n",
       "  1.4514875411987305,\n",
       "  1.458394169807434,\n",
       "  1.4637593030929565,\n",
       "  1.467914342880249,\n",
       "  1.4711254835128784,\n",
       "  1.4736073017120361,\n",
       "  1.4755247831344604,\n",
       "  1.4770084619522095,\n",
       "  1.4781584739685059,\n",
       "  1.479051113128662,\n",
       "  1.4797453880310059,\n",
       "  1.4802861213684082,\n",
       "  1.4807090759277344,\n",
       "  1.481040120124817,\n",
       "  1.4812997579574585,\n",
       "  1.4815040826797485],\n",
       " 95909: [-0.5945543050765991,\n",
       "  -0.5928896069526672,\n",
       "  -0.5912832617759705,\n",
       "  -0.5897409915924072,\n",
       "  -0.5882665514945984,\n",
       "  -0.5868595838546753,\n",
       "  -0.5855159163475037,\n",
       "  -0.5842287540435791,\n",
       "  -0.5829897522926331,\n",
       "  -0.5817897915840149,\n",
       "  -0.5806206464767456,\n",
       "  -0.5794747471809387,\n",
       "  -0.578346312046051,\n",
       "  -0.5772308111190796,\n",
       "  -0.5761248469352722,\n",
       "  -0.575026273727417,\n",
       "  -0.5739341974258423,\n",
       "  -0.5728476047515869,\n",
       "  -0.5717663168907166,\n",
       "  -0.570690393447876,\n",
       "  -0.5696201324462891,\n",
       "  -0.568555474281311,\n",
       "  -0.5674967169761658,\n",
       "  -0.5664440393447876],\n",
       " 95771: [0.280814528465271,\n",
       "  0.2825053036212921,\n",
       "  0.28248363733291626,\n",
       "  0.28070366382598877,\n",
       "  0.2772544324398041,\n",
       "  0.2722911238670349,\n",
       "  0.2659924328327179,\n",
       "  0.2585375905036926,\n",
       "  0.25009065866470337,\n",
       "  0.2407946139574051,\n",
       "  0.23077130317687988,\n",
       "  0.2201244831085205,\n",
       "  0.2089388370513916,\n",
       "  0.19727939367294312,\n",
       "  0.185195654630661,\n",
       "  0.1727144718170166,\n",
       "  0.159842386841774,\n",
       "  0.14657899737358093,\n",
       "  0.13291987776756287,\n",
       "  0.1188487783074379,\n",
       "  0.10434111952781677,\n",
       "  0.08936886489391327,\n",
       "  0.0739019587635994,\n",
       "  0.05791230499744415],\n",
       " 95935: [-0.5700796842575073,\n",
       "  -0.5638433694839478,\n",
       "  -0.5581763386726379,\n",
       "  -0.5532006025314331,\n",
       "  -0.5489729046821594,\n",
       "  -0.5454829335212708,\n",
       "  -0.5426668524742126,\n",
       "  -0.540425181388855,\n",
       "  -0.5386412143707275,\n",
       "  -0.5371971130371094,\n",
       "  -0.535984456539154,\n",
       "  -0.5349122881889343,\n",
       "  -0.533909261226654,\n",
       "  -0.5329240560531616,\n",
       "  -0.531923234462738,\n",
       "  -0.5308886170387268,\n",
       "  -0.5298131704330444,\n",
       "  -0.5286975502967834,\n",
       "  -0.5275471210479736,\n",
       "  -0.5263695120811462,\n",
       "  -0.5251724720001221,\n",
       "  -0.5239627361297607,\n",
       "  -0.5227455496788025,\n",
       "  -0.5215248465538025],\n",
       " 95798: [0.5124460458755493,\n",
       "  0.5120282769203186,\n",
       "  0.5113649964332581,\n",
       "  0.510417103767395,\n",
       "  0.5091469883918762,\n",
       "  0.5075412392616272,\n",
       "  0.505587637424469,\n",
       "  0.5032727122306824,\n",
       "  0.5005773305892944,\n",
       "  0.4974743127822876,\n",
       "  0.49392592906951904,\n",
       "  0.4898807108402252,\n",
       "  0.48527154326438904,\n",
       "  0.48001527786254883,\n",
       "  0.47400984168052673,\n",
       "  0.46713441610336304,\n",
       "  0.4592478573322296,\n",
       "  0.45018908381462097,\n",
       "  0.4397798180580139,\n",
       "  0.42782720923423767,\n",
       "  0.4141322672367096,\n",
       "  0.3985035717487335,\n",
       "  0.38077324628829956,\n",
       "  0.36081749200820923],\n",
       " 95835: [0.12902674078941345,\n",
       "  0.12882381677627563,\n",
       "  0.1286243051290512,\n",
       "  0.1284298151731491,\n",
       "  0.12817129492759705,\n",
       "  0.1278514415025711,\n",
       "  0.1274797022342682,\n",
       "  0.12706267833709717,\n",
       "  0.12659858167171478,\n",
       "  0.12607745826244354,\n",
       "  0.12548141181468964,\n",
       "  0.12479181587696075,\n",
       "  0.12399372458457947,\n",
       "  0.12308024615049362,\n",
       "  0.12205269187688828,\n",
       "  0.12091808766126633,\n",
       "  0.11968310922384262,\n",
       "  0.11834196001291275,\n",
       "  0.11686644703149796,\n",
       "  0.11520051211118698,\n",
       "  0.11326803267002106,\n",
       "  0.11099004000425339,\n",
       "  0.10830346494913101,\n",
       "  0.10517320036888123],\n",
       " 95845: [-0.29972636699676514,\n",
       "  -0.29242056608200073,\n",
       "  -0.2854911983013153,\n",
       "  -0.2791045606136322,\n",
       "  -0.27333855628967285,\n",
       "  -0.26818275451660156,\n",
       "  -0.26355674862861633,\n",
       "  -0.25933560729026794,\n",
       "  -0.25537583231925964,\n",
       "  -0.2515351176261902,\n",
       "  -0.24768821895122528,\n",
       "  -0.2437342405319214,\n",
       "  -0.23959971964359283,\n",
       "  -0.23523713648319244,\n",
       "  -0.23062129318714142,\n",
       "  -0.2257433533668518,\n",
       "  -0.22060652077198029,\n",
       "  -0.21522024273872375,\n",
       "  -0.20959720015525818,\n",
       "  -0.20375025272369385,\n",
       "  -0.19769151508808136,\n",
       "  -0.19143152236938477,\n",
       "  -0.18497951328754425,\n",
       "  -0.17834457755088806],\n",
       " 95865: [-0.002846590941771865,\n",
       "  -0.004822469782084227,\n",
       "  -0.007273585069924593,\n",
       "  -0.010269618593156338,\n",
       "  -0.013849485665559769,\n",
       "  -0.01802058331668377,\n",
       "  -0.02276565134525299,\n",
       "  -0.028047921136021614,\n",
       "  -0.03381957486271858,\n",
       "  -0.04002594202756882,\n",
       "  -0.046610742807388306,\n",
       "  -0.053519297391176224,\n",
       "  -0.060699936002492905,\n",
       "  -0.06810473650693893,\n",
       "  -0.07569093257188797,\n",
       "  -0.08342045545578003,\n",
       "  -0.09125825762748718,\n",
       "  -0.09917184710502625,\n",
       "  -0.10713169723749161,\n",
       "  -0.11510929465293884,\n",
       "  -0.12307622283697128,\n",
       "  -0.13100476562976837,\n",
       "  -0.13886786997318268,\n",
       "  -0.14663882553577423],\n",
       " 95809: [0.14815717935562134,\n",
       "  0.14838965237140656,\n",
       "  0.14827664196491241,\n",
       "  0.14779119193553925,\n",
       "  0.14686761796474457,\n",
       "  0.1454782783985138,\n",
       "  0.14363053441047668,\n",
       "  0.14135245978832245,\n",
       "  0.13868063688278198,\n",
       "  0.13565337657928467,\n",
       "  0.1323060393333435,\n",
       "  0.12867100536823273,\n",
       "  0.1247769445180893,\n",
       "  0.12064661830663681,\n",
       "  0.116294264793396,\n",
       "  0.11172080039978027,\n",
       "  0.10691383481025696,\n",
       "  0.10184754431247711,\n",
       "  0.09648780524730682,\n",
       "  0.09079436212778091,\n",
       "  0.0847262293100357,\n",
       "  0.07824426144361496,\n",
       "  0.07131403684616089,\n",
       "  0.06390789151191711],\n",
       " 95944: [-0.5806169509887695,\n",
       "  -0.5783951282501221,\n",
       "  -0.5762723684310913,\n",
       "  -0.5743579864501953,\n",
       "  -0.5727381706237793,\n",
       "  -0.5714648365974426,\n",
       "  -0.570554256439209,\n",
       "  -0.5699933171272278,\n",
       "  -0.5697477459907532,\n",
       "  -0.5697711110115051,\n",
       "  -0.5700114965438843,\n",
       "  -0.5704177021980286,\n",
       "  -0.570942759513855,\n",
       "  -0.5715463161468506,\n",
       "  -0.5721951127052307,\n",
       "  -0.5728635191917419,\n",
       "  -0.5735327005386353,\n",
       "  -0.5741894245147705,\n",
       "  -0.5748255848884583,\n",
       "  -0.5754362344741821,\n",
       "  -0.5760196447372437,\n",
       "  -0.5765755772590637,\n",
       "  -0.5771049857139587,\n",
       "  -0.5776097774505615],\n",
       " 399671: [0.1168127954006195,\n",
       "  0.11946387588977814,\n",
       "  0.11976303905248642,\n",
       "  0.11721256375312805,\n",
       "  0.11159331351518631,\n",
       "  0.10290102660655975,\n",
       "  0.09127839654684067,\n",
       "  0.07696249336004257,\n",
       "  0.06024385988712311,\n",
       "  0.04143453389406204,\n",
       "  0.020852534100413322,\n",
       "  -0.0011914781061932445,\n",
       "  -0.024405697360634804,\n",
       "  -0.048523902893066406,\n",
       "  -0.07331240922212601,\n",
       "  -0.09857328236103058,\n",
       "  -0.1241489052772522,\n",
       "  -0.1499192863702774,\n",
       "  -0.1757906824350357,\n",
       "  -0.20167499780654907,\n",
       "  -0.22747550904750824,\n",
       "  -0.2530710697174072,\n",
       "  -0.27830785512924194,\n",
       "  -0.3030011057853699],\n",
       " 95831: [-0.1087571308016777,\n",
       "  -0.08704447001218796,\n",
       "  -0.06743168085813522,\n",
       "  -0.05005035176873207,\n",
       "  -0.034868255257606506,\n",
       "  -0.02173345349729061,\n",
       "  -0.01043005846440792,\n",
       "  -0.0007273348164744675,\n",
       "  0.007585254497826099,\n",
       "  0.014679256826639175,\n",
       "  0.020681291818618774,\n",
       "  0.025680474936962128,\n",
       "  0.02973034977912903,\n",
       "  0.032855793833732605,\n",
       "  0.03507322445511818,\n",
       "  0.03640216588973999,\n",
       "  0.036866310983896255,\n",
       "  0.03649336099624634,\n",
       "  0.03531799465417862,\n",
       "  0.03337712213397026,\n",
       "  0.030710626393556595,\n",
       "  0.027366772294044495,\n",
       "  0.02339940331876278,\n",
       "  0.018862582743167877],\n",
       " 95803: [-0.3254718482494354,\n",
       "  -0.3233928978443146,\n",
       "  -0.3210453391075134,\n",
       "  -0.318386048078537,\n",
       "  -0.31540074944496155,\n",
       "  -0.3121003806591034,\n",
       "  -0.3085135519504547,\n",
       "  -0.30467960238456726,\n",
       "  -0.3006404638290405,\n",
       "  -0.29643604159355164,\n",
       "  -0.2921018898487091,\n",
       "  -0.2876664400100708,\n",
       "  -0.2831505537033081,\n",
       "  -0.2785676419734955,\n",
       "  -0.27392512559890747,\n",
       "  -0.2692253589630127,\n",
       "  -0.2644672095775604,\n",
       "  -0.25964871048927307,\n",
       "  -0.25476643443107605,\n",
       "  -0.24981611967086792,\n",
       "  -0.24479413032531738,\n",
       "  -0.23969848453998566,\n",
       "  -0.23452933132648468,\n",
       "  -0.22928652167320251],\n",
       " 95939: [-0.6848499774932861,\n",
       "  -0.6844236850738525,\n",
       "  -0.684050440788269,\n",
       "  -0.6837667226791382,\n",
       "  -0.6836040616035461,\n",
       "  -0.6835829615592957,\n",
       "  -0.6837118268013,\n",
       "  -0.6839888095855713,\n",
       "  -0.6844033002853394,\n",
       "  -0.6849392056465149,\n",
       "  -0.6855769753456116,\n",
       "  -0.686295211315155,\n",
       "  -0.6870728731155396,\n",
       "  -0.6878899335861206,\n",
       "  -0.6887285113334656,\n",
       "  -0.6895729303359985,\n",
       "  -0.6904100775718689,\n",
       "  -0.6912297010421753,\n",
       "  -0.6920236349105835,\n",
       "  -0.692785918712616,\n",
       "  -0.6935125589370728,\n",
       "  -0.6942013502120972,\n",
       "  -0.6948511600494385,\n",
       "  -0.6954620480537415],\n",
       " 399665: [-0.4345969259738922,\n",
       "  -0.430514395236969,\n",
       "  -0.426934152841568,\n",
       "  -0.4240957796573639,\n",
       "  -0.4221610724925995,\n",
       "  -0.42120081186294556,\n",
       "  -0.4212026000022888,\n",
       "  -0.4220905601978302,\n",
       "  -0.423748642206192,\n",
       "  -0.4260421395301819,\n",
       "  -0.42883428931236267,\n",
       "  -0.4319983720779419,\n",
       "  -0.4354254901409149,\n",
       "  -0.4390271306037903,\n",
       "  -0.44273555278778076,\n",
       "  -0.4465020000934601,\n",
       "  -0.45029428601264954,\n",
       "  -0.45409271121025085,\n",
       "  -0.4578869044780731,\n",
       "  -0.46167317032814026,\n",
       "  -0.4654512405395508,\n",
       "  -0.4692229628562927,\n",
       "  -0.4729902446269989,\n",
       "  -0.47675463557243347],\n",
       " 95826: [0.24764294922351837,\n",
       "  0.2478562444448471,\n",
       "  0.24794647097587585,\n",
       "  0.24783459305763245,\n",
       "  0.24737177789211273,\n",
       "  0.2464008927345276,\n",
       "  0.24479547142982483,\n",
       "  0.2424660474061966,\n",
       "  0.23935279250144958,\n",
       "  0.23541471362113953,\n",
       "  0.2306206226348877,\n",
       "  0.2249435931444168,\n",
       "  0.21836349368095398,\n",
       "  0.2108704000711441,\n",
       "  0.20247341692447662,\n",
       "  0.1932038515806198,\n",
       "  0.18311947584152222,\n",
       "  0.17230109870433807,\n",
       "  0.16084955632686615,\n",
       "  0.1488783359527588,\n",
       "  0.13650472462177277,\n",
       "  0.12384242564439774,\n",
       "  0.11099159717559814,\n",
       "  0.09803406894207001],\n",
       " 95830: [0.02237863652408123,\n",
       "  0.02150658331811428,\n",
       "  0.019358588382601738,\n",
       "  0.015826454386115074,\n",
       "  0.011014347895979881,\n",
       "  0.005129029043018818,\n",
       "  -0.0015929638175293803,\n",
       "  -0.008927476592361927,\n",
       "  -0.016684256494045258,\n",
       "  -0.024714622646570206,\n",
       "  -0.03291136026382446,\n",
       "  -0.04120361804962158,\n",
       "  -0.04954982176423073,\n",
       "  -0.05793071910738945,\n",
       "  -0.0663415864109993,\n",
       "  -0.07478655874729156,\n",
       "  -0.08327329158782959,\n",
       "  -0.09180935472249985,\n",
       "  -0.10039956867694855,\n",
       "  -0.10904432088136673,\n",
       "  -0.11773908883333206,\n",
       "  -0.12647445499897003,\n",
       "  -0.13523603975772858,\n",
       "  -0.14400608837604523],\n",
       " 95932: [-0.4493549168109894,\n",
       "  -0.44325247406959534,\n",
       "  -0.43743017315864563,\n",
       "  -0.4321095645427704,\n",
       "  -0.4274296760559082,\n",
       "  -0.4234393537044525,\n",
       "  -0.4201095998287201,\n",
       "  -0.41735583543777466,\n",
       "  -0.415061354637146,\n",
       "  -0.4130990505218506,\n",
       "  -0.41134676337242126,\n",
       "  -0.4096980392932892,\n",
       "  -0.4080672264099121,\n",
       "  -0.4063909947872162,\n",
       "  -0.4046268165111542,\n",
       "  -0.40274977684020996,\n",
       "  -0.4007485508918762,\n",
       "  -0.3986212909221649,\n",
       "  -0.39637163281440735,\n",
       "  -0.3940061330795288,\n",
       "  -0.39153173565864563,\n",
       "  -0.38895416259765625,\n",
       "  -0.3862771987915039,\n",
       "  -0.38350242376327515],\n",
       " 95792: [-0.3481696844100952,\n",
       "  -0.34769406914711,\n",
       "  -0.3471398651599884,\n",
       "  -0.3464685082435608,\n",
       "  -0.3456592559814453,\n",
       "  -0.3447074890136719,\n",
       "  -0.3436201810836792,\n",
       "  -0.34241175651550293,\n",
       "  -0.34110018610954285,\n",
       "  -0.33970344066619873,\n",
       "  -0.3382374346256256,\n",
       "  -0.3367149233818054,\n",
       "  -0.33514469861984253,\n",
       "  -0.3335317373275757,\n",
       "  -0.331878125667572,\n",
       "  -0.33018380403518677,\n",
       "  -0.3284474313259125,\n",
       "  -0.32666605710983276,\n",
       "  -0.32483580708503723,\n",
       "  -0.3229528069496155,\n",
       "  -0.3210134506225586,\n",
       "  -0.3190140724182129,\n",
       "  -0.3169512450695038,\n",
       "  -0.3148219883441925],\n",
       " 95837: [-0.5747454166412354,\n",
       "  -0.5718604922294617,\n",
       "  -0.5696133375167847,\n",
       "  -0.5683891177177429,\n",
       "  -0.5684328675270081,\n",
       "  -0.5698311924934387,\n",
       "  -0.5725343823432922,\n",
       "  -0.5763951539993286,\n",
       "  -0.581209659576416,\n",
       "  -0.5867523550987244,\n",
       "  -0.5928019285202026,\n",
       "  -0.5991584062576294,\n",
       "  -0.6056517958641052,\n",
       "  -0.6121454238891602,\n",
       "  -0.6185349225997925,\n",
       "  -0.6247447729110718,\n",
       "  -0.6307234764099121,\n",
       "  -0.6364389657974243,\n",
       "  -0.6418741941452026,\n",
       "  -0.6470229625701904,\n",
       "  -0.6518861651420593,\n",
       "  -0.656470000743866,\n",
       "  -0.6607833504676819,\n",
       "  -0.6648372411727905],\n",
       " 95750: [-0.08987510949373245,\n",
       "  -0.09096630662679672,\n",
       "  -0.09194719791412354,\n",
       "  -0.09303419291973114,\n",
       "  -0.09439615905284882,\n",
       "  -0.09614294767379761,\n",
       "  -0.09832517057657242,\n",
       "  -0.10094449669122696,\n",
       "  -0.10396683216094971,\n",
       "  -0.10733480006456375,\n",
       "  -0.11097922921180725,\n",
       "  -0.1148269772529602,\n",
       "  -0.11880720406770706,\n",
       "  -0.12285618484020233,\n",
       "  -0.12692022323608398,\n",
       "  -0.13095533847808838,\n",
       "  -0.1349266767501831,\n",
       "  -0.13880802690982819,\n",
       "  -0.1425836980342865,\n",
       "  -0.1462436020374298,\n",
       "  -0.14977623522281647,\n",
       "  -0.15316873788833618,\n",
       "  -0.15641063451766968,\n",
       "  -0.15949882566928864],\n",
       " 95838: [0.5107825994491577,\n",
       "  0.5105848908424377,\n",
       "  0.5102713704109192,\n",
       "  0.5098020434379578,\n",
       "  0.5091316103935242,\n",
       "  0.5082115530967712,\n",
       "  0.5069816708564758,\n",
       "  0.5053700804710388,\n",
       "  0.5032964944839478,\n",
       "  0.5006717443466187,\n",
       "  0.49740442633628845,\n",
       "  0.49340954422950745,\n",
       "  0.48861491680145264,\n",
       "  0.48295676708221436,\n",
       "  0.4763765037059784,\n",
       "  0.4688124358654022,\n",
       "  0.4601879119873047,\n",
       "  0.45040127635002136,\n",
       "  0.43931981921195984,\n",
       "  0.42677631974220276,\n",
       "  0.41257649660110474,\n",
       "  0.3965147137641907,\n",
       "  0.37839311361312866,\n",
       "  0.35804539918899536],\n",
       " 95912: [0.17336352169513702,\n",
       "  0.17293891310691833,\n",
       "  0.17123736441135406,\n",
       "  0.1676400899887085,\n",
       "  0.1618538647890091,\n",
       "  0.1538224220275879,\n",
       "  0.1436794400215149,\n",
       "  0.13167771697044373,\n",
       "  0.11813122034072876,\n",
       "  0.10337336361408234,\n",
       "  0.08772608637809753,\n",
       "  0.07147971540689468,\n",
       "  0.0548805333673954,\n",
       "  0.0381268635392189,\n",
       "  0.021370088681578636,\n",
       "  0.004720345605164766,\n",
       "  -0.01174724381417036,\n",
       "  -0.027984706684947014,\n",
       "  -0.04396644979715347,\n",
       "  -0.05968428775668144,\n",
       "  -0.07514412701129913,\n",
       "  -0.09036482870578766,\n",
       "  -0.10537653416395187,\n",
       "  -0.12022131681442261],\n",
       " 95940: [-0.5040311813354492,\n",
       "  -0.5041353106498718,\n",
       "  -0.5042514801025391,\n",
       "  -0.5043826103210449,\n",
       "  -0.5045299530029297,\n",
       "  -0.5046932101249695,\n",
       "  -0.5048707127571106,\n",
       "  -0.5050593018531799,\n",
       "  -0.5052543878555298,\n",
       "  -0.5054498910903931,\n",
       "  -0.505638837814331,\n",
       "  -0.5058140158653259,\n",
       "  -0.5059680938720703,\n",
       "  -0.5060948729515076,\n",
       "  -0.5061890482902527,\n",
       "  -0.5062469840049744,\n",
       "  -0.5062664151191711,\n",
       "  -0.5062469244003296,\n",
       "  -0.5061895847320557,\n",
       "  -0.5060967206954956,\n",
       "  -0.5059717297554016,\n",
       "  -0.5058189034461975,\n",
       "  -0.5056430101394653,\n",
       "  -0.5054489970207214],\n",
       " 95841: [0.2318185269832611,\n",
       "  0.23179960250854492,\n",
       "  0.23150795698165894,\n",
       "  0.2308969795703888,\n",
       "  0.22991149127483368,\n",
       "  0.22852692008018494,\n",
       "  0.22674763202667236,\n",
       "  0.22459757328033447,\n",
       "  0.2221039980649948,\n",
       "  0.2192848175764084,\n",
       "  0.21614450216293335,\n",
       "  0.21266956627368927,\n",
       "  0.2088293731212616,\n",
       "  0.2045728713274002,\n",
       "  0.1998264491558075,\n",
       "  0.1944936215877533,\n",
       "  0.18845821917057037,\n",
       "  0.18159067630767822,\n",
       "  0.1737566888332367,\n",
       "  0.1648261845111847,\n",
       "  0.15467874705791473,\n",
       "  0.14320695400238037,\n",
       "  0.13031713664531708,\n",
       "  0.11592720448970795],\n",
       " 95793: [0.05273432284593582,\n",
       "  0.053732309490442276,\n",
       "  0.05317595228552818,\n",
       "  0.050550319254398346,\n",
       "  0.04560678079724312,\n",
       "  0.038312606513500214,\n",
       "  0.02878008410334587,\n",
       "  0.017201058566570282,\n",
       "  0.003795233089476824,\n",
       "  -0.011225423775613308,\n",
       "  -0.027677860110998154,\n",
       "  -0.045411333441734314,\n",
       "  -0.06429978460073471,\n",
       "  -0.08422203361988068,\n",
       "  -0.10504576563835144,\n",
       "  -0.1266126036643982,\n",
       "  -0.14873448014259338,\n",
       "  -0.17119571566581726,\n",
       "  -0.1937713921070099,\n",
       "  -0.21624623239040375,\n",
       "  -0.23842765390872955,\n",
       "  -0.2601516544818878,\n",
       "  -0.2812904715538025,\n",
       "  -0.30174756050109863],\n",
       " 95952: [-0.10827622562646866,\n",
       "  -0.10129145532846451,\n",
       "  -0.09421025961637497,\n",
       "  -0.08693678677082062,\n",
       "  -0.07942211627960205,\n",
       "  -0.0716744065284729,\n",
       "  -0.06375519931316376,\n",
       "  -0.055766697973012924,\n",
       "  -0.04783790558576584,\n",
       "  -0.04010797664523125,\n",
       "  -0.0327138751745224,\n",
       "  -0.025781365111470222,\n",
       "  -0.019418537616729736,\n",
       "  -0.013711659237742424,\n",
       "  -0.008723785169422626,\n",
       "  -0.004496720153838396,\n",
       "  -0.0010529841529205441,\n",
       "  0.001601418131031096,\n",
       "  0.0034741084091365337,\n",
       "  0.004583795089274645,\n",
       "  0.004957270808517933,\n",
       "  0.004627563524991274,\n",
       "  0.0036322062369436026,\n",
       "  0.0020116176456212997],\n",
       " 95963: [-0.08353469520807266,\n",
       "  -0.08367622643709183,\n",
       "  -0.08416052162647247,\n",
       "  -0.08493460714817047,\n",
       "  -0.08592630177736282,\n",
       "  -0.08706822991371155,\n",
       "  -0.08830536901950836,\n",
       "  -0.0895874947309494,\n",
       "  -0.0908566266298294,\n",
       "  -0.09203912317752838,\n",
       "  -0.09304508566856384,\n",
       "  -0.09377623349428177,\n",
       "  -0.09413627535104752,\n",
       "  -0.09404145181179047,\n",
       "  -0.09342867136001587,\n",
       "  -0.09226133674383163,\n",
       "  -0.09053177386522293,\n",
       "  -0.08826199173927307,\n",
       "  -0.08550182729959488,\n",
       "  -0.08232589811086655,\n",
       "  -0.07882793247699738,\n",
       "  -0.075114406645298,\n",
       "  -0.07129792124032974,\n",
       "  -0.06748976558446884],\n",
       " 95816: [0.1739661991596222,\n",
       "  0.17978699505329132,\n",
       "  0.18330170214176178,\n",
       "  0.18416863679885864,\n",
       "  0.18222913146018982,\n",
       "  0.17746560275554657,\n",
       "  0.16996145248413086,\n",
       "  0.15986943244934082,\n",
       "  0.14737926423549652,\n",
       "  0.13270126283168793,\n",
       "  0.11605929583311081,\n",
       "  0.09767801314592361,\n",
       "  0.07777099311351776,\n",
       "  0.056536052376031876,\n",
       "  0.03415239602327347,\n",
       "  0.010786290280520916,\n",
       "  -0.013406546786427498,\n",
       "  -0.03828338533639908,\n",
       "  -0.06371400505304337,\n",
       "  -0.0895778015255928,\n",
       "  -0.11575456708669662,\n",
       "  -0.1421244591474533,\n",
       "  -0.1685672402381897,\n",
       "  -0.19495956599712372],\n",
       " 95779: [-0.0514741912484169,\n",
       "  -0.04821304231882095,\n",
       "  -0.045162152498960495,\n",
       "  -0.04235124960541725,\n",
       "  -0.03980536386370659,\n",
       "  -0.03754047304391861,\n",
       "  -0.03556375950574875,\n",
       "  -0.03387487679719925,\n",
       "  -0.03246849402785301,\n",
       "  -0.03133653476834297,\n",
       "  -0.030470481142401695,\n",
       "  -0.029861900955438614,\n",
       "  -0.02950262650847435,\n",
       "  -0.029385650530457497,\n",
       "  -0.029504580423235893,\n",
       "  -0.029853468760848045,\n",
       "  -0.030426636338233948,\n",
       "  -0.031218137592077255,\n",
       "  -0.03222184628248215,\n",
       "  -0.03343048691749573,\n",
       "  -0.03483669459819794,\n",
       "  -0.03643256425857544,\n",
       "  -0.03820931538939476,\n",
       "  -0.04015762731432915],\n",
       " 95852: [0.19326192140579224,\n",
       "  0.1945911943912506,\n",
       "  0.1946297585964203,\n",
       "  0.19318319857120514,\n",
       "  0.19011639058589935,\n",
       "  0.1853349506855011,\n",
       "  0.1787586361169815,\n",
       "  0.1703016608953476,\n",
       "  0.1598581075668335,\n",
       "  0.14729487895965576,\n",
       "  0.13245448470115662,\n",
       "  0.11516594886779785,\n",
       "  0.09526820480823517,\n",
       "  0.07263975590467453,\n",
       "  0.047234389930963516,\n",
       "  0.019114278256893158,\n",
       "  -0.011529356241226196,\n",
       "  -0.04437179118394852,\n",
       "  -0.07897177338600159,\n",
       "  -0.11480319499969482,\n",
       "  -0.15129528939723969,\n",
       "  -0.1878703087568283,\n",
       "  -0.22397787868976593,\n",
       "  -0.25911977887153625],\n",
       " 95783: [0.4112582504749298,\n",
       "  0.41854748129844666,\n",
       "  0.4257049560546875,\n",
       "  0.43277081847190857,\n",
       "  0.43974894285202026,\n",
       "  0.44661539793014526,\n",
       "  0.4533267617225647,\n",
       "  0.4598332643508911,\n",
       "  0.4660869836807251,\n",
       "  0.47204577922821045,\n",
       "  0.4776780903339386,\n",
       "  0.4829622805118561,\n",
       "  0.48788732290267944,\n",
       "  0.49245068430900574,\n",
       "  0.4966578483581543,\n",
       "  0.5005195140838623,\n",
       "  0.5040506720542908,\n",
       "  0.5072697997093201,\n",
       "  0.5101967453956604,\n",
       "  0.5128514170646667,\n",
       "  0.5152552723884583,\n",
       "  0.5174281597137451,\n",
       "  0.5193901062011719,\n",
       "  0.5211591720581055],\n",
       " 95814: [0.24482020735740662,\n",
       "  0.24467642605304718,\n",
       "  0.24437153339385986,\n",
       "  0.2437436580657959,\n",
       "  0.242621049284935,\n",
       "  0.24093784391880035,\n",
       "  0.2386859506368637,\n",
       "  0.2358923852443695,\n",
       "  0.2326059341430664,\n",
       "  0.228888139128685,\n",
       "  0.2248079776763916,\n",
       "  0.22043471038341522,\n",
       "  0.21583537757396698,\n",
       "  0.2110692858695984,\n",
       "  0.2061804085969925,\n",
       "  0.2011871039867401,\n",
       "  0.19606731832027435,\n",
       "  0.1907496303319931,\n",
       "  0.18511123955249786,\n",
       "  0.17899256944656372,\n",
       "  0.17222200334072113,\n",
       "  0.16464653611183167,\n",
       "  0.15615640580654144,\n",
       "  0.14669497311115265],\n",
       " 95957: [0.22280414402484894,\n",
       "  0.22315676510334015,\n",
       "  0.22112557291984558,\n",
       "  0.21615467965602875,\n",
       "  0.20817312598228455,\n",
       "  0.19740363955497742,\n",
       "  0.18433070182800293,\n",
       "  0.1695750206708908,\n",
       "  0.1537642627954483,\n",
       "  0.13743889331817627,\n",
       "  0.1210077702999115,\n",
       "  0.10474374145269394,\n",
       "  0.0888029932975769,\n",
       "  0.07325267046689987,\n",
       "  0.05809960514307022,\n",
       "  0.043314702808856964,\n",
       "  0.02885354869067669,\n",
       "  0.014671961776912212,\n",
       "  0.0007339981384575367,\n",
       "  -0.012981447391211987,\n",
       "  -0.026482142508029938,\n",
       "  -0.03976420313119888,\n",
       "  -0.05281638354063034,\n",
       "  -0.06562476605176926],\n",
       " 95888: [-0.0292412918061018,\n",
       "  -0.006040743552148342,\n",
       "  0.031096326187253,\n",
       "  0.07777933031320572,\n",
       "  0.12781739234924316,\n",
       "  0.17601697146892548,\n",
       "  0.21911248564720154,\n",
       "  0.25559279322624207,\n",
       "  0.2852162718772888,\n",
       "  0.30852702260017395,\n",
       "  0.32645392417907715,\n",
       "  0.3400217592716217,\n",
       "  0.35018202662467957,\n",
       "  0.3577395975589752,\n",
       "  0.3633387088775635,\n",
       "  0.3674774467945099,\n",
       "  0.37053239345550537,\n",
       "  0.3727858364582062,\n",
       "  0.3744470179080963,\n",
       "  0.37567099928855896,\n",
       "  0.3765721619129181,\n",
       "  0.3772350251674652,\n",
       "  0.3777216970920563,\n",
       "  0.37807825207710266],\n",
       " 95861: [0.16676758229732513,\n",
       "  0.16736717522144318,\n",
       "  0.16781798005104065,\n",
       "  0.16805799305438995,\n",
       "  0.16793382167816162,\n",
       "  0.1673736423254013,\n",
       "  0.16635534167289734,\n",
       "  0.1648840457201004,\n",
       "  0.16297389566898346,\n",
       "  0.1606382429599762,\n",
       "  0.15788652002811432,\n",
       "  0.1547226458787918,\n",
       "  0.15114302933216095,\n",
       "  0.14713351428508759,\n",
       "  0.14266763627529144,\n",
       "  0.13770563900470734,\n",
       "  0.13219483196735382,\n",
       "  0.12607206404209137,\n",
       "  0.11926846206188202,\n",
       "  0.11172061413526535,\n",
       "  0.10337837040424347,\n",
       "  0.09421480447053909,\n",
       "  0.08423133939504623,\n",
       "  0.07346027344465256],\n",
       " 95840: [-0.5695681571960449,\n",
       "  -0.569579541683197,\n",
       "  -0.5696427822113037,\n",
       "  -0.5697395205497742,\n",
       "  -0.5698360204696655,\n",
       "  -0.5698958039283752,\n",
       "  -0.5698873400688171,\n",
       "  -0.5697876214981079,\n",
       "  -0.5695834755897522,\n",
       "  -0.569270670413971,\n",
       "  -0.5688528418540955,\n",
       "  -0.568338930606842,\n",
       "  -0.5677424073219299,\n",
       "  -0.5670785307884216,\n",
       "  -0.5663636326789856,\n",
       "  -0.565613865852356,\n",
       "  -0.5648441910743713,\n",
       "  -0.5640683174133301,\n",
       "  -0.5632977485656738,\n",
       "  -0.562542200088501,\n",
       "  -0.5618094205856323,\n",
       "  -0.5611052513122559,\n",
       "  -0.560434103012085,\n",
       "  -0.5597985982894897],\n",
       " 95842: [-0.14109861850738525,\n",
       "  -0.14158090949058533,\n",
       "  -0.1415814906358719,\n",
       "  -0.14061211049556732,\n",
       "  -0.13837391138076782,\n",
       "  -0.13476742804050446,\n",
       "  -0.12986122071743011,\n",
       "  -0.1238388791680336,\n",
       "  -0.11694449186325073,\n",
       "  -0.1094343513250351,\n",
       "  -0.1015431135892868,\n",
       "  -0.09346456825733185,\n",
       "  -0.08534517884254456,\n",
       "  -0.07728828489780426,\n",
       "  -0.069361612200737,\n",
       "  -0.06160895526409149,\n",
       "  -0.054059647023677826,\n",
       "  -0.04673720523715019,\n",
       "  -0.039665352553129196,\n",
       "  -0.03287066891789436,\n",
       "  -0.02638356387615204,\n",
       "  -0.020237991586327553,\n",
       "  -0.014471161179244518,\n",
       "  -0.009120373986661434],\n",
       " 95766: [-0.0347348190844059,\n",
       "  -0.03546319156885147,\n",
       "  -0.03739814832806587,\n",
       "  -0.04072830453515053,\n",
       "  -0.045459549874067307,\n",
       "  -0.051475539803504944,\n",
       "  -0.05858441814780235,\n",
       "  -0.06655383110046387,\n",
       "  -0.07513739168643951,\n",
       "  -0.08409419655799866,\n",
       "  -0.09320289641618729,\n",
       "  -0.1022706851363182,\n",
       "  -0.111139215528965,\n",
       "  -0.11968584358692169,\n",
       "  -0.1278230845928192,\n",
       "  -0.13549569249153137,\n",
       "  -0.1426769644021988,\n",
       "  -0.14936354756355286,\n",
       "  -0.15557126700878143,\n",
       "  -0.1613299399614334,\n",
       "  -0.16667971014976501,\n",
       "  -0.17166660726070404,\n",
       "  -0.17634034156799316,\n",
       "  -0.18075139820575714],\n",
       " 95883: [0.3832819163799286,\n",
       "  0.38151392340660095,\n",
       "  0.3795163631439209,\n",
       "  0.37733665108680725,\n",
       "  0.37505558133125305,\n",
       "  0.37271350622177124,\n",
       "  0.37033572793006897,\n",
       "  0.3679378628730774,\n",
       "  0.36553075909614563,\n",
       "  0.36312365531921387,\n",
       "  0.36072275042533875,\n",
       "  0.35833242535591125,\n",
       "  0.3559540808200836,\n",
       "  0.35358676314353943,\n",
       "  0.3512260913848877,\n",
       "  0.3488665819168091,\n",
       "  0.3464986979961395,\n",
       "  0.34411221742630005,\n",
       "  0.34169474244117737,\n",
       "  0.3392344117164612,\n",
       "  0.3367203176021576,\n",
       "  0.33414342999458313,\n",
       "  0.33150017261505127,\n",
       "  0.3287910223007202],\n",
       " 95911: [-0.15382583439350128,\n",
       "  -0.1483660787343979,\n",
       "  -0.14252562820911407,\n",
       "  -0.13616299629211426,\n",
       "  -0.12923434376716614,\n",
       "  -0.12178649753332138,\n",
       "  -0.11393645405769348,\n",
       "  -0.10584332793951035,\n",
       "  -0.09768404811620712,\n",
       "  -0.08963368088006973,\n",
       "  -0.0818505808711052,\n",
       "  -0.07446831464767456,\n",
       "  -0.0675927996635437,\n",
       "  -0.061305828392505646,\n",
       "  -0.05566701292991638,\n",
       "  -0.0507177971303463,\n",
       "  -0.04648580774664879,\n",
       "  -0.0429871492087841,\n",
       "  -0.04022864252328873,\n",
       "  -0.03821132332086563,\n",
       "  -0.03692864999175072,\n",
       "  -0.036368533968925476,\n",
       "  -0.03651435300707817,\n",
       "  -0.03734397888183594],\n",
       " 95744: [0.07148021459579468,\n",
       "  0.07189524173736572,\n",
       "  0.07216354459524155,\n",
       "  0.07225214689970016,\n",
       "  0.07212576270103455,\n",
       "  0.07176254689693451,\n",
       "  0.07114837318658829,\n",
       "  0.07027451694011688,\n",
       "  0.06913366168737411,\n",
       "  0.06771768629550934,\n",
       "  0.06601617485284805,\n",
       "  0.0640185996890068,\n",
       "  0.06171363964676857,\n",
       "  0.05909086391329765,\n",
       "  0.05613764747977257,\n",
       "  0.05284016206860542,\n",
       "  0.049186378717422485,\n",
       "  0.04516647383570671,\n",
       "  0.04077182710170746,\n",
       "  0.03599753603339195,\n",
       "  0.03084489330649376,\n",
       "  0.025322120636701584,\n",
       "  0.019443955272436142,\n",
       "  0.013232541270554066],\n",
       " 95834: [0.12980034947395325,\n",
       "  0.1305834800004959,\n",
       "  0.13083520531654358,\n",
       "  0.13047434389591217,\n",
       "  0.12946352362632751,\n",
       "  0.1277572214603424,\n",
       "  0.12530872225761414,\n",
       "  0.12207386642694473,\n",
       "  0.11801308393478394,\n",
       "  0.11309325695037842,\n",
       "  0.10728814452886581,\n",
       "  0.10058170557022095,\n",
       "  0.09296778589487076,\n",
       "  0.08444968611001968,\n",
       "  0.07504215836524963,\n",
       "  0.06476717442274094,\n",
       "  0.05365142226219177,\n",
       "  0.041730768978595734,\n",
       "  0.029041243717074394,\n",
       "  0.015614514239132404,\n",
       "  0.001480897655710578,\n",
       "  -0.013333642855286598,\n",
       "  -0.028800787404179573,\n",
       "  -0.04487353190779686],\n",
       " 95928: [-0.2027733474969864,\n",
       "  -0.20297791063785553,\n",
       "  -0.20286381244659424,\n",
       "  -0.20224632322788239,\n",
       "  -0.20099253952503204,\n",
       "  -0.19902989268302917,\n",
       "  -0.19634443521499634,\n",
       "  -0.19297049939632416,\n",
       "  -0.18897776305675507,\n",
       "  -0.18445630371570587,\n",
       "  -0.1795036792755127,\n",
       "  -0.17421415448188782,\n",
       "  -0.16867122054100037,\n",
       "  -0.16294312477111816,\n",
       "  -0.1570816934108734,\n",
       "  -0.15112335979938507,\n",
       "  -0.1450914740562439,\n",
       "  -0.1389988362789154,\n",
       "  -0.13285261392593384,\n",
       "  -0.12665678560733795,\n",
       "  -0.12041579186916351,\n",
       "  -0.11413682252168655,\n",
       "  -0.1078312024474144,\n",
       "  -0.10151482373476028],\n",
       " 95901: [-0.3614080250263214,\n",
       "  -0.35286691784858704,\n",
       "  -0.34458470344543457,\n",
       "  -0.3368311822414398,\n",
       "  -0.3298221826553345,\n",
       "  -0.32369664311408997,\n",
       "  -0.31851282715797424,\n",
       "  -0.3142571449279785,\n",
       "  -0.3108604848384857,\n",
       "  -0.3082159459590912,\n",
       "  -0.3061961233615875,\n",
       "  -0.304667204618454,\n",
       "  -0.30350038409233093,\n",
       "  -0.30257901549339294,\n",
       "  -0.3018040359020233,\n",
       "  -0.30109500885009766,\n",
       "  -0.30039021372795105,\n",
       "  -0.2996460497379303,\n",
       "  -0.2988341748714447,\n",
       "  -0.2979387938976288,\n",
       "  -0.29695403575897217,\n",
       "  -0.29588115215301514,\n",
       "  -0.29472610354423523,\n",
       "  -0.2934975326061249],\n",
       " 95890: [0.316434770822525,\n",
       "  0.316423237323761,\n",
       "  0.3163466155529022,\n",
       "  0.3161639869213104,\n",
       "  0.3159268796443939,\n",
       "  0.3156440854072571,\n",
       "  0.3153207004070282,\n",
       "  0.31495562195777893,\n",
       "  0.31454434990882874,\n",
       "  0.31407999992370605,\n",
       "  0.3135547637939453,\n",
       "  0.3129603862762451,\n",
       "  0.3122888505458832,\n",
       "  0.3115321397781372,\n",
       "  0.31068024039268494,\n",
       "  0.30972060561180115,\n",
       "  0.3086346387863159,\n",
       "  0.3073958456516266,\n",
       "  0.3059661090373993,\n",
       "  0.304293155670166,\n",
       "  0.3023073375225067,\n",
       "  0.2999197244644165,\n",
       "  0.2970188856124878,\n",
       "  0.29346686601638794],\n",
       " 95966: [-0.09244827926158905,\n",
       "  -0.09229196608066559,\n",
       "  -0.09206835180521011,\n",
       "  -0.09175983816385269,\n",
       "  -0.09136052429676056,\n",
       "  -0.09086832404136658,\n",
       "  -0.09027810394763947,\n",
       "  -0.08958064764738083,\n",
       "  -0.0887625589966774,\n",
       "  -0.08780895918607712,\n",
       "  -0.08670683205127716,\n",
       "  -0.08544845879077911,\n",
       "  -0.08403120189905167,\n",
       "  -0.08246013522148132,\n",
       "  -0.08074762672185898,\n",
       "  -0.0789116621017456,\n",
       "  -0.07697276771068573,\n",
       "  -0.07495330274105072,\n",
       "  -0.07287690788507462,\n",
       "  -0.07076682150363922,\n",
       "  -0.06864495575428009,\n",
       "  -0.06653126329183578,\n",
       "  -0.06444314122200012,\n",
       "  -0.06239533796906471],\n",
       " 95768: [0.15136149525642395,\n",
       "  0.1524057537317276,\n",
       "  0.15525610744953156,\n",
       "  0.17047953605651855,\n",
       "  0.18879233300685883,\n",
       "  0.18994463980197906,\n",
       "  0.19180437922477722,\n",
       "  0.19367532432079315,\n",
       "  0.19548271596431732,\n",
       "  0.19717732071876526,\n",
       "  0.19866006076335907,\n",
       "  0.19977277517318726,\n",
       "  0.20031490921974182,\n",
       "  0.20006829500198364,\n",
       "  0.19882337749004364,\n",
       "  0.19638675451278687,\n",
       "  0.19256415963172913,\n",
       "  0.1871325671672821,\n",
       "  0.17981477081775665,\n",
       "  0.1702878326177597,\n",
       "  0.1582738310098648,\n",
       "  0.1436598002910614,\n",
       "  0.12655982375144958,\n",
       "  0.10727269947528839],\n",
       " 95805: [-0.4796527326107025,\n",
       "  -0.4807365834712982,\n",
       "  -0.4814198315143585,\n",
       "  -0.4810216426849365,\n",
       "  -0.4787377715110779,\n",
       "  -0.4738434851169586,\n",
       "  -0.46580618619918823,\n",
       "  -0.4543231129646301,\n",
       "  -0.43932032585144043,\n",
       "  -0.4209379255771637,\n",
       "  -0.3995113968849182,\n",
       "  -0.3755493462085724,\n",
       "  -0.3497093915939331,\n",
       "  -0.32276737689971924,\n",
       "  -0.29558420181274414,\n",
       "  -0.2690662741661072,\n",
       "  -0.24412409961223602,\n",
       "  -0.22162848711013794,\n",
       "  -0.2023656815290451,\n",
       "  -0.1869940459728241,\n",
       "  -0.17600597441196442,\n",
       "  -0.16969607770442963,\n",
       "  -0.16814132034778595,\n",
       "  -0.1711951047182083],\n",
       " 399673: [-0.5691110491752625,\n",
       "  -0.567416250705719,\n",
       "  -0.5660569667816162,\n",
       "  -0.5649811625480652,\n",
       "  -0.5641304850578308,\n",
       "  -0.5634480714797974,\n",
       "  -0.5628825426101685,\n",
       "  -0.562391459941864,\n",
       "  -0.5619408488273621,\n",
       "  -0.5615067481994629,\n",
       "  -0.561072826385498,\n",
       "  -0.5606305003166199,\n",
       "  -0.5601755976676941,\n",
       "  -0.5597071647644043,\n",
       "  -0.5592280030250549,\n",
       "  -0.5587431192398071,\n",
       "  -0.5582565069198608,\n",
       "  -0.5577704310417175,\n",
       "  -0.5572872161865234,\n",
       "  -0.5568095445632935,\n",
       "  -0.5563402771949768,\n",
       "  -0.5558799505233765,\n",
       "  -0.5554280877113342,\n",
       "  -0.5549830794334412],\n",
       " 95954: [0.22391091287136078,\n",
       "  0.22400987148284912,\n",
       "  0.2230747491121292,\n",
       "  0.22080136835575104,\n",
       "  0.2170543670654297,\n",
       "  0.211818128824234,\n",
       "  0.20516152679920197,\n",
       "  0.197206050157547,\n",
       "  0.18809722363948822,\n",
       "  0.1779826432466507,\n",
       "  0.16700074076652527,\n",
       "  0.15527448058128357,\n",
       "  0.14291083812713623,\n",
       "  0.13000507652759552,\n",
       "  0.11664414405822754,\n",
       "  0.10291130840778351,\n",
       "  0.08888751268386841,\n",
       "  0.07465248554944992,\n",
       "  0.06028356775641441,\n",
       "  0.045853931456804276,\n",
       "  0.03143060579895973,\n",
       "  0.01707191951572895,\n",
       "  0.002827813383191824,\n",
       "  -0.011262466199696064],\n",
       " 399672: [0.21459291875362396,\n",
       "  0.21417729556560516,\n",
       "  0.21283797919750214,\n",
       "  0.21043412387371063,\n",
       "  0.2068854719400406,\n",
       "  0.2021552473306656,\n",
       "  0.196243017911911,\n",
       "  0.18916206061840057,\n",
       "  0.1809261292219162,\n",
       "  0.17154549062252045,\n",
       "  0.16103126108646393,\n",
       "  0.14940494298934937,\n",
       "  0.13671086728572845,\n",
       "  0.12302543967962265,\n",
       "  0.1084580048918724,\n",
       "  0.09314357489347458,\n",
       "  0.07723133265972137,\n",
       "  0.06087152287364006,\n",
       "  0.04420485720038414,\n",
       "  0.02735378034412861,\n",
       "  0.010417483747005463,\n",
       "  -0.00652802549302578,\n",
       "  -0.023427098989486694,\n",
       "  -0.040242139250040054],\n",
       " 95787: [0.3235296607017517,\n",
       "  0.3261157274246216,\n",
       "  0.3288872539997101,\n",
       "  0.3317069709300995,\n",
       "  0.33443155884742737,\n",
       "  0.3369385600090027,\n",
       "  0.33913999795913696,\n",
       "  0.34098392724990845,\n",
       "  0.34245139360427856,\n",
       "  0.34354928135871887,\n",
       "  0.34430068731307983,\n",
       "  0.34474095702171326,\n",
       "  0.34491145610809326,\n",
       "  0.3448514938354492,\n",
       "  0.34459564089775085,\n",
       "  0.3441713750362396,\n",
       "  0.34360045194625854,\n",
       "  0.3428974449634552,\n",
       "  0.3420737683773041,\n",
       "  0.3411381244659424,\n",
       "  0.3400956690311432,\n",
       "  0.3389500081539154,\n",
       "  0.3377043306827545,\n",
       "  0.33636146783828735],\n",
       " 95839: [0.03062443993985653,\n",
       "  0.030698595568537712,\n",
       "  0.029703494161367416,\n",
       "  0.02734844759106636,\n",
       "  0.023494429886341095,\n",
       "  0.018112584948539734,\n",
       "  0.011249328963458538,\n",
       "  0.002998613053932786,\n",
       "  -0.00651731388643384,\n",
       "  -0.017164235934615135,\n",
       "  -0.028804022818803787,\n",
       "  -0.0412987656891346,\n",
       "  -0.05451022461056709,\n",
       "  -0.06830518692731857,\n",
       "  -0.08255547285079956,\n",
       "  -0.09713929146528244,\n",
       "  -0.11193813383579254,\n",
       "  -0.12683887779712677,\n",
       "  -0.14173178374767303,\n",
       "  -0.15651077032089233,\n",
       "  -0.17107345163822174,\n",
       "  -0.18532586097717285,\n",
       "  -0.19918251037597656,\n",
       "  -0.21256710588932037],\n",
       " 399674: [0.04539079591631889,\n",
       "  0.0455518402159214,\n",
       "  0.045025452971458435,\n",
       "  0.04355711117386818,\n",
       "  0.04096751660108566,\n",
       "  0.037159767001867294,\n",
       "  0.03210509568452835,\n",
       "  0.025836149230599403,\n",
       "  0.018431108444929123,\n",
       "  0.010004907846450806,\n",
       "  0.0007031820714473724,\n",
       "  -0.009306062944233418,\n",
       "  -0.019846374168992043,\n",
       "  -0.030741531401872635,\n",
       "  -0.04182455316185951,\n",
       "  -0.052945807576179504,\n",
       "  -0.06397958099842072,\n",
       "  -0.07482954114675522,\n",
       "  -0.08543205261230469,\n",
       "  -0.09575522691011429,\n",
       "  -0.10579180717468262,\n",
       "  -0.11555749177932739,\n",
       "  -0.12508240342140198,\n",
       "  -0.1344069540500641],\n",
       " 95922: [0.40216904878616333,\n",
       "  0.4262865483760834,\n",
       "  0.4638018310070038,\n",
       "  0.506301760673523,\n",
       "  0.547535240650177,\n",
       "  0.5840498805046082,\n",
       "  0.6145361661911011,\n",
       "  0.6390053629875183,\n",
       "  0.658143162727356,\n",
       "  0.6728724241256714,\n",
       "  0.684110164642334,\n",
       "  0.6926541328430176,\n",
       "  0.699150800704956,\n",
       "  0.7041038870811462,\n",
       "  0.7078952193260193,\n",
       "  0.7108117341995239,\n",
       "  0.713066577911377,\n",
       "  0.7148193717002869,\n",
       "  0.7161886096000671,\n",
       "  0.7172636985778809,\n",
       "  0.7181114554405212,\n",
       "  0.7187822461128235,\n",
       "  0.7193152904510498,\n",
       "  0.7197399735450745],\n",
       " 95866: [-0.3105848431587219,\n",
       "  -0.2976020872592926,\n",
       "  -0.28495705127716064,\n",
       "  -0.2732791304588318,\n",
       "  -0.26304781436920166,\n",
       "  -0.2545354962348938,\n",
       "  -0.24780619144439697,\n",
       "  -0.24275024235248566,\n",
       "  -0.23913612961769104,\n",
       "  -0.23666398227214813,\n",
       "  -0.23501256108283997,\n",
       "  -0.23387524485588074,\n",
       "  -0.2329842448234558,\n",
       "  -0.232124462723732,\n",
       "  -0.23113751411437988,\n",
       "  -0.22991913557052612,\n",
       "  -0.2284112572669983,\n",
       "  -0.22659355401992798,\n",
       "  -0.22447234392166138,\n",
       "  -0.22206933796405792,\n",
       "  -0.21941344439983368,\n",
       "  -0.2165350615978241,\n",
       "  -0.21346193552017212,\n",
       "  -0.21021443605422974],\n",
       " 95907: [0.8807616233825684,\n",
       "  0.8825531601905823,\n",
       "  0.8846224546432495,\n",
       "  0.886629045009613,\n",
       "  0.8884458541870117,\n",
       "  0.8900238275527954,\n",
       "  0.8913594484329224,\n",
       "  0.8924720287322998,\n",
       "  0.893388569355011,\n",
       "  0.8941386342048645,\n",
       "  0.8947486877441406,\n",
       "  0.8952434062957764,\n",
       "  0.8956432938575745,\n",
       "  0.8959655165672302,\n",
       "  0.8962242007255554,\n",
       "  0.8964313864707947,\n",
       "  0.8965965509414673,\n",
       "  0.896727442741394,\n",
       "  0.8968307375907898,\n",
       "  0.8969113826751709,\n",
       "  0.896973729133606,\n",
       "  0.8970218896865845,\n",
       "  0.8970574140548706,\n",
       "  0.8970840573310852],\n",
       " 95788: [0.5897276401519775,\n",
       "  0.5975260138511658,\n",
       "  0.6059092283248901,\n",
       "  0.6149503588676453,\n",
       "  0.6245575547218323,\n",
       "  0.6345439553260803,\n",
       "  0.6446801424026489,\n",
       "  0.6547377109527588,\n",
       "  0.6645171046257019,\n",
       "  0.6738614439964294,\n",
       "  0.682657778263092,\n",
       "  0.6908379793167114,\n",
       "  0.6983692049980164,\n",
       "  0.7052463293075562,\n",
       "  0.7114851474761963,\n",
       "  0.7171163558959961,\n",
       "  0.7221789956092834,\n",
       "  0.7267167568206787,\n",
       "  0.7307745814323425,\n",
       "  0.7343971729278564,\n",
       "  0.737626850605011,\n",
       "  0.7405043840408325,\n",
       "  0.7430674433708191,\n",
       "  0.7453504204750061],\n",
       " 95926: [0.2620740830898285,\n",
       "  0.2617768943309784,\n",
       "  0.26122936606407166,\n",
       "  0.2605424225330353,\n",
       "  0.25973016023635864,\n",
       "  0.2588033974170685,\n",
       "  0.2577628493309021,\n",
       "  0.25660136342048645,\n",
       "  0.2553052008152008,\n",
       "  0.2538584768772125,\n",
       "  0.25224554538726807,\n",
       "  0.25045114755630493,\n",
       "  0.24845820665359497,\n",
       "  0.2462494671344757,\n",
       "  0.24380667507648468,\n",
       "  0.2411099523305893,\n",
       "  0.23813775181770325,\n",
       "  0.23486705124378204,\n",
       "  0.2312745898962021,\n",
       "  0.22733615338802338,\n",
       "  0.22302651405334473,\n",
       "  0.21832118928432465,\n",
       "  0.21319732069969177,\n",
       "  0.20763635635375977],\n",
       " 95930: [0.2724112272262573,\n",
       "  0.2724063992500305,\n",
       "  0.27068787813186646,\n",
       "  0.26721569895744324,\n",
       "  0.26200881600379944,\n",
       "  0.25517746806144714,\n",
       "  0.24687935411930084,\n",
       "  0.23729681968688965,\n",
       "  0.22661937773227692,\n",
       "  0.215030238032341,\n",
       "  0.20269791781902313,\n",
       "  0.189768984913826,\n",
       "  0.17636606097221375,\n",
       "  0.1625886857509613,\n",
       "  0.14851389825344086,\n",
       "  0.13420076668262482,\n",
       "  0.11969073861837387,\n",
       "  0.10500980168581009,\n",
       "  0.09017062932252884,\n",
       "  0.07517192512750626,\n",
       "  0.059999167919158936,\n",
       "  0.04462387040257454,\n",
       "  0.029005253687500954,\n",
       "  0.013090874068439007],\n",
       " 95956: [-0.00931551307439804,\n",
       "  -0.008461372926831245,\n",
       "  -0.008600892499089241,\n",
       "  -0.009902258403599262,\n",
       "  -0.012367133982479572,\n",
       "  -0.015885742381215096,\n",
       "  -0.02028813771903515,\n",
       "  -0.025379715487360954,\n",
       "  -0.03096822090446949,\n",
       "  -0.03687764331698418,\n",
       "  -0.04295700788497925,\n",
       "  -0.049084316939115524,\n",
       "  -0.055166326463222504,\n",
       "  -0.0611349418759346,\n",
       "  -0.0669451504945755,\n",
       "  -0.07256937026977539,\n",
       "  -0.07799467444419861,\n",
       "  -0.08321882784366608,\n",
       "  -0.08824758976697922,\n",
       "  -0.09309173375368118,\n",
       "  -0.09776553511619568,\n",
       "  -0.10228455811738968,\n",
       "  -0.10666503757238388,\n",
       "  -0.11092370748519897],\n",
       " 95938: [-0.5664753317832947,\n",
       "  -0.5645216107368469,\n",
       "  -0.5627099275588989,\n",
       "  -0.5611732602119446,\n",
       "  -0.5600175261497498,\n",
       "  -0.559305727481842,\n",
       "  -0.5590569376945496,\n",
       "  -0.5592525005340576,\n",
       "  -0.5598462820053101,\n",
       "  -0.5607753992080688,\n",
       "  -0.5619696378707886,\n",
       "  -0.5633589029312134,\n",
       "  -0.5648782253265381,\n",
       "  -0.5664712190628052,\n",
       "  -0.5680912137031555,\n",
       "  -0.5697020888328552,\n",
       "  -0.5712766051292419,\n",
       "  -0.5727962255477905,\n",
       "  -0.574249267578125,\n",
       "  -0.5756292343139648,\n",
       "  -0.5769342184066772,\n",
       "  -0.5781648755073547,\n",
       "  -0.579323947429657,\n",
       "  -0.5804152488708496],\n",
       " 95795: [0.10194374620914459,\n",
       "  0.10256361961364746,\n",
       "  0.10105174034833908,\n",
       "  0.09688229858875275,\n",
       "  0.08976399898529053,\n",
       "  0.07960746437311172,\n",
       "  0.06649933755397797,\n",
       "  0.05066706985235214,\n",
       "  0.03243524208664894,\n",
       "  0.012181457132101059,\n",
       "  -0.009699813090264797,\n",
       "  -0.03282059356570244,\n",
       "  -0.05682455003261566,\n",
       "  -0.08141941577196121,\n",
       "  -0.10642529278993607,\n",
       "  -0.1318332701921463,\n",
       "  -0.15786072611808777,\n",
       "  -0.18495148420333862,\n",
       "  -0.21361136436462402,\n",
       "  -0.24401792883872986,\n",
       "  -0.27563348412513733,\n",
       "  -0.3072241544723511,\n",
       "  -0.3373279571533203,\n",
       "  -0.3647642135620117],\n",
       " 95923: [-0.5877372026443481,\n",
       "  -0.5874846577644348,\n",
       "  -0.5869479775428772,\n",
       "  -0.5859352946281433,\n",
       "  -0.5843061804771423,\n",
       "  -0.5819829702377319,\n",
       "  -0.5789473652839661,\n",
       "  -0.5752302408218384,\n",
       "  -0.5708982944488525,\n",
       "  -0.5660415291786194,\n",
       "  -0.5607606768608093,\n",
       "  -0.5551581978797913,\n",
       "  -0.5493298768997192,\n",
       "  -0.5433605313301086,\n",
       "  -0.5373206734657288,\n",
       "  -0.5312656760215759,\n",
       "  -0.5252361297607422,\n",
       "  -0.5192583799362183,\n",
       "  -0.5133476257324219,\n",
       "  -0.5075094103813171,\n",
       "  -0.5017416477203369,\n",
       "  -0.496037095785141,\n",
       "  -0.49038487672805786,\n",
       "  -0.4847724139690399],\n",
       " 95955: [-0.07665655016899109,\n",
       "  -0.07675207406282425,\n",
       "  -0.07749360799789429,\n",
       "  -0.07915332168340683,\n",
       "  -0.08185560256242752,\n",
       "  -0.08560656011104584,\n",
       "  -0.09032668173313141,\n",
       "  -0.0958787202835083,\n",
       "  -0.10210057348012924,\n",
       "  -0.10882612317800522,\n",
       "  -0.11589451879262924,\n",
       "  -0.12316666543483734,\n",
       "  -0.13052992522716522,\n",
       "  -0.1378975808620453,\n",
       "  -0.14520154893398285,\n",
       "  -0.15239492058753967,\n",
       "  -0.1594567745923996,\n",
       "  -0.1663735955953598,\n",
       "  -0.17313389480113983,\n",
       "  -0.17973695695400238,\n",
       "  -0.18618835508823395,\n",
       "  -0.19248861074447632,\n",
       "  -0.19864647090435028,\n",
       "  -0.20467567443847656],\n",
       " 95924: [-0.3577268421649933,\n",
       "  -0.3544354736804962,\n",
       "  -0.35126104950904846,\n",
       "  -0.34824639558792114,\n",
       "  -0.34542316198349,\n",
       "  -0.3428083062171936,\n",
       "  -0.34040310978889465,\n",
       "  -0.33819547295570374,\n",
       "  -0.33616313338279724,\n",
       "  -0.3342770040035248,\n",
       "  -0.3325054943561554,\n",
       "  -0.33081677556037903,\n",
       "  -0.3291813135147095,\n",
       "  -0.3275734484195709,\n",
       "  -0.32597243785858154,\n",
       "  -0.324362188577652,\n",
       "  -0.32273152470588684,\n",
       "  -0.3210733234882355,\n",
       "  -0.3193836510181427,\n",
       "  -0.317661315202713,\n",
       "  -0.3159063756465912,\n",
       "  -0.3141203820705414,\n",
       "  -0.3123047351837158,\n",
       "  -0.3104604482650757],\n",
       " 95775: [-0.48311758041381836,\n",
       "  -0.4826580882072449,\n",
       "  -0.48199161887168884,\n",
       "  -0.4810114800930023,\n",
       "  -0.47964316606521606,\n",
       "  -0.47785332798957825,\n",
       "  -0.4756476581096649,\n",
       "  -0.4730636477470398,\n",
       "  -0.4701606333255768,\n",
       "  -0.46701040863990784,\n",
       "  -0.4636882245540619,\n",
       "  -0.4602665603160858,\n",
       "  -0.45680999755859375,\n",
       "  -0.45337244868278503,\n",
       "  -0.44999611377716064,\n",
       "  -0.4467112720012665,\n",
       "  -0.4435376524925232,\n",
       "  -0.44048574566841125,\n",
       "  -0.4375589191913605,\n",
       "  -0.43475502729415894,\n",
       "  -0.4320683181285858,\n",
       "  -0.4294905960559845,\n",
       "  -0.42701277136802673,\n",
       "  -0.4246256351470947],\n",
       " 95919: [0.1293056160211563,\n",
       "  0.12862904369831085,\n",
       "  0.12608161568641663,\n",
       "  0.12078387290239334,\n",
       "  0.11198317259550095,\n",
       "  0.09929490834474564,\n",
       "  0.08282288163900375,\n",
       "  0.06316378712654114,\n",
       "  0.041276443749666214,\n",
       "  0.018275130540132523,\n",
       "  -0.00477950181812048,\n",
       "  -0.027033107355237007,\n",
       "  -0.04790909215807915,\n",
       "  -0.06710270792245865,\n",
       "  -0.08452349156141281,\n",
       "  -0.10022421181201935,\n",
       "  -0.11434052884578705,\n",
       "  -0.12705014646053314,\n",
       "  -0.13854949176311493,\n",
       "  -0.14903967082500458,\n",
       "  -0.15871848165988922,\n",
       "  -0.1677722930908203,\n",
       "  -0.17636904120445251,\n",
       "  -0.18465161323547363],\n",
       " 95794: [0.03148695081472397,\n",
       "  0.03350188583135605,\n",
       "  0.03387521207332611,\n",
       "  0.03214909881353378,\n",
       "  0.02809574268758297,\n",
       "  0.021669810637831688,\n",
       "  0.012958596460521221,\n",
       "  0.002144479425624013,\n",
       "  -0.01053424272686243,\n",
       "  -0.024807322770357132,\n",
       "  -0.04039189964532852,\n",
       "  -0.05701031535863876,\n",
       "  -0.07440454512834549,\n",
       "  -0.09233687072992325,\n",
       "  -0.11058679223060608,\n",
       "  -0.12895134091377258,\n",
       "  -0.1472492814064026,\n",
       "  -0.16531221568584442,\n",
       "  -0.18298670649528503,\n",
       "  -0.20014412701129913,\n",
       "  -0.21668170392513275,\n",
       "  -0.23252007365226746,\n",
       "  -0.2476101517677307,\n",
       "  -0.26192909479141235],\n",
       " 399666: [0.04460339620709419,\n",
       "  0.044576700776815414,\n",
       "  0.042695000767707825,\n",
       "  0.03828832879662514,\n",
       "  0.030985340476036072,\n",
       "  0.020695805549621582,\n",
       "  0.007593099493533373,\n",
       "  -0.007931050844490528,\n",
       "  -0.02535220794379711,\n",
       "  -0.044106993824243546,\n",
       "  -0.06362857669591904,\n",
       "  -0.08331646025180817,\n",
       "  -0.10251358896493912,\n",
       "  -0.12055610865354538,\n",
       "  -0.13687260448932648,\n",
       "  -0.15107280015945435,\n",
       "  -0.16298022866249084,\n",
       "  -0.17260706424713135,\n",
       "  -0.18010257184505463,\n",
       "  -0.18569877743721008,\n",
       "  -0.18966969847679138,\n",
       "  -0.19230327010154724,\n",
       "  -0.19388099014759064,\n",
       "  -0.19466473162174225],\n",
       " 95760: [0.2681293785572052,\n",
       "  0.26907581090927124,\n",
       "  0.2688651382923126,\n",
       "  0.2671700417995453,\n",
       "  0.26373112201690674,\n",
       "  0.258355051279068,\n",
       "  0.2509010434150696,\n",
       "  0.24127578735351562,\n",
       "  0.22943134605884552,\n",
       "  0.21536792814731598,\n",
       "  0.1991414874792099,\n",
       "  0.18086886405944824,\n",
       "  0.16073058545589447,\n",
       "  0.13896341621875763,\n",
       "  0.11584459990262985,\n",
       "  0.09166809171438217,\n",
       "  0.06672017276287079,\n",
       "  0.04125924035906792,\n",
       "  0.015503168106079102,\n",
       "  -0.010373822413384914,\n",
       "  -0.03623934090137482,\n",
       "  -0.06199591979384422,\n",
       "  -0.08757218718528748,\n",
       "  -0.11291453242301941],\n",
       " 95916: [-0.5148540735244751,\n",
       "  -0.5146280527114868,\n",
       "  -0.5143889784812927,\n",
       "  -0.5141037702560425,\n",
       "  -0.5137373805046082,\n",
       "  -0.5132619738578796,\n",
       "  -0.5126601457595825,\n",
       "  -0.5119257569313049,\n",
       "  -0.5110629796981812,\n",
       "  -0.510083794593811,\n",
       "  -0.5090060830116272,\n",
       "  -0.5078510642051697,\n",
       "  -0.5066410303115845,\n",
       "  -0.5053984522819519,\n",
       "  -0.5041439533233643,\n",
       "  -0.5028956532478333,\n",
       "  -0.50166916847229,\n",
       "  -0.5004770755767822,\n",
       "  -0.49932876229286194,\n",
       "  -0.4982311427593231,\n",
       "  -0.4971883296966553,\n",
       "  -0.49620264768600464,\n",
       "  -0.49527469277381897,\n",
       "  -0.49440354108810425],\n",
       " 95891: [-0.378085196018219,\n",
       "  -0.37790724635124207,\n",
       "  -0.3776876926422119,\n",
       "  -0.3773655295372009,\n",
       "  -0.3768816888332367,\n",
       "  -0.37619033455848694,\n",
       "  -0.37526339292526245,\n",
       "  -0.37408992648124695,\n",
       "  -0.3726743757724762,\n",
       "  -0.3710332214832306,\n",
       "  -0.36919111013412476,\n",
       "  -0.3671781122684479,\n",
       "  -0.3650265634059906,\n",
       "  -0.36276909708976746,\n",
       "  -0.36043673753738403,\n",
       "  -0.35805800557136536,\n",
       "  -0.3556579053401947,\n",
       "  -0.3532577455043793,\n",
       "  -0.35087504982948303,\n",
       "  -0.3485237658023834,\n",
       "  -0.34621432423591614,\n",
       "  -0.34395429491996765,\n",
       "  -0.3417486250400543,\n",
       "  -0.3395999073982239],\n",
       " 95820: [0.36095529794692993,\n",
       "  0.36206597089767456,\n",
       "  0.36131608486175537,\n",
       "  0.3584549129009247,\n",
       "  0.3534253239631653,\n",
       "  0.3462960124015808,\n",
       "  0.3372111916542053,\n",
       "  0.32635700702667236,\n",
       "  0.31393662095069885,\n",
       "  0.3001539409160614,\n",
       "  0.28520840406417847,\n",
       "  0.2692900598049164,\n",
       "  0.2525773346424103,\n",
       "  0.23523761332035065,\n",
       "  0.21742674708366394,\n",
       "  0.19928723573684692,\n",
       "  0.18095020949840546,\n",
       "  0.1625334918498993,\n",
       "  0.14413920044898987,\n",
       "  0.12585526704788208,\n",
       "  0.10775631666183472,\n",
       "  0.08990560472011566,\n",
       "  0.07235441356897354,\n",
       "  0.05514104664325714],\n",
       " 95917: [0.6567484736442566,\n",
       "  0.6568835377693176,\n",
       "  0.6566299796104431,\n",
       "  0.6560713052749634,\n",
       "  0.6552512049674988,\n",
       "  0.6542052626609802,\n",
       "  0.6529684662818909,\n",
       "  0.6515759825706482,\n",
       "  0.650061309337616,\n",
       "  0.6484566926956177,\n",
       "  0.6467915177345276,\n",
       "  0.6450924277305603,\n",
       "  0.6433832049369812,\n",
       "  0.6416836977005005,\n",
       "  0.6400119066238403,\n",
       "  0.6383808255195618,\n",
       "  0.636802077293396,\n",
       "  0.6352841854095459,\n",
       "  0.6338328719139099,\n",
       "  0.6324526071548462,\n",
       "  0.6311452388763428,\n",
       "  0.6299121975898743,\n",
       "  0.6287530660629272,\n",
       "  0.6276660561561584],\n",
       " 95893: [-0.6029619574546814,\n",
       "  -0.602985143661499,\n",
       "  -0.6030447483062744,\n",
       "  -0.6030790209770203,\n",
       "  -0.6030025482177734,\n",
       "  -0.6027302145957947,\n",
       "  -0.6021919846534729,\n",
       "  -0.6013391017913818,\n",
       "  -0.6001462936401367,\n",
       "  -0.5986095666885376,\n",
       "  -0.5967428088188171,\n",
       "  -0.5945742130279541,\n",
       "  -0.5921413898468018,\n",
       "  -0.5894878506660461,\n",
       "  -0.5866591930389404,\n",
       "  -0.5837008953094482,\n",
       "  -0.5806559324264526,\n",
       "  -0.5775632262229919,\n",
       "  -0.5744568109512329,\n",
       "  -0.5713655948638916,\n",
       "  -0.5683134198188782,\n",
       "  -0.5653185248374939,\n",
       "  -0.562394917011261,\n",
       "  -0.5595521926879883],\n",
       " 95851: [-0.5724940299987793,\n",
       "  -0.5721896290779114,\n",
       "  -0.5718791484832764,\n",
       "  -0.5715416669845581,\n",
       "  -0.5711562037467957,\n",
       "  -0.5707085728645325,\n",
       "  -0.5701934695243835,\n",
       "  -0.5696136951446533,\n",
       "  -0.5689783692359924,\n",
       "  -0.5683003067970276,\n",
       "  -0.5675947070121765,\n",
       "  -0.5668765306472778,\n",
       "  -0.5661599040031433,\n",
       "  -0.565457284450531,\n",
       "  -0.5647785663604736,\n",
       "  -0.5641316771507263,\n",
       "  -0.5635218620300293,\n",
       "  -0.5629527568817139,\n",
       "  -0.5624260902404785,\n",
       "  -0.5619421005249023,\n",
       "  -0.561500072479248,\n",
       "  -0.5610983371734619,\n",
       "  -0.5607348680496216,\n",
       "  -0.5604069828987122]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_changes_dict = {}\n",
    "for zip in zipcodes_list:\n",
    "    price_changes = []\n",
    "    starting_price = df_time_series[zip][-1]\n",
    "    for price in dict_next_5_years[zip]:\n",
    "        price_changes.append(float((price-starting_price)/starting_price))\n",
    "    price_changes_dict[zip] = price_changes\n",
    "\n",
    "price_changes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_changes_dict = {}\n",
    "for zip in zipcodes_list:\n",
    "    price_changes = []\n",
    "    starting_price = df_time_series[zip][-1]\n",
    "    for price in dict_next_5_years[zip]:\n",
    "        price_changes.append(float((price-starting_price)/starting_price))\n",
    "    price_changes_dict[zip] = price_changes\n",
    "\n",
    "price_changes_dict\n",
    "\n",
    "five_year_total_change = {}\n",
    "for zip in price_changes_dict.keys:\n",
    "    five_year_total_change[zip] = price_changes_dict[zip][-1]\n",
    "\n",
    "dict(sorted(five_year_total_change.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_total_change = {}\n",
    "for zip in price_changes_dict.keys:\n",
    "    five_year_total_change[zip] = price_changes_dict[zip][-1]\n",
    "\n",
    "dict(sorted(five_year_total_change.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nv_zipcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = df_time_series[nv_zipcodes]\n",
    "df_time_series.fillna(method='bfill',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 139619.5781 - val_loss: 105.6819\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 38879.8203 - val_loss: 107.2705\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2307.9272 - val_loss: 107.5364\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 50975.1172 - val_loss: 108.2509\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 29106.7656 - val_loss: 103.9321\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 46060.8125 - val_loss: 103.4052\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 34934.6367 - val_loss: 104.7273\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1612.8590 - val_loss: 108.7745\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 109828.8828 - val_loss: 110.7470\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 103022.6875 - val_loss: 107.3356\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14851.8340 - val_loss: 104.6583\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 26064.7871 - val_loss: 103.6182\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 32582.0566 - val_loss: 104.8018\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 730.4402 - val_loss: 104.2553\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14412.1641 - val_loss: 105.0931\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 16746.2852 - val_loss: 105.1355\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 728.0956 - val_loss: 102.6481\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 81431.2734 - val_loss: 101.3935\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 63010.0000 - val_loss: 102.4051\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28876.0488 - val_loss: 105.1123\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 39782.8477 - val_loss: 106.1483\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 42966.1367 - val_loss: 105.0453\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12041.9092 - val_loss: 103.8976\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 23586.8496 - val_loss: 103.6225\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15471.8682 - val_loss: 104.1881\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6049.1284 - val_loss: 106.8371\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 57597.8242 - val_loss: 106.8354\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 62773.4609 - val_loss: 105.7465\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40666.4492 - val_loss: 103.8835\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1011.2993 - val_loss: 104.0444\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3365.2898 - val_loss: 103.2680\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23361.7754 - val_loss: 103.3877\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7365.6201 - val_loss: 104.2860\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24296.6953 - val_loss: 104.8553\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 17890.4434 - val_loss: 104.2244\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 260.1161 - val_loss: 104.0907\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3171.9941 - val_loss: 103.4607\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25447.4746 - val_loss: 103.2132\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11665.5283 - val_loss: 104.3487\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25379.9355 - val_loss: 104.7427\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25142.4258 - val_loss: 104.3355\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10825.6748 - val_loss: 103.0265\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 31469.5098 - val_loss: 102.6395\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 29371.5586 - val_loss: 103.0179\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7530.6768 - val_loss: 103.9751\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14713.0605 - val_loss: 104.3075\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17771.8379 - val_loss: 103.6589\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5244.1963 - val_loss: 103.7430\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2611.4851 - val_loss: 103.3183\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9213.4561 - val_loss: 103.7714\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff74527a940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:28:06.098 WARNING tensorflow: 5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff74527a940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 273ms/step - loss: 158570.7344 - val_loss: 105.4910\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 54402.5117 - val_loss: 109.7624\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 137056.1719 - val_loss: 104.2770\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 31478.0684 - val_loss: 105.2016\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25771.7832 - val_loss: 106.5041\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 22952.9434 - val_loss: 106.4346\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 27257.8262 - val_loss: 106.5745\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10928.4473 - val_loss: 106.4339\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 21503.8320 - val_loss: 108.9771\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 132164.4688 - val_loss: 107.3334\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 30274.0684 - val_loss: 106.0576\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 16245.7188 - val_loss: 107.1064\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14789.2168 - val_loss: 109.4970\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 80960.3984 - val_loss: 108.1812\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 33523.1836 - val_loss: 105.1205\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 108937.2188 - val_loss: 105.0767\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 40606.3438 - val_loss: 107.3668\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 44436.3281 - val_loss: 106.0390\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 35332.7773 - val_loss: 106.9383\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 13346.6133 - val_loss: 105.2133\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 35768.9258 - val_loss: 106.3563\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 27372.8047 - val_loss: 105.3192\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 11667.7637 - val_loss: 105.4403\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 5680.2612 - val_loss: 104.7530\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 31198.6836 - val_loss: 105.5303\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 53240.8477 - val_loss: 105.7924\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 10760.7090 - val_loss: 104.0911\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 49625.0039 - val_loss: 105.4333\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 28445.2012 - val_loss: 104.6202\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 67721.2734 - val_loss: 104.8464\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 32404.3848 - val_loss: 107.5970\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 178594.3125 - val_loss: 107.7282\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 112975.0156 - val_loss: 104.3703\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33712.8945 - val_loss: 103.5371\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 26906.7441 - val_loss: 104.7396\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13279.9082 - val_loss: 104.1067\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12505.8545 - val_loss: 105.1795\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 61122.1992 - val_loss: 104.6936\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13547.0078 - val_loss: 103.5905\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 35781.6562 - val_loss: 103.8356\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 27931.9941 - val_loss: 103.9822\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 1731.3735 - val_loss: 103.7821\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 8811.7949 - val_loss: 104.0301\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5894.9282 - val_loss: 102.3882\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 121116.1719 - val_loss: 102.2977\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 70375.9297 - val_loss: 104.5160\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 70342.5000 - val_loss: 105.1876\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 65469.1133 - val_loss: 103.6514\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 29241.7695 - val_loss: 103.0380\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 17658.6406 - val_loss: 104.3560\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff75072f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:28:20.670 WARNING tensorflow: 5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff75072f0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 73.3229 - val_loss: 53.9215\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 44.7800 - val_loss: 27.5422\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 40.0423 - val_loss: 34.6571\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 31.2568 - val_loss: 32.1918\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 26.3360 - val_loss: 19.3689\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 23.5004 - val_loss: 18.7244\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 20.7228 - val_loss: 13.5793\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 17.8872 - val_loss: 5.6358\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16.8156 - val_loss: 6.1780\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.3281 - val_loss: 3.6630\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.4430 - val_loss: 3.4808\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 14.5196 - val_loss: 5.5869\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.1404 - val_loss: 2.8901\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12.4978 - val_loss: 5.3384\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 12.1375 - val_loss: 2.5885\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12.0889 - val_loss: 4.9288\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.0619 - val_loss: 2.6245\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.1096 - val_loss: 4.5310\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10.6632 - val_loss: 3.0900\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.2209 - val_loss: 3.3120\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.8399 - val_loss: 3.4074\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.7014 - val_loss: 4.1280\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.9815 - val_loss: 3.2396\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.5375 - val_loss: 3.3148\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5000 - val_loss: 2.0275\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.1444 - val_loss: 5.0892\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.8248 - val_loss: 2.9818\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.2150 - val_loss: 2.5606\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.0719 - val_loss: 4.9176\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.9247 - val_loss: 1.8447\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.9722 - val_loss: 3.1634\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.8691 - val_loss: 2.8782\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.6026 - val_loss: 3.9970\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.3562 - val_loss: 2.4167\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.1059 - val_loss: 2.6595\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8180 - val_loss: 3.3004\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.0774 - val_loss: 1.9279\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7362 - val_loss: 6.2923\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.2924 - val_loss: 1.8813\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.0124 - val_loss: 4.2496\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.0484 - val_loss: 2.0166\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.6942 - val_loss: 3.9816\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.2757 - val_loss: 1.4510\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.7595 - val_loss: 3.3651\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7600 - val_loss: 1.7939\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4006 - val_loss: 2.7875\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4996 - val_loss: 2.0201\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.5131 - val_loss: 1.7142\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.6825 - val_loss: 3.3285\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.6443 - val_loss: 1.6076\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70e99b9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:28:34.227 WARNING tensorflow: 5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70e99b9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 2 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 207ms/step - loss: 83.7711 - val_loss: 58.4907\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 31.6202 - val_loss: 8.0529\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 30.3673 - val_loss: 10.5296\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19.9731 - val_loss: 27.5689\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19.4665 - val_loss: 24.9805\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 16.2988 - val_loss: 14.5816\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.0001 - val_loss: 10.8720\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.4923 - val_loss: 11.9001\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.4172 - val_loss: 9.3390\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6653 - val_loss: 7.4451\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.2141 - val_loss: 4.3585\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.8562 - val_loss: 4.4298\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.7551 - val_loss: 2.5877\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.9516 - val_loss: 3.6579\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3671 - val_loss: 4.2070\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9054 - val_loss: 2.9471\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.9365 - val_loss: 4.9615\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.8603 - val_loss: 2.6321\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4044 - val_loss: 3.6994\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.5410 - val_loss: 2.8264\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4210 - val_loss: 3.0603\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0783 - val_loss: 2.9306\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.9037 - val_loss: 3.5964\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8457 - val_loss: 2.5400\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.7077 - val_loss: 3.4567\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7714 - val_loss: 2.3395\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.7641 - val_loss: 3.0958\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3852 - val_loss: 2.3180\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.2844 - val_loss: 3.1761\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.3310 - val_loss: 2.1878\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.2700 - val_loss: 2.8766\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.7436 - val_loss: 2.3298\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.1177 - val_loss: 3.3794\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4888 - val_loss: 2.5594\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.2227 - val_loss: 2.3141\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.2580 - val_loss: 2.5414\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.8358 - val_loss: 2.3389\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.5327 - val_loss: 2.9096\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.6380 - val_loss: 2.3154\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4.5778 - val_loss: 2.9890\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.5066 - val_loss: 2.3828\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.4460 - val_loss: 2.5186\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.4245 - val_loss: 2.2290\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.1826 - val_loss: 2.2304\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.1287 - val_loss: 2.1653\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8814 - val_loss: 2.5261\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.0567 - val_loss: 2.0851\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9127 - val_loss: 2.0549\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.8097 - val_loss: 2.0002\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.8145 - val_loss: 2.4547\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff711b885e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:28:46.312 WARNING tensorflow: 5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff711b885e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 3 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 77.2080 - val_loss: 64.0526\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 56.9167 - val_loss: 40.4519\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 47.4118 - val_loss: 41.5761\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 36.7804 - val_loss: 40.9793\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30.6808 - val_loss: 28.0800\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26.2045 - val_loss: 17.4853\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22.2700 - val_loss: 18.8595\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18.3843 - val_loss: 8.5472\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.3012 - val_loss: 6.0965\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14.6749 - val_loss: 5.1099\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.5628 - val_loss: 4.8204\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.0947 - val_loss: 4.5164\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.5962 - val_loss: 6.4161\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 11.2831 - val_loss: 3.4813\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 11.3630 - val_loss: 5.9353\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.5180 - val_loss: 4.5921\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 9.9913 - val_loss: 4.8371\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.0514 - val_loss: 4.8610\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6049 - val_loss: 3.4730\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.7222 - val_loss: 6.8864\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.5507 - val_loss: 3.2046\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.0839 - val_loss: 3.9501\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.8258 - val_loss: 3.7704\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.5127 - val_loss: 4.0765\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.6242 - val_loss: 5.7413\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7332 - val_loss: 3.1463\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.9086 - val_loss: 4.4115\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.1678 - val_loss: 3.0502\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6570 - val_loss: 5.4936\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.7015 - val_loss: 2.4021\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9829 - val_loss: 4.7914\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.9246 - val_loss: 2.6936\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5743 - val_loss: 3.0714\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.1508 - val_loss: 3.4416\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6225 - val_loss: 3.1221\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.1769 - val_loss: 2.7484\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4157 - val_loss: 2.7076\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.3081 - val_loss: 4.4849\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4842 - val_loss: 2.5078\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0623 - val_loss: 3.8050\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6872 - val_loss: 2.7752\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.5315 - val_loss: 2.4950\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4837 - val_loss: 3.8761\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6477 - val_loss: 2.7184\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6552 - val_loss: 2.9200\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5961 - val_loss: 2.8525\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.7002 - val_loss: 2.4608\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.6305 - val_loss: 3.4454\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5649 - val_loss: 2.3553\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4451 - val_loss: 3.5882\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff715834040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:28:58.445 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff715834040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 4 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 32s 6s/step - loss: 160226.7500 - val_loss: 89.9043\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 20515.4746 - val_loss: 94.9922\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 341.4645 - val_loss: 98.8682\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 70242.4844 - val_loss: 98.5831\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12201.6533 - val_loss: 94.4889\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11622.7051 - val_loss: 92.2714\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 51975.5977 - val_loss: 92.3506\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7852.4702 - val_loss: 94.8789\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 45150.4883 - val_loss: 97.1922\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 59187.6367 - val_loss: 96.6488\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15508.8770 - val_loss: 92.3903\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18688.7305 - val_loss: 91.2087\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39090.5430 - val_loss: 92.5783\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1849.5159 - val_loss: 93.0519\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35022.4453 - val_loss: 91.7982\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8487.1846 - val_loss: 93.8130\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 52559.5195 - val_loss: 96.3188\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 45786.8438 - val_loss: 94.6498\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19438.3320 - val_loss: 91.4475\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 69411.4922 - val_loss: 90.1026\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 49729.1992 - val_loss: 91.3908\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26313.1992 - val_loss: 94.7614\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31305.3809 - val_loss: 95.5563\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 49935.9531 - val_loss: 95.1373\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26231.2500 - val_loss: 93.8257\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11189.2920 - val_loss: 93.0262\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8686.5967 - val_loss: 94.7699\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21480.3418 - val_loss: 94.4646\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12458.5459 - val_loss: 93.2382\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16555.7793 - val_loss: 93.3504\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9011.9033 - val_loss: 94.6180\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15285.6396 - val_loss: 94.3729\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8312.7324 - val_loss: 93.4583\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22536.6816 - val_loss: 93.0619\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11111.2090 - val_loss: 93.8220\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13615.5703 - val_loss: 94.6717\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16739.0273 - val_loss: 94.0598\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5236.6289 - val_loss: 94.0583\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5611.8154 - val_loss: 93.7054\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11735.8828 - val_loss: 93.8567\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1753.5673 - val_loss: 93.5957\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11474.9502 - val_loss: 93.9961\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7068.7065 - val_loss: 94.0567\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 18526.1289 - val_loss: 93.3525\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5107.1860 - val_loss: 94.5444\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28445.2500 - val_loss: 95.3331\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 25096.6621 - val_loss: 94.7514\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4420.1973 - val_loss: 93.1748\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21144.5645 - val_loss: 92.9492\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29340.8418 - val_loss: 93.1514\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7470a8b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:29:40.862 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7470a8b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 5 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 4325.2422 - val_loss: 71.9812\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 157718.3750 - val_loss: 69.0970\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 103779.7969 - val_loss: 76.1629\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4782.2080 - val_loss: 84.4537\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 131523.1562 - val_loss: 89.2783\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 106008.9609 - val_loss: 87.0666\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 32915.3672 - val_loss: 83.7247\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12523.9346 - val_loss: 77.4789\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 128007.6719 - val_loss: 75.5553\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 136660.9531 - val_loss: 76.8788\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 103203.2344 - val_loss: 79.6393\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 68492.6016 - val_loss: 83.4604\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31520.8184 - val_loss: 85.1945\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15766.7139 - val_loss: 84.3412\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5874.2607 - val_loss: 83.6705\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13512.7129 - val_loss: 84.8129\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11035.6855 - val_loss: 84.4586\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4688.3252 - val_loss: 85.1189\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15728.2578 - val_loss: 84.9110\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1038.1763 - val_loss: 83.3074\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48273.2188 - val_loss: 82.8229\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37740.4883 - val_loss: 83.7865\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2873.2266 - val_loss: 85.3394\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 164.4820 - val_loss: 83.7639\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 45243.5000 - val_loss: 83.7310\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39319.2383 - val_loss: 85.4755\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8336.8760 - val_loss: 86.1290\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5559.6211 - val_loss: 86.1340\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4034.6074 - val_loss: 85.7820\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8590.8184 - val_loss: 86.3458\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6389.8442 - val_loss: 86.1574\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10454.9707 - val_loss: 86.2097\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2895.8840 - val_loss: 87.7752\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 46224.4922 - val_loss: 88.3637\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 41331.4922 - val_loss: 86.6327\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 566.1862 - val_loss: 86.7265\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1587.0571 - val_loss: 87.5051\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24799.3320 - val_loss: 87.8487\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13669.0801 - val_loss: 86.9985\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11390.1445 - val_loss: 86.6587\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8972.2705 - val_loss: 87.9111\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26623.3848 - val_loss: 88.1805\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14766.7432 - val_loss: 87.3233\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10236.1426 - val_loss: 86.9769\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7545.0879 - val_loss: 88.0149\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20564.2969 - val_loss: 88.1369\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8684.8701 - val_loss: 87.5450\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13842.4355 - val_loss: 86.8789\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16379.2383 - val_loss: 87.5266\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 870.9348 - val_loss: 87.4202\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff790fe23a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:29:49.991 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff790fe23a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 6 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 74.8629 - val_loss: 59.4335\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50.7440 - val_loss: 32.7716\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 43.6438 - val_loss: 32.2832\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36.9337 - val_loss: 34.5564\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28.5152 - val_loss: 18.9251\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.7071 - val_loss: 20.2684\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22.0939 - val_loss: 16.3131\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.1835 - val_loss: 9.1549\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.1200 - val_loss: 6.5484\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.3996 - val_loss: 5.3640\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.5420 - val_loss: 3.4969\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.1109 - val_loss: 5.1938\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.1799 - val_loss: 3.0235\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.1872 - val_loss: 4.6452\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.3016 - val_loss: 3.6209\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.8093 - val_loss: 3.9483\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.0741 - val_loss: 2.9906\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.9357 - val_loss: 4.7038\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.1865 - val_loss: 4.2817\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.8763 - val_loss: 2.8766\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.6576 - val_loss: 4.2380\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1254 - val_loss: 4.9641\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.6864 - val_loss: 2.5906\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.7252 - val_loss: 2.1738\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3611 - val_loss: 6.5000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.3231 - val_loss: 2.8409\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.3724 - val_loss: 5.5543\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.3123 - val_loss: 1.7247\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.3609 - val_loss: 4.5588\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.2165 - val_loss: 3.2782\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2278 - val_loss: 3.4393\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8798 - val_loss: 2.0967\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4163 - val_loss: 6.4612\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4406 - val_loss: 2.1541\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8092 - val_loss: 3.5457\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4858 - val_loss: 2.4494\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.1354 - val_loss: 4.1848\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2856 - val_loss: 2.1028\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7475 - val_loss: 2.9527\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.6159 - val_loss: 3.1044\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6806 - val_loss: 3.0229\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.1677 - val_loss: 3.0265\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2011 - val_loss: 4.1377\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3018 - val_loss: 2.1815\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5053 - val_loss: 2.6794\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.2643 - val_loss: 4.2486\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.3527 - val_loss: 2.6685\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9891 - val_loss: 3.2912\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9234 - val_loss: 1.7852\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.8700 - val_loss: 4.5777\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8db74f430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:29:58.502 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8db74f430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 7 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 103769.1641 - val_loss: 94.3138\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 65281.7031 - val_loss: 103.0183\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27813.1973 - val_loss: 97.9785\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29062.4707 - val_loss: 96.5572\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 28833.5820 - val_loss: 98.0776\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 13586.1445 - val_loss: 98.6110\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 5148.6812 - val_loss: 99.2419\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 12548.0000 - val_loss: 98.1791\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 10681.9561 - val_loss: 98.5138\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 42855.1367 - val_loss: 100.8163\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 15462.8916 - val_loss: 98.2272\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 1716.2079 - val_loss: 96.7360\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 26325.4316 - val_loss: 98.1273\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9657.6904 - val_loss: 98.1479\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16191.7402 - val_loss: 97.9407\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5228.3501 - val_loss: 101.8346\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 67847.8125 - val_loss: 102.3682\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 52660.3438 - val_loss: 100.9732\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7251.1001 - val_loss: 96.6737\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50907.9414 - val_loss: 94.4917\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 61781.6172 - val_loss: 95.0010\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29251.8691 - val_loss: 97.0618\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8150.7666 - val_loss: 98.9313\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17520.1875 - val_loss: 98.1943\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13549.9736 - val_loss: 97.7294\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6928.5264 - val_loss: 100.9038\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 53313.5352 - val_loss: 101.3261\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 52205.7109 - val_loss: 100.2106\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19998.0938 - val_loss: 98.2778\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 27594.6152 - val_loss: 96.8097\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 16123.8438 - val_loss: 98.2040\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12897.8965 - val_loss: 98.8354\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8849.0752 - val_loss: 98.2427\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14528.5986 - val_loss: 97.4048\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14768.3252 - val_loss: 97.8533\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 638.7156 - val_loss: 98.3581\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2586.7749 - val_loss: 97.2946\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23949.2129 - val_loss: 97.3413\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10231.6240 - val_loss: 98.4443\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10026.7617 - val_loss: 98.6930\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7891.7539 - val_loss: 97.7537\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20677.7012 - val_loss: 97.3640\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9992.3672 - val_loss: 98.4485\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4057.1465 - val_loss: 98.4861\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4747.2397 - val_loss: 97.4265\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24890.2969 - val_loss: 97.1930\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12413.6816 - val_loss: 98.3420\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1962.1141 - val_loss: 98.4325\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3709.2234 - val_loss: 97.4319\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21911.5000 - val_loss: 97.4710\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff80d182790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:30:08.060 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff80d182790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 8 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 86.4036 - val_loss: 70.3278\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 58.9233 - val_loss: 44.8564\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 47.7814 - val_loss: 44.2842\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36.6523 - val_loss: 32.5985\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31.1065 - val_loss: 19.8252\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27.2384 - val_loss: 19.9034\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24.8709 - val_loss: 9.1079\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21.9113 - val_loss: 8.6652\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.2957 - val_loss: 4.2536\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19.5427 - val_loss: 4.4198\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 17.6753 - val_loss: 4.5071\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17.2135 - val_loss: 4.1646\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.6785 - val_loss: 3.9768\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.9448 - val_loss: 3.9697\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.4767 - val_loss: 3.9127\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.6593 - val_loss: 4.5582\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.5219 - val_loss: 4.4520\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.0431 - val_loss: 4.6283\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.0732 - val_loss: 3.5700\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.8372 - val_loss: 5.8095\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.1664 - val_loss: 5.2498\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.9764 - val_loss: 3.2354\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.9648 - val_loss: 6.5699\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.7266 - val_loss: 3.4934\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.7638 - val_loss: 3.7320\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.6842 - val_loss: 3.1306\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.0382 - val_loss: 4.6851\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.4478 - val_loss: 3.0701\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.0801 - val_loss: 3.6999\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.7548 - val_loss: 3.7868\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.5078 - val_loss: 2.6250\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.8021 - val_loss: 3.2438\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.6102 - val_loss: 2.7647\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.4365 - val_loss: 2.5708\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.2250 - val_loss: 2.6494\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.0671 - val_loss: 2.6916\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3832 - val_loss: 2.3040\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.2852 - val_loss: 2.6044\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6085 - val_loss: 2.6172\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6286 - val_loss: 3.6368\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.3871 - val_loss: 2.7785\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.8316 - val_loss: 2.9768\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.3909 - val_loss: 2.7454\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6097 - val_loss: 3.4145\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5967 - val_loss: 2.9887\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6505 - val_loss: 3.3058\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2833 - val_loss: 2.9886\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.1179 - val_loss: 3.2543\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.9579 - val_loss: 3.0756\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.0440 - val_loss: 3.1432\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70e99b4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:30:16.207 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70e99b4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 9 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 39799.0117 - val_loss: 71.2527\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 163439.2812 - val_loss: 76.1352\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 80967.3828 - val_loss: 80.7928\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 24193.5488 - val_loss: 84.6609\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2005.8447 - val_loss: 85.3046\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 24633.5977 - val_loss: 84.8485\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10907.0371 - val_loss: 85.5242\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7965.0264 - val_loss: 84.1362\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22962.5234 - val_loss: 85.6799\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18901.3125 - val_loss: 85.8355\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1445.7965 - val_loss: 86.7223\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25402.3926 - val_loss: 85.9935\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16542.1523 - val_loss: 85.5169\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12232.6016 - val_loss: 86.2100\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 339.2757 - val_loss: 84.1717\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 63320.4883 - val_loss: 82.9641\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 34751.3672 - val_loss: 85.9547\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 35451.9922 - val_loss: 88.2351\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 22683.1875 - val_loss: 86.2004\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40992.1992 - val_loss: 85.0142\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25029.5352 - val_loss: 87.5686\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 39517.2852 - val_loss: 89.1490\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34680.8867 - val_loss: 86.9428\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21084.9512 - val_loss: 87.0007\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14875.0459 - val_loss: 88.2955\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22645.5352 - val_loss: 88.9042\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16931.2969 - val_loss: 87.0942\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37102.7656 - val_loss: 86.7067\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24422.1719 - val_loss: 87.9083\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6015.3862 - val_loss: 88.5064\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2754.8394 - val_loss: 87.1602\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33247.1250 - val_loss: 87.5218\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18668.9336 - val_loss: 88.5708\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3268.5581 - val_loss: 88.8196\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 295.1617 - val_loss: 89.8249\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20876.0859 - val_loss: 89.3849\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7345.0928 - val_loss: 88.3090\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24608.6328 - val_loss: 88.1040\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17748.2871 - val_loss: 89.4607\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22915.9980 - val_loss: 89.9298\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13563.0215 - val_loss: 88.8442\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14434.6543 - val_loss: 88.8173\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8512.1279 - val_loss: 89.3641\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8239.6455 - val_loss: 89.9070\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12683.2061 - val_loss: 89.5440\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10968.0391 - val_loss: 89.2708\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5747.3438 - val_loss: 90.8247\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33134.0117 - val_loss: 90.8699\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24449.1719 - val_loss: 90.2530\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11634.6777 - val_loss: 88.3018\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff72d207820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:30:25.161 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff72d207820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 10 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 2620.9324 - val_loss: 88.1764\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 123601.0781 - val_loss: 90.4279\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 215643.7031 - val_loss: 95.8797\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 145731.5781 - val_loss: 87.9343\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 166757.0625 - val_loss: 86.0184\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 102585.7188 - val_loss: 91.2105\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21976.8086 - val_loss: 91.2392\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21231.6035 - val_loss: 91.8363\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6292.5874 - val_loss: 91.3382\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22247.8848 - val_loss: 93.8330\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 71675.2109 - val_loss: 94.4760\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15361.7617 - val_loss: 89.2343\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 150545.0938 - val_loss: 88.4129\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 139114.5156 - val_loss: 91.0780\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32612.9961 - val_loss: 94.0401\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 57585.7188 - val_loss: 91.8149\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 56162.3008 - val_loss: 91.5211\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24676.5078 - val_loss: 95.0225\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 122332.4219 - val_loss: 95.2699\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 47030.5703 - val_loss: 92.5561\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 92010.3438 - val_loss: 90.6664\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 74415.5156 - val_loss: 92.8101\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34713.3789 - val_loss: 93.7885\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3236.8179 - val_loss: 92.8829\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5250.3647 - val_loss: 92.9215\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10834.4326 - val_loss: 93.1219\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15464.2549 - val_loss: 93.1877\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29198.9766 - val_loss: 95.1485\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38314.1797 - val_loss: 92.8716\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 74527.1016 - val_loss: 93.2671\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 45049.5742 - val_loss: 96.5194\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 76681.5781 - val_loss: 97.1171\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 60294.0469 - val_loss: 94.6234\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27880.4180 - val_loss: 94.1224\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28565.4805 - val_loss: 95.4738\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14498.4922 - val_loss: 93.4581\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42886.8242 - val_loss: 93.4505\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19673.7402 - val_loss: 95.6324\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 88296.6406 - val_loss: 96.2474\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 48727.1328 - val_loss: 93.9756\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 47932.8398 - val_loss: 93.0024\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 39114.9219 - val_loss: 95.1713\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47109.3047 - val_loss: 95.4430\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 32937.2852 - val_loss: 93.3420\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 47534.6992 - val_loss: 93.0343\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24905.4941 - val_loss: 94.8435\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 47836.2695 - val_loss: 95.5502\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37499.6797 - val_loss: 93.9321\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47780.9023 - val_loss: 93.5742\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9641.0029 - val_loss: 95.4933\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff728195280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:30:33.351 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff728195280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 11 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 174660.9531 - val_loss: 98.9602\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 36676.6172 - val_loss: 87.2182\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 331686.5938 - val_loss: 84.7213\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 268407.0312 - val_loss: 91.4034\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42334.8242 - val_loss: 96.5315\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 96005.6797 - val_loss: 94.3317\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20188.9473 - val_loss: 94.4292\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21726.9297 - val_loss: 93.9401\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20449.4004 - val_loss: 94.5614\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32914.0273 - val_loss: 94.2359\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23304.7852 - val_loss: 94.1817\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1134.4617 - val_loss: 93.5724\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26492.6191 - val_loss: 94.8079\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21324.5410 - val_loss: 94.4017\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5950.3008 - val_loss: 95.8884\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 53148.5078 - val_loss: 95.2218\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25035.9316 - val_loss: 91.2160\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 136582.9219 - val_loss: 90.6729\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 139774.3281 - val_loss: 94.6199\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13004.2393 - val_loss: 96.7997\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 90132.7266 - val_loss: 96.3564\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 36708.3906 - val_loss: 93.4932\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 81985.2266 - val_loss: 92.5714\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 57680.7109 - val_loss: 94.4639\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 34870.4297 - val_loss: 96.7555\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 88844.7500 - val_loss: 95.4516\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23488.4082 - val_loss: 93.0463\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 57544.3086 - val_loss: 93.2026\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 39197.1445 - val_loss: 94.9959\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 26158.2344 - val_loss: 94.9034\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15196.5186 - val_loss: 94.4007\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19579.0098 - val_loss: 95.3134\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17744.4082 - val_loss: 95.3747\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8443.7578 - val_loss: 95.1643\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13704.9727 - val_loss: 96.7066\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 54171.1250 - val_loss: 96.8021\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30652.3223 - val_loss: 95.1748\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14071.3096 - val_loss: 96.3562\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47384.4883 - val_loss: 96.4678\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21176.3164 - val_loss: 94.8562\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18159.4746 - val_loss: 96.1414\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 47027.7617 - val_loss: 95.5188\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10786.9678 - val_loss: 93.9881\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 75802.8047 - val_loss: 93.1989\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 51402.2812 - val_loss: 94.7755\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31800.5195 - val_loss: 95.5942\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11836.5381 - val_loss: 94.5222\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14458.4404 - val_loss: 95.3005\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28760.5137 - val_loss: 94.8395\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12522.4111 - val_loss: 95.0152\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff753d061f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:30:41.716 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff753d061f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 12 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 109491.4844 - val_loss: 107.1264\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 57516.2383 - val_loss: 102.2763\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 52399.4414 - val_loss: 107.6654\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 70553.2891 - val_loss: 107.0673\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29743.3984 - val_loss: 100.4606\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 196029.7969 - val_loss: 98.8267\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 135394.9219 - val_loss: 102.5182\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19960.1152 - val_loss: 107.7196\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 102762.6953 - val_loss: 107.3436\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39412.0469 - val_loss: 103.9845\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 82713.9922 - val_loss: 102.4206\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 57382.8359 - val_loss: 104.5560\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8895.7363 - val_loss: 104.4711\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5816.8472 - val_loss: 104.7594\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 31589.5254 - val_loss: 103.1958\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24609.9434 - val_loss: 105.1865\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51983.5625 - val_loss: 106.0145\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 41840.1758 - val_loss: 104.5314\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 59401.5859 - val_loss: 100.7801\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 119940.2891 - val_loss: 102.7234\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44037.4492 - val_loss: 105.8521\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 46432.4844 - val_loss: 106.3223\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44394.4688 - val_loss: 105.2606\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20937.3555 - val_loss: 103.6413\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19195.4922 - val_loss: 104.9263\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 25778.9434 - val_loss: 105.3249\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19185.7246 - val_loss: 104.1928\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19702.8184 - val_loss: 104.5206\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31119.1191 - val_loss: 103.1404\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7847.4727 - val_loss: 103.1510\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1605.3531 - val_loss: 103.4208\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5548.2983 - val_loss: 103.3181\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10349.7705 - val_loss: 102.2054\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26911.1094 - val_loss: 102.6202\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10533.8311 - val_loss: 103.6068\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10994.4189 - val_loss: 102.8158\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39080.6133 - val_loss: 102.5776\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19254.9297 - val_loss: 104.0742\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 42922.0469 - val_loss: 104.3208\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20711.9355 - val_loss: 102.8008\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26315.7715 - val_loss: 102.8905\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12960.3496 - val_loss: 103.8157\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18921.4746 - val_loss: 103.0926\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16797.9473 - val_loss: 104.1657\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16798.8633 - val_loss: 102.9788\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 53303.6641 - val_loss: 102.7916\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22068.6055 - val_loss: 104.4207\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12561.3760 - val_loss: 103.8715\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 44171.9688 - val_loss: 103.3999\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18991.3887 - val_loss: 104.7775\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff83667c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:30:50.005 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff83667c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 13 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 144510.6875 - val_loss: 99.0401\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3444.1975 - val_loss: 115.8764\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 283380.0938 - val_loss: 120.0770\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 225915.7344 - val_loss: 114.4462\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 156957.2031 - val_loss: 105.9638\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33439.0156 - val_loss: 98.3731\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 128992.9219 - val_loss: 94.4319\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 101455.7734 - val_loss: 95.9170\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 53444.5938 - val_loss: 100.1208\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20950.8945 - val_loss: 101.3739\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37652.0234 - val_loss: 100.1610\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 419.6887 - val_loss: 99.7235\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15952.4561 - val_loss: 99.7888\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7756.6060 - val_loss: 98.7142\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17217.7773 - val_loss: 98.6235\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8218.6553 - val_loss: 99.6833\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23753.4961 - val_loss: 100.0987\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12649.8047 - val_loss: 98.9211\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 32955.1133 - val_loss: 97.8435\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16672.3086 - val_loss: 100.1298\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 26564.3359 - val_loss: 100.6158\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 26559.0742 - val_loss: 98.9395\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6563.2900 - val_loss: 99.6099\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10553.2607 - val_loss: 98.9051\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11763.3408 - val_loss: 99.3111\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5998.2905 - val_loss: 98.7170\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25486.3164 - val_loss: 98.3308\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6423.8560 - val_loss: 99.2359\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 526.6597 - val_loss: 96.4909\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 67656.1250 - val_loss: 95.9380\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 59922.1133 - val_loss: 96.9004\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19857.0742 - val_loss: 98.8098\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29533.7832 - val_loss: 100.7453\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34775.1758 - val_loss: 99.7618\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10876.9355 - val_loss: 97.6986\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38422.9414 - val_loss: 97.2126\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34267.4023 - val_loss: 98.8455\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3600.0098 - val_loss: 98.6206\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8835.1973 - val_loss: 99.1774\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13595.2080 - val_loss: 99.0552\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5445.2642 - val_loss: 98.9536\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19814.5176 - val_loss: 99.6352\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10520.4922 - val_loss: 98.8541\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 573.6722 - val_loss: 95.6920\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 80658.6172 - val_loss: 94.7090\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 83502.0078 - val_loss: 95.1883\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 53985.2617 - val_loss: 96.5546\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31778.6738 - val_loss: 99.5580\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 40375.8555 - val_loss: 100.6981\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45712.2891 - val_loss: 99.8050\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ea0d2d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:30:58.281 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ea0d2d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 14 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 98831.4609 - val_loss: 88.7673\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 40518.2539 - val_loss: 95.5064\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 26864.5391 - val_loss: 94.5046\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 41451.4766 - val_loss: 92.2222\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36574.3828 - val_loss: 93.7308\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3740.7175 - val_loss: 97.7708\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 105200.3750 - val_loss: 99.8761\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 77764.0078 - val_loss: 98.4971\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 61418.8633 - val_loss: 94.4565\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 68701.9766 - val_loss: 93.0555\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 68351.7266 - val_loss: 93.8701\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30421.5723 - val_loss: 96.6058\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2388.1191 - val_loss: 100.3892\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 78933.9844 - val_loss: 100.8245\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 89404.0000 - val_loss: 99.8860\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 64765.0508 - val_loss: 97.9393\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1650.5797 - val_loss: 95.8720\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 48334.4609 - val_loss: 95.5904\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46233.5039 - val_loss: 96.8918\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16379.9766 - val_loss: 98.9033\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 47436.5117 - val_loss: 99.2510\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40987.1758 - val_loss: 98.2487\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4004.5396 - val_loss: 96.9445\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 44692.8242 - val_loss: 96.3515\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42652.3633 - val_loss: 97.4978\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5359.0146 - val_loss: 98.7401\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27536.2852 - val_loss: 99.0324\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27351.2188 - val_loss: 98.2230\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4740.0576 - val_loss: 98.3049\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11483.3848 - val_loss: 98.3725\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 856.2834 - val_loss: 98.2817\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2227.1489 - val_loss: 97.6737\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16658.8750 - val_loss: 98.0178\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2534.0676 - val_loss: 98.4865\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33218.6523 - val_loss: 99.3299\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23997.5254 - val_loss: 98.6625\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1964.0835 - val_loss: 98.3809\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 999.3702 - val_loss: 97.8157\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16856.8574 - val_loss: 98.1669\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2787.7625 - val_loss: 98.6341\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31853.5293 - val_loss: 99.4645\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22995.5000 - val_loss: 98.8117\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11098.7451 - val_loss: 98.2702\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3435.8970 - val_loss: 98.4953\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10768.7734 - val_loss: 98.3659\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4066.8701 - val_loss: 99.0570\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29242.5781 - val_loss: 99.4829\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12268.2549 - val_loss: 98.7437\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13554.7715 - val_loss: 98.0667\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21914.7715 - val_loss: 98.1686\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff75bb15e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:31:06.883 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff75bb15e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 15 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 150372.3281 - val_loss: 89.4208\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 293121.5312 - val_loss: 88.9616\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 140701.2031 - val_loss: 84.7663\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 58528.5391 - val_loss: 79.0198\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 184921.5156 - val_loss: 79.4020\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 106895.3594 - val_loss: 84.6450\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 84719.7188 - val_loss: 85.6934\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 97842.1094 - val_loss: 83.6961\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39865.7930 - val_loss: 83.4476\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19865.9629 - val_loss: 86.4247\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 124682.6875 - val_loss: 86.9845\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 88482.2578 - val_loss: 84.9548\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51900.6445 - val_loss: 82.6696\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 111682.6953 - val_loss: 83.6495\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 53324.6211 - val_loss: 86.0863\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 70925.5859 - val_loss: 87.1761\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 38156.2383 - val_loss: 85.6251\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23406.8027 - val_loss: 85.5547\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29797.0664 - val_loss: 86.5189\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 39751.2500 - val_loss: 87.0893\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12866.0244 - val_loss: 85.9188\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28895.2930 - val_loss: 86.3259\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21848.2871 - val_loss: 86.2477\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28687.4629 - val_loss: 85.4969\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21633.6250 - val_loss: 86.5676\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17223.5781 - val_loss: 86.6342\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14242.5283 - val_loss: 87.7593\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 23850.0098 - val_loss: 88.3264\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 20783.4355 - val_loss: 88.0309\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7908.7329 - val_loss: 88.6004\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10408.7969 - val_loss: 89.0607\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10626.4629 - val_loss: 88.7730\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33856.5039 - val_loss: 89.6554\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8011.0132 - val_loss: 89.0946\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31899.1484 - val_loss: 89.5001\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22522.9121 - val_loss: 90.4621\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 56429.3477 - val_loss: 89.7181\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6811.1030 - val_loss: 88.3531\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39142.7812 - val_loss: 88.2703\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9149.5283 - val_loss: 90.4731\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 115148.9844 - val_loss: 90.8796\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 114542.7891 - val_loss: 89.9800\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 32443.6113 - val_loss: 88.2759\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 61267.3750 - val_loss: 87.3385\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 67325.7188 - val_loss: 88.3530\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6010.2490 - val_loss: 89.5001\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36000.3633 - val_loss: 89.0457\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10463.7949 - val_loss: 88.5075\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20738.7969 - val_loss: 89.5682\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 36262.1562 - val_loss: 89.6166\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7527003a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:31:15.030 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7527003a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 16 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 93.7180 - val_loss: 77.0746\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 53.4814 - val_loss: 46.5865\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 48.9195 - val_loss: 39.4393\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 38.3392 - val_loss: 41.9863\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31.8353 - val_loss: 32.3793\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26.0680 - val_loss: 15.9261\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22.0491 - val_loss: 19.7599\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20.8829 - val_loss: 8.2238\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.3298 - val_loss: 7.6391\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.0937 - val_loss: 6.9884\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.1016 - val_loss: 4.6738\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.2565 - val_loss: 4.2991\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.6646 - val_loss: 3.2430\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.7644 - val_loss: 3.8273\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.1938 - val_loss: 2.1204\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.9425 - val_loss: 2.1590\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.3312 - val_loss: 2.2441\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.8847 - val_loss: 2.9442\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.6146 - val_loss: 2.4499\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.7151 - val_loss: 2.3357\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.9071 - val_loss: 2.2743\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.7359 - val_loss: 2.4139\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.3383 - val_loss: 2.4613\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.2433 - val_loss: 2.4624\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.1047 - val_loss: 3.0721\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9794 - val_loss: 2.2088\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.5149 - val_loss: 2.7617\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.5819 - val_loss: 2.1033\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.0026 - val_loss: 2.0186\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.4796 - val_loss: 1.6737\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.2600 - val_loss: 2.0937\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.8816 - val_loss: 2.0881\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.9069 - val_loss: 1.9573\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.1697 - val_loss: 1.8213\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.1395 - val_loss: 5.0400\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.1072 - val_loss: 5.2393\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.5820 - val_loss: 2.1850\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.1288 - val_loss: 1.9586\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0593 - val_loss: 2.0671\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8054 - val_loss: 1.9667\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.6637 - val_loss: 2.1529\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.6937 - val_loss: 2.2165\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3210 - val_loss: 1.9237\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3061 - val_loss: 1.9859\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.1472 - val_loss: 1.8426\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4311 - val_loss: 3.4690\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.3689 - val_loss: 2.0401\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0001 - val_loss: 2.4618\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.8035 - val_loss: 1.9906\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.6387 - val_loss: 2.2010\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7a8fc8160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:31:23.107 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7a8fc8160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 17 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 72.6897 - val_loss: 52.5369\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27.2205 - val_loss: 13.6335\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 25.8667 - val_loss: 23.6601\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19.1306 - val_loss: 32.1989\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.4666 - val_loss: 23.2963\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.9742 - val_loss: 13.9214\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.5778 - val_loss: 18.8152\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 12.6566 - val_loss: 15.7921\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.5444 - val_loss: 11.5115\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.2870 - val_loss: 10.4277\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.2695 - val_loss: 5.0607\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.8608 - val_loss: 6.2679\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.0042 - val_loss: 2.1339\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5830 - val_loss: 2.5263\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.3085 - val_loss: 1.7462\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.7502 - val_loss: 4.2017\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2675 - val_loss: 1.8647\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.6676 - val_loss: 2.3312\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3829 - val_loss: 2.1523\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1912 - val_loss: 3.6197\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4015 - val_loss: 2.3481\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.7562 - val_loss: 2.4261\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5380 - val_loss: 2.6295\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.1147 - val_loss: 2.5998\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9712 - val_loss: 3.2489\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5113 - val_loss: 2.7841\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8234 - val_loss: 2.7979\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.6731 - val_loss: 2.5659\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.5918 - val_loss: 2.7702\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8201 - val_loss: 3.6015\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.3193 - val_loss: 3.2754\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.1926 - val_loss: 2.4997\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.2802 - val_loss: 2.5074\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.0589 - val_loss: 2.4043\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.0436 - val_loss: 2.6452\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.0800 - val_loss: 2.3387\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1153 - val_loss: 2.4772\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.0122 - val_loss: 2.7288\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7088 - val_loss: 2.3146\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.9157 - val_loss: 2.1960\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.0837 - val_loss: 2.9779\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.0194 - val_loss: 2.2973\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.5568 - val_loss: 2.4661\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.6222 - val_loss: 2.5061\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.6265 - val_loss: 2.2353\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.5918 - val_loss: 3.1239\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.8306 - val_loss: 2.9611\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.0206 - val_loss: 4.1424\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8632 - val_loss: 2.4325\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.1686 - val_loss: 1.9266\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7812ed3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:31:31.160 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7812ed3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 18 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 167572.2188 - val_loss: 100.4733\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 68023.0234 - val_loss: 106.4427\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27918.6621 - val_loss: 99.1114\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 123051.2734 - val_loss: 97.0504\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 105475.4141 - val_loss: 98.0148\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 85243.2891 - val_loss: 103.5203\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 83833.3984 - val_loss: 105.4550\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 77669.6484 - val_loss: 103.6517\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 43658.6289 - val_loss: 100.1707\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 59287.4531 - val_loss: 99.0669\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 48315.4688 - val_loss: 100.2009\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17908.5703 - val_loss: 103.2086\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 75170.5625 - val_loss: 104.4091\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 71375.9375 - val_loss: 102.4029\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5204.1514 - val_loss: 100.8652\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 46411.9648 - val_loss: 99.2482\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51493.8281 - val_loss: 99.5908\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31072.9375 - val_loss: 101.5037\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 46005.4141 - val_loss: 102.4051\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 36258.2812 - val_loss: 100.5721\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24782.8477 - val_loss: 99.8626\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19223.1250 - val_loss: 101.0044\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22425.5469 - val_loss: 101.1218\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13328.3027 - val_loss: 99.8941\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37921.3008 - val_loss: 99.3424\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22367.3867 - val_loss: 100.7974\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8100.1758 - val_loss: 100.9446\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12597.2832 - val_loss: 99.9849\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20295.1484 - val_loss: 100.0950\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9732.5225 - val_loss: 101.5275\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 45637.7109 - val_loss: 101.8855\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32745.6309 - val_loss: 101.1941\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10138.4873 - val_loss: 100.0813\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 741.4302 - val_loss: 100.8615\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15215.6875 - val_loss: 100.8647\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15988.4180 - val_loss: 99.9071\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17250.6016 - val_loss: 99.9608\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5557.9829 - val_loss: 101.0937\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39603.1445 - val_loss: 101.4681\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28737.9629 - val_loss: 100.8686\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 10461.5762 - val_loss: 98.8248\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 37539.8867 - val_loss: 98.3729\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 50791.0938 - val_loss: 98.8295\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 34911.3633 - val_loss: 100.0621\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11659.0801 - val_loss: 100.2335\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5062.4463 - val_loss: 99.5171\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 13262.8477 - val_loss: 99.6224\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 8739.1846 - val_loss: 100.2093\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 20084.9531 - val_loss: 100.4517\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10039.7705 - val_loss: 99.7954\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b162e310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:31:40.104 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b162e310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 19 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 175308.3281 - val_loss: 87.1999\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 87755.2266 - val_loss: 99.8949\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 98899.5938 - val_loss: 102.1435\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 109071.6562 - val_loss: 101.4639\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 41745.7109 - val_loss: 97.3651\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 54569.3438 - val_loss: 93.4465\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 49129.5898 - val_loss: 95.1769\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11092.0820 - val_loss: 98.3415\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 52701.6562 - val_loss: 98.9201\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 53065.1758 - val_loss: 98.4165\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 42076.7148 - val_loss: 94.8838\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 40435.6641 - val_loss: 94.1530\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44500.3242 - val_loss: 95.5078\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12258.1953 - val_loss: 97.4932\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 33252.5625 - val_loss: 97.7053\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27151.9316 - val_loss: 97.1503\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8648.5137 - val_loss: 96.0805\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6521.0176 - val_loss: 97.1081\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29692.8809 - val_loss: 97.5527\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14614.0459 - val_loss: 96.6470\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3405.4465 - val_loss: 93.4416\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 85997.8359 - val_loss: 92.4828\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 88194.6406 - val_loss: 93.7851\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 57660.8242 - val_loss: 95.6033\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10576.2246 - val_loss: 97.6518\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29928.9180 - val_loss: 98.2479\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 36328.8984 - val_loss: 98.0308\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14693.5166 - val_loss: 96.0039\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17891.4844 - val_loss: 95.5629\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29334.4746 - val_loss: 96.2285\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6686.2954 - val_loss: 97.4993\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38409.9258 - val_loss: 98.2707\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27565.1172 - val_loss: 96.9974\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8664.0303 - val_loss: 96.5909\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6247.9673 - val_loss: 97.5514\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23289.8691 - val_loss: 97.6140\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13108.3320 - val_loss: 96.6464\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6653.8628 - val_loss: 96.7386\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3880.3997 - val_loss: 97.2553\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7608.9795 - val_loss: 97.0997\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2344.1147 - val_loss: 96.5433\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26733.6035 - val_loss: 96.0027\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14790.1270 - val_loss: 97.3521\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27052.2480 - val_loss: 98.0278\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23659.6641 - val_loss: 97.2640\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5892.6260 - val_loss: 96.9335\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 416.3539 - val_loss: 96.4534\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15746.5391 - val_loss: 96.8185\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2502.5083 - val_loss: 97.9792\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 40028.0391 - val_loss: 98.5128\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ce0763a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:31:48.290 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ce0763a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 20 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 58104.3438 - val_loss: 116.4086\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 196670.8906 - val_loss: 119.3581\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 110520.9766 - val_loss: 114.2660\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 663.7169 - val_loss: 108.9009\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12220.2061 - val_loss: 108.0153\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12036.3359 - val_loss: 108.1722\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9214.9482 - val_loss: 107.0761\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23855.1504 - val_loss: 107.1113\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1331.6102 - val_loss: 109.4078\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 61663.3633 - val_loss: 110.4290\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 45807.0508 - val_loss: 108.9960\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4495.3765 - val_loss: 106.9225\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4650.4854 - val_loss: 107.7568\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38186.0312 - val_loss: 108.6675\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18602.4043 - val_loss: 105.8240\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29098.4121 - val_loss: 105.0752\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31094.1836 - val_loss: 105.7783\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2864.3499 - val_loss: 108.3628\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44107.1328 - val_loss: 108.8953\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 46235.8047 - val_loss: 107.3533\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14256.0889 - val_loss: 104.7235\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 43760.1602 - val_loss: 103.9723\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39259.7188 - val_loss: 105.3858\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10248.0518 - val_loss: 108.0094\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 69227.5000 - val_loss: 109.0174\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51316.8867 - val_loss: 107.2232\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23005.9785 - val_loss: 104.2304\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 43499.4180 - val_loss: 103.1578\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 45802.8867 - val_loss: 103.5884\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23162.6035 - val_loss: 105.4368\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30519.4766 - val_loss: 106.2936\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26175.1035 - val_loss: 104.8727\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 913.5299 - val_loss: 105.1007\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6467.2432 - val_loss: 104.2733\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11985.2666 - val_loss: 104.7443\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 2147.4731 - val_loss: 104.2674\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 20665.5176 - val_loss: 104.1197\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6154.1421 - val_loss: 105.1666\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16186.0840 - val_loss: 105.4222\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15552.1797 - val_loss: 104.4282\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16573.0137 - val_loss: 104.1732\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2134.3477 - val_loss: 104.9750\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37177.7070 - val_loss: 106.1894\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27635.7656 - val_loss: 105.4023\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10377.0771 - val_loss: 102.9750\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 57243.8281 - val_loss: 101.8480\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 59030.7578 - val_loss: 102.0302\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29531.7559 - val_loss: 103.3847\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18209.5742 - val_loss: 106.0174\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 62441.3672 - val_loss: 106.8983\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79bcdab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:31:56.792 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79bcdab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 21 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 85.6822 - val_loss: 72.9048\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 45.7571 - val_loss: 37.1286\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 42.6645 - val_loss: 34.7336\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34.3378 - val_loss: 39.8607\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30.5580 - val_loss: 30.9506\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25.9834 - val_loss: 13.5900\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23.0787 - val_loss: 20.2683\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19.8071 - val_loss: 10.2732\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.9055 - val_loss: 5.7401\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.7264 - val_loss: 2.8570\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.1140 - val_loss: 3.3742\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.8705 - val_loss: 4.1997\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.4516 - val_loss: 3.9015\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.6798 - val_loss: 6.2604\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.7169 - val_loss: 3.7297\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.3077 - val_loss: 5.0949\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.4451 - val_loss: 3.7011\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.2802 - val_loss: 3.3663\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.1870 - val_loss: 4.7237\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.6139 - val_loss: 2.8245\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.2879 - val_loss: 3.0305\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.1128 - val_loss: 4.6364\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.1250 - val_loss: 3.0548\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.9639 - val_loss: 3.5235\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2255 - val_loss: 2.5196\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.1823 - val_loss: 2.7860\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.8089 - val_loss: 3.1201\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.7617 - val_loss: 2.3457\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4306 - val_loss: 2.3611\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.5434 - val_loss: 3.0718\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.6341 - val_loss: 4.0886\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.6624 - val_loss: 1.8721\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.9196 - val_loss: 2.8406\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5868 - val_loss: 2.2473\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.1310 - val_loss: 2.3457\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.6057 - val_loss: 3.6789\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.0137 - val_loss: 2.5110\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.5650 - val_loss: 2.6155\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.0799 - val_loss: 2.4303\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.7731 - val_loss: 2.4823\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.5235 - val_loss: 2.6526\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.9973 - val_loss: 2.4741\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3765 - val_loss: 2.4059\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.5931 - val_loss: 2.3438\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.0824 - val_loss: 2.1725\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.4272 - val_loss: 3.2481\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.4225 - val_loss: 2.6601\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.1811 - val_loss: 2.3598\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3985 - val_loss: 2.2032\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.9543 - val_loss: 2.3941\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff793ee8d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:32:04.724 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff793ee8d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 22 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 140559.8594 - val_loss: 102.8913\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 65084.9531 - val_loss: 106.1483\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30749.6816 - val_loss: 99.0529\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 84773.8516 - val_loss: 97.4657\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 71660.5859 - val_loss: 101.3431\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17130.1172 - val_loss: 102.1139\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7034.1826 - val_loss: 98.2032\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 69645.8984 - val_loss: 97.6456\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 54581.3477 - val_loss: 99.8183\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22399.4746 - val_loss: 104.1795\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 69743.2266 - val_loss: 105.1215\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 63804.5742 - val_loss: 103.7857\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30781.0566 - val_loss: 100.4459\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19763.3613 - val_loss: 99.5393\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22652.5781 - val_loss: 101.2335\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8224.3340 - val_loss: 100.3505\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12146.0498 - val_loss: 100.6506\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9395.2842 - val_loss: 100.6039\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 654.0568 - val_loss: 101.7286\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19820.9062 - val_loss: 100.6254\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9057.4473 - val_loss: 100.5255\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24705.2559 - val_loss: 101.6503\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 15074.7891 - val_loss: 99.1856\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 31217.2871 - val_loss: 98.8379\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 27750.6621 - val_loss: 99.4039\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11532.9590 - val_loss: 100.8036\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 808.0734 - val_loss: 100.7647\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11269.9033 - val_loss: 100.2961\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8615.1914 - val_loss: 100.2479\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16516.7305 - val_loss: 101.0018\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10400.6084 - val_loss: 99.7298\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20943.6035 - val_loss: 99.4243\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11761.9062 - val_loss: 100.3571\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23069.8809 - val_loss: 101.1965\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4664.4312 - val_loss: 99.8690\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39759.4727 - val_loss: 98.1761\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37875.0195 - val_loss: 99.7801\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8813.6689 - val_loss: 100.3141\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7722.4966 - val_loss: 99.9554\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 807.3061 - val_loss: 101.3550\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 41751.9180 - val_loss: 101.9835\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24226.1836 - val_loss: 100.8017\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2196.4919 - val_loss: 99.7173\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9908.4355 - val_loss: 99.9476\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1579.8885 - val_loss: 101.7047\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39564.4062 - val_loss: 101.8470\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36005.7617 - val_loss: 101.4256\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22548.9609 - val_loss: 99.3208\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21183.3691 - val_loss: 98.7554\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28271.9844 - val_loss: 98.8892\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7917d35e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:32:13.078 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7917d35e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 23 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 85.8076 - val_loss: 63.1925\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36.1320 - val_loss: 17.2105\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33.4534 - val_loss: 19.1841\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.0804 - val_loss: 30.3024\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22.1140 - val_loss: 28.2589\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19.2671 - val_loss: 19.6513\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17.7502 - val_loss: 13.1550\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.8770 - val_loss: 15.2362\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.5881 - val_loss: 12.9372\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.5576 - val_loss: 11.1450\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.2304 - val_loss: 5.0564\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.7204 - val_loss: 4.4334\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.1470 - val_loss: 5.6612\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9643 - val_loss: 2.4768\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.6954 - val_loss: 4.1447\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.6994 - val_loss: 4.3735\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.1055 - val_loss: 3.5826\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.0443 - val_loss: 4.4749\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6115 - val_loss: 3.4430\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4116 - val_loss: 4.8853\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.4840 - val_loss: 3.1808\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.1476 - val_loss: 3.8727\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.9719 - val_loss: 3.5739\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7333 - val_loss: 3.3461\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7385 - val_loss: 3.5684\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.6615 - val_loss: 2.5820\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.6153 - val_loss: 2.6506\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2894 - val_loss: 4.5801\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5187 - val_loss: 2.4026\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2589 - val_loss: 4.0349\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9342 - val_loss: 1.7488\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.9613 - val_loss: 3.9137\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9044 - val_loss: 2.1418\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3410 - val_loss: 3.5457\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.7480 - val_loss: 2.6319\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4046 - val_loss: 2.0915\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1243 - val_loss: 2.7725\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0813 - val_loss: 2.3264\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.0276 - val_loss: 3.0003\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0010 - val_loss: 2.4138\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.7334 - val_loss: 2.6867\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.5508 - val_loss: 1.7767\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.4567 - val_loss: 3.0861\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.4241 - val_loss: 2.0611\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.6309 - val_loss: 4.3284\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.4897 - val_loss: 1.7629\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.1921 - val_loss: 2.3993\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2502 - val_loss: 2.5407\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0762 - val_loss: 2.2397\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2171 - val_loss: 1.7366\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b7513e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:32:21.396 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b7513e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 24 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 2707541.5000 - val_loss: 102.9702\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4548538.0000 - val_loss: 101.2670\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4392923.0000 - val_loss: 100.4688\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1473110.1250 - val_loss: 99.5435\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 783959.6250 - val_loss: 98.3892\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 809848.5000 - val_loss: 97.3161\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 646992.5000 - val_loss: 96.3873\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 204086.8125 - val_loss: 95.4775\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 622487.8125 - val_loss: 94.7168\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 601091.8125 - val_loss: 93.7765\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 560190.5000 - val_loss: 92.8977\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 680577.0625 - val_loss: 92.2020\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 558366.1250 - val_loss: 91.3250\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 795823.0000 - val_loss: 90.5445\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1139634.2500 - val_loss: 89.7945\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 524708.5000 - val_loss: 89.2022\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 299446.3750 - val_loss: 88.5331\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 142038.7188 - val_loss: 87.9101\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 438041.5938 - val_loss: 87.3070\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 125852.2109 - val_loss: 86.6226\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 450079.6250 - val_loss: 85.9486\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 724545.6250 - val_loss: 85.4472\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 956986.3125 - val_loss: 84.8445\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 514277.5312 - val_loss: 84.2983\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 267660.4375 - val_loss: 83.8446\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 299679.8750 - val_loss: 83.2361\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 357549.9375 - val_loss: 82.6448\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 97073.7266 - val_loss: 82.1113\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 270434.1875 - val_loss: 81.5968\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 246419.5781 - val_loss: 81.1262\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 79449.0312 - val_loss: 80.6645\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 363260.1562 - val_loss: 80.0943\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 248796.6094 - val_loss: 79.6202\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 243583.0469 - val_loss: 79.1656\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 200099.0312 - val_loss: 78.6198\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 293379.4688 - val_loss: 78.1347\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 329530.7188 - val_loss: 77.6707\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 234735.2344 - val_loss: 77.2390\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 202786.2500 - val_loss: 76.8736\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 508666.0938 - val_loss: 76.4633\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 667242.5000 - val_loss: 76.0571\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 635200.4375 - val_loss: 75.8758\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 627717.0625 - val_loss: 75.6781\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 617710.1250 - val_loss: 75.5644\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 596346.8125 - val_loss: 75.4222\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 332056.8750 - val_loss: 75.2877\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 104200.1250 - val_loss: 75.0539\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 207501.9375 - val_loss: 74.7670\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 213144.6719 - val_loss: 74.5282\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 69433.7109 - val_loss: 74.2111\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7eafc1d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:32:29.752 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7eafc1d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 25 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 15494.8174 - val_loss: 93.4126\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 249589.5469 - val_loss: 92.6248\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 200083.6250 - val_loss: 97.2875\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 73525.1172 - val_loss: 101.4363\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 67121.9062 - val_loss: 103.4606\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36929.2812 - val_loss: 101.9923\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32496.5918 - val_loss: 100.2090\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21545.3574 - val_loss: 101.1850\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7540.6865 - val_loss: 101.4081\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3807.3718 - val_loss: 100.5896\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37283.2383 - val_loss: 100.2421\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19935.1738 - val_loss: 101.0163\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4254.6035 - val_loss: 101.6538\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11649.0654 - val_loss: 100.7654\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22405.9863 - val_loss: 100.9449\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2811.4790 - val_loss: 102.1277\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 53500.0078 - val_loss: 102.8479\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34532.1133 - val_loss: 101.6715\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4186.2393 - val_loss: 101.1000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2993.6248 - val_loss: 101.5700\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17662.5391 - val_loss: 101.5396\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11088.7998 - val_loss: 100.3734\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33768.6055 - val_loss: 100.3283\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24995.5000 - val_loss: 101.5940\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29518.0059 - val_loss: 101.8902\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23636.7402 - val_loss: 100.5907\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21062.7129 - val_loss: 100.6138\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9964.4141 - val_loss: 101.0745\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9033.9512 - val_loss: 101.4968\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14655.2383 - val_loss: 100.9983\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6471.2202 - val_loss: 101.1499\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7827.8569 - val_loss: 100.9672\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14445.1797 - val_loss: 100.7896\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4994.9951 - val_loss: 101.9073\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 38108.9180 - val_loss: 102.1168\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31876.7969 - val_loss: 101.1990\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8479.8955 - val_loss: 100.9163\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 982.3608 - val_loss: 101.9174\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 38392.0117 - val_loss: 102.0797\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33071.1523 - val_loss: 101.0553\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7046.1548 - val_loss: 100.9899\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3413.8428 - val_loss: 100.6938\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12612.4199 - val_loss: 100.9157\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4903.6772 - val_loss: 100.9168\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13424.8574 - val_loss: 100.6413\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8210.7520 - val_loss: 101.1051\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18738.0430 - val_loss: 101.4350\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14767.9111 - val_loss: 100.4405\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24782.0000 - val_loss: 100.3608\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18666.4492 - val_loss: 101.3244\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff82184f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:32:38.047 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff82184f040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 26 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 170644.7188 - val_loss: 76.5653\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 26429.1836 - val_loss: 87.2354\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7519.6445 - val_loss: 82.3936\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 88048.2656 - val_loss: 82.5930\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 68119.8359 - val_loss: 84.4939\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 49928.8438 - val_loss: 91.7759\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 59106.8477 - val_loss: 92.8041\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 79828.1094 - val_loss: 92.5344\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46205.7188 - val_loss: 88.2484\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28793.7344 - val_loss: 86.6889\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32498.6094 - val_loss: 87.7198\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3442.4524 - val_loss: 88.6026\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6981.7876 - val_loss: 89.6918\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17440.2246 - val_loss: 89.0835\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3857.5781 - val_loss: 89.9971\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18890.9590 - val_loss: 89.3062\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 11668.8672 - val_loss: 89.2023\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5648.5806 - val_loss: 89.4905\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1734.3615 - val_loss: 87.5323\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46156.3867 - val_loss: 87.6567\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29107.3379 - val_loss: 89.6337\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8039.7202 - val_loss: 89.9222\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1154.8389 - val_loss: 89.2300\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10267.8623 - val_loss: 88.8278\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22866.5801 - val_loss: 89.1655\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17697.5391 - val_loss: 90.6093\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12998.8135 - val_loss: 88.1195\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35805.1797 - val_loss: 88.3811\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32700.7930 - val_loss: 90.1852\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 12563.2646 - val_loss: 90.5361\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3771.1829 - val_loss: 90.6336\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19538.4199 - val_loss: 91.0295\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6311.1357 - val_loss: 88.5575\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45850.0195 - val_loss: 88.1850\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 46949.0625 - val_loss: 89.4925\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11464.6982 - val_loss: 91.1307\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22643.4531 - val_loss: 91.9528\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22059.6348 - val_loss: 90.7162\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5365.2598 - val_loss: 91.2795\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7994.2705 - val_loss: 90.7868\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9946.1709 - val_loss: 91.1374\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 442.5845 - val_loss: 90.3073\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26579.1875 - val_loss: 90.2318\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19811.7754 - val_loss: 92.7216\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37666.8398 - val_loss: 93.2214\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38638.6484 - val_loss: 92.4158\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5229.6836 - val_loss: 91.4144\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22746.5684 - val_loss: 90.4800\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30211.6680 - val_loss: 90.8424\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20867.1621 - val_loss: 92.7407\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff84a22ad30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:32:46.032 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff84a22ad30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 27 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 91978.9766 - val_loss: 96.9638\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 88297.6875 - val_loss: 102.3709\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24613.7168 - val_loss: 98.1239\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23649.5254 - val_loss: 95.6003\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46075.5820 - val_loss: 97.1460\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16481.3828 - val_loss: 100.8522\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 57034.4883 - val_loss: 101.1274\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47574.7148 - val_loss: 99.2035\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14341.3291 - val_loss: 98.4154\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17199.1367 - val_loss: 99.4229\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12427.6123 - val_loss: 98.6476\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1162.6001 - val_loss: 98.5510\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9666.4033 - val_loss: 99.7555\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17921.9590 - val_loss: 99.2192\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6696.8218 - val_loss: 99.1898\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10620.1299 - val_loss: 99.4999\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6369.4512 - val_loss: 98.5508\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16840.1758 - val_loss: 98.5458\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10225.6885 - val_loss: 100.6213\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33391.5547 - val_loss: 100.3804\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20731.6855 - val_loss: 98.5157\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12766.5273 - val_loss: 98.8660\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4135.6821 - val_loss: 99.8522\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 35568.5664 - val_loss: 100.8087\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24496.6406 - val_loss: 98.3904\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44925.9688 - val_loss: 97.2405\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 40098.8281 - val_loss: 99.7340\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6832.2983 - val_loss: 99.7315\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4774.6675 - val_loss: 99.0872\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28008.3027 - val_loss: 98.4130\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17871.3789 - val_loss: 100.5273\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38870.9180 - val_loss: 101.3871\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32219.9746 - val_loss: 100.8044\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 562.3701 - val_loss: 98.6660\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22848.5078 - val_loss: 97.9966\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44644.1953 - val_loss: 97.9942\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37791.7617 - val_loss: 100.3380\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11447.8037 - val_loss: 100.5876\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15755.1631 - val_loss: 99.6415\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9292.3721 - val_loss: 100.0099\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1989.4432 - val_loss: 99.3367\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21276.7559 - val_loss: 99.2868\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9085.6963 - val_loss: 101.1625\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34192.6992 - val_loss: 101.5175\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35439.1758 - val_loss: 101.1566\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13011.4922 - val_loss: 98.7121\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 54688.4531 - val_loss: 97.6661\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 46050.4531 - val_loss: 99.3997\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2967.8325 - val_loss: 100.5217\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33913.1055 - val_loss: 101.6084\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff86c155040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:32:54.384 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff86c155040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 28 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 83994.9062 - val_loss: 108.1390\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 148646.2969 - val_loss: 106.2564\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 95317.9922 - val_loss: 98.5687\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 59218.8164 - val_loss: 95.7997\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38272.7539 - val_loss: 100.6531\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 78074.1250 - val_loss: 103.0055\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 58660.5078 - val_loss: 99.6438\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 17073.0039 - val_loss: 94.5151\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 78455.0234 - val_loss: 93.4724\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 81383.3125 - val_loss: 95.0208\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35160.0195 - val_loss: 97.1656\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 350.7620 - val_loss: 101.0210\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 100635.8438 - val_loss: 102.9943\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 86642.2734 - val_loss: 101.3874\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 66232.9375 - val_loss: 97.9081\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 482.4052 - val_loss: 94.3712\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 93842.2578 - val_loss: 93.0380\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 91273.4688 - val_loss: 93.8145\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 76624.2969 - val_loss: 96.4261\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15883.6709 - val_loss: 98.9859\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 46424.9961 - val_loss: 99.8555\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 47409.9609 - val_loss: 98.9378\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21888.8320 - val_loss: 97.2533\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18485.4395 - val_loss: 97.0461\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16792.3828 - val_loss: 98.2383\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20084.0059 - val_loss: 98.4170\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13786.7471 - val_loss: 96.8005\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31695.2539 - val_loss: 96.5786\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25204.9023 - val_loss: 97.7409\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 517.0938 - val_loss: 99.6696\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 64506.3945 - val_loss: 100.4570\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 60636.4023 - val_loss: 98.7706\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8235.7949 - val_loss: 97.3147\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10012.4248 - val_loss: 96.9429\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19631.6621 - val_loss: 97.5716\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3786.4412 - val_loss: 97.6440\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5553.7661 - val_loss: 97.9840\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8816.1318 - val_loss: 97.6928\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1860.6232 - val_loss: 98.1697\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12159.9521 - val_loss: 97.8020\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 745.4121 - val_loss: 96.2693\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34405.1250 - val_loss: 96.3565\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31518.4531 - val_loss: 96.7258\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17370.9688 - val_loss: 98.7213\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 47288.8359 - val_loss: 99.7382\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37914.4766 - val_loss: 99.0930\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22286.9648 - val_loss: 97.0689\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32328.2344 - val_loss: 96.3313\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37296.6367 - val_loss: 96.6002\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28949.9414 - val_loss: 98.0579\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8440b45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:02.779 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8440b45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 29 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 29934.8262 - val_loss: 112.8649\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 99824.0781 - val_loss: 113.7870\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 68785.3281 - val_loss: 106.8458\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 56191.1758 - val_loss: 106.3036\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21019.8438 - val_loss: 109.3744\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 51264.7070 - val_loss: 111.3809\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 44359.4336 - val_loss: 107.2993\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 40358.7148 - val_loss: 106.3131\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25201.4355 - val_loss: 109.6502\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 52384.9492 - val_loss: 110.8353\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36376.3008 - val_loss: 109.0238\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12599.8086 - val_loss: 104.3762\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 69079.2500 - val_loss: 104.0352\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 74722.3438 - val_loss: 104.8726\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 42730.9258 - val_loss: 108.0133\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19962.0957 - val_loss: 108.8659\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17245.8145 - val_loss: 106.2262\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39499.5039 - val_loss: 105.4049\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31397.5195 - val_loss: 108.7529\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 46985.0547 - val_loss: 109.5386\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 42642.2422 - val_loss: 108.0731\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12648.6738 - val_loss: 105.4166\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 58346.9688 - val_loss: 104.9305\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 49006.9648 - val_loss: 106.1622\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7933.4941 - val_loss: 107.9661\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45521.2148 - val_loss: 109.2140\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34948.9062 - val_loss: 107.2228\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15184.6250 - val_loss: 106.0295\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6612.7310 - val_loss: 107.9107\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 40265.5898 - val_loss: 108.3487\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34414.1953 - val_loss: 107.7709\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25759.6504 - val_loss: 105.0874\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30034.8262 - val_loss: 104.6258\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 42314.6250 - val_loss: 104.6994\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29197.7070 - val_loss: 106.6711\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16685.8867 - val_loss: 107.0633\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19681.9199 - val_loss: 106.0510\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8421.7744 - val_loss: 105.9257\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 721.8170 - val_loss: 105.5476\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16256.2207 - val_loss: 105.4812\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 438.9192 - val_loss: 107.0885\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 49569.0547 - val_loss: 108.0342\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42473.7773 - val_loss: 106.8814\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5541.7627 - val_loss: 105.5831\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35857.3008 - val_loss: 104.1923\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 32201.1504 - val_loss: 104.5349\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3436.2947 - val_loss: 105.8618\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25109.0254 - val_loss: 106.9000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39271.4688 - val_loss: 106.9028\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29564.7930 - val_loss: 105.2464\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7e0002310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:10.710 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7e0002310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 30 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 133176.7031 - val_loss: 61.5874\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 74591.3516 - val_loss: 73.3640\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 63233.0117 - val_loss: 67.1708\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 91976.2812 - val_loss: 65.4916\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 79088.8984 - val_loss: 73.3418\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 60476.2188 - val_loss: 76.7448\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36350.2266 - val_loss: 74.4560\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7744.6924 - val_loss: 68.8051\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 85148.3828 - val_loss: 68.8858\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 94127.1172 - val_loss: 71.6149\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 54280.8633 - val_loss: 75.9805\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8033.1816 - val_loss: 76.8816\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4669.4321 - val_loss: 75.9085\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 40804.1758 - val_loss: 74.8408\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20879.5410 - val_loss: 78.2811\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28349.2695 - val_loss: 79.2589\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30501.4375 - val_loss: 78.4964\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4988.2817 - val_loss: 77.9209\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2289.8574 - val_loss: 76.5860\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24814.0371 - val_loss: 77.7038\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1062.8440 - val_loss: 80.5550\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 68317.3125 - val_loss: 81.9572\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 51139.5781 - val_loss: 79.5045\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10081.9619 - val_loss: 78.3748\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4697.2983 - val_loss: 80.1931\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 43950.1016 - val_loss: 81.3473\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21856.2891 - val_loss: 79.2589\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11423.0752 - val_loss: 78.6829\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15267.5654 - val_loss: 79.9642\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24149.3828 - val_loss: 80.7942\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11193.3672 - val_loss: 77.9081\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34582.7109 - val_loss: 77.8947\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38449.9570 - val_loss: 79.5294\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4203.5469 - val_loss: 81.9826\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42308.9219 - val_loss: 82.9350\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39611.6133 - val_loss: 81.5770\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12713.2627 - val_loss: 78.8436\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 45590.0391 - val_loss: 78.7795\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42127.2852 - val_loss: 80.5381\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5898.1973 - val_loss: 82.9194\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 45851.3711 - val_loss: 84.1774\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 43750.7734 - val_loss: 82.6852\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3696.5144 - val_loss: 80.8042\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45371.5117 - val_loss: 79.9437\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 41712.5742 - val_loss: 81.6745\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2233.6523 - val_loss: 82.4734\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3336.8198 - val_loss: 82.9368\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3441.7312 - val_loss: 81.7971\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25342.3711 - val_loss: 82.1073\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9515.3467 - val_loss: 83.7406\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7c23380d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:18.545 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7c23380d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 31 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 90.6725 - val_loss: 68.9802\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 61.0801 - val_loss: 51.3816\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 48.9963 - val_loss: 55.2395\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 43.7474 - val_loss: 49.7775\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37.5434 - val_loss: 35.6233\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 32.5516 - val_loss: 27.7271\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28.5922 - val_loss: 28.4325\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25.0614 - val_loss: 18.1524\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21.8307 - val_loss: 12.5790\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20.4704 - val_loss: 6.6577\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19.6013 - val_loss: 10.2670\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17.7558 - val_loss: 4.4921\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17.6710 - val_loss: 5.7523\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.8354 - val_loss: 3.3101\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.4137 - val_loss: 6.3730\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.2782 - val_loss: 3.5870\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.7736 - val_loss: 4.6922\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.6279 - val_loss: 3.4385\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.1054 - val_loss: 5.3868\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.0856 - val_loss: 3.7427\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.0618 - val_loss: 3.1175\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.4904 - val_loss: 6.2699\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.3440 - val_loss: 2.9384\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.7387 - val_loss: 3.3914\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.2589 - val_loss: 3.5275\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.8830 - val_loss: 3.2150\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.8707 - val_loss: 4.3356\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.3737 - val_loss: 3.1839\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.2096 - val_loss: 3.4950\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.6412 - val_loss: 2.8709\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.5874 - val_loss: 3.3890\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.7080 - val_loss: 5.5431\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.4209 - val_loss: 2.9514\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.0001 - val_loss: 3.1381\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.9043 - val_loss: 3.9707\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.6636 - val_loss: 2.8836\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.4002 - val_loss: 2.5791\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5443 - val_loss: 4.1267\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.4081 - val_loss: 4.5564\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.6276 - val_loss: 3.8133\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5526 - val_loss: 2.3647\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0210 - val_loss: 4.3267\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0969 - val_loss: 4.2322\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.2601 - val_loss: 2.4029\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.8730 - val_loss: 3.9184\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.4922 - val_loss: 3.4124\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5825 - val_loss: 1.8408\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7192 - val_loss: 3.2825\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.3853 - val_loss: 4.7590\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5116 - val_loss: 3.1882\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b1d351f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:26.276 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b1d351f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 32 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 77.0251 - val_loss: 61.0260\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 43.9764 - val_loss: 31.4318\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 39.7649 - val_loss: 32.6994\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29.8377 - val_loss: 33.4254\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25.1951 - val_loss: 21.3892\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21.4828 - val_loss: 15.1100\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17.2847 - val_loss: 11.1497\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.4721 - val_loss: 9.4595\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.5059 - val_loss: 6.8850\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.0967 - val_loss: 7.6717\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.0481 - val_loss: 6.2118\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.2544 - val_loss: 5.8349\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.3495 - val_loss: 3.7084\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.8086 - val_loss: 5.9511\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.1789 - val_loss: 3.2467\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.3396 - val_loss: 3.8278\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.2703 - val_loss: 4.7147\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.2417 - val_loss: 3.2293\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.1984 - val_loss: 5.4266\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.4455 - val_loss: 3.7347\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.1481 - val_loss: 5.9419\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.0042 - val_loss: 2.6424\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.7353 - val_loss: 3.1476\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.8529 - val_loss: 3.8979\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.1881 - val_loss: 2.7718\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.9049 - val_loss: 2.4457\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8206 - val_loss: 2.0089\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.1301 - val_loss: 4.1575\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5784 - val_loss: 2.5452\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.9368 - val_loss: 3.3501\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.9216 - val_loss: 2.2556\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.1041 - val_loss: 4.6875\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0937 - val_loss: 1.8596\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.3615 - val_loss: 2.2113\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.8873 - val_loss: 4.5901\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.6210 - val_loss: 2.0707\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.6643 - val_loss: 1.9265\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3266 - val_loss: 2.8938\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4260 - val_loss: 2.9844\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2279 - val_loss: 2.0399\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.8225 - val_loss: 3.9299\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1702 - val_loss: 1.9778\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7910 - val_loss: 2.8440\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.1595 - val_loss: 2.4311\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.9571 - val_loss: 2.2968\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7408 - val_loss: 2.0004\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.3974 - val_loss: 2.5342\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.4386 - val_loss: 2.3551\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5302 - val_loss: 2.4582\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.2930 - val_loss: 2.9207\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79e4e4af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:34.181 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79e4e4af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 33 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 104.9459 - val_loss: 88.3776\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 59.6703 - val_loss: 53.8478\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 35.9736 - val_loss: 26.0081\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35.2384 - val_loss: 29.9711\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27.4642 - val_loss: 38.6198\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27.5831 - val_loss: 36.1382\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23.7658 - val_loss: 23.3433\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21.3709 - val_loss: 16.2838\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18.6102 - val_loss: 20.9562\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.9542 - val_loss: 14.5804\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.4144 - val_loss: 13.5549\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.6640 - val_loss: 6.7586\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.2615 - val_loss: 4.0923\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.4939 - val_loss: 2.9746\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.1162 - val_loss: 3.0437\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.2153 - val_loss: 2.9246\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.6621 - val_loss: 2.9371\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.7411 - val_loss: 3.1348\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.9969 - val_loss: 2.8551\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.4249 - val_loss: 3.6215\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5193 - val_loss: 2.3141\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.9160 - val_loss: 4.2646\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7095 - val_loss: 2.4309\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2731 - val_loss: 3.0963\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.5745 - val_loss: 2.7954\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.3153 - val_loss: 2.1165\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1838 - val_loss: 2.7575\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.1794 - val_loss: 2.1564\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1169 - val_loss: 2.1039\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.8404 - val_loss: 2.1548\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7239 - val_loss: 2.7048\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9267 - val_loss: 2.0640\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6895 - val_loss: 2.2092\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5759 - val_loss: 1.9806\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6463 - val_loss: 2.6424\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4572 - val_loss: 2.2811\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.4364 - val_loss: 2.0081\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.8663 - val_loss: 1.9788\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4409 - val_loss: 2.4674\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3958 - val_loss: 1.9591\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.6828 - val_loss: 2.2130\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.5041 - val_loss: 2.1829\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.2314 - val_loss: 1.9036\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3245 - val_loss: 2.7212\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3960 - val_loss: 2.0084\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3763 - val_loss: 3.1915\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.0669 - val_loss: 2.0197\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4324 - val_loss: 1.9702\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.0265 - val_loss: 3.3539\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.2378 - val_loss: 1.9195\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ac99c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:42.238 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ac99c820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 34 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 60053.8281 - val_loss: 114.8740\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 345483.5000 - val_loss: 112.3271\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 211949.0469 - val_loss: 106.2323\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34284.8242 - val_loss: 98.6106\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 243801.3750 - val_loss: 97.9181\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 224817.2031 - val_loss: 99.4015\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 111242.5547 - val_loss: 102.4379\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 26652.8867 - val_loss: 103.4507\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 36808.2266 - val_loss: 102.1765\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 48147.0430 - val_loss: 102.6240\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20638.5469 - val_loss: 103.9090\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27567.7988 - val_loss: 102.9560\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 56131.4141 - val_loss: 103.6898\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 12005.1377 - val_loss: 104.0488\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7155.0430 - val_loss: 103.1321\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6375.8760 - val_loss: 101.2243\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 68345.6875 - val_loss: 100.8860\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25220.7051 - val_loss: 101.2601\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8186.7778 - val_loss: 99.4959\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 80094.3516 - val_loss: 99.0916\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18609.2012 - val_loss: 100.6068\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 83785.8438 - val_loss: 101.9713\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 77748.1172 - val_loss: 101.2230\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 40328.4336 - val_loss: 99.6915\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50326.1875 - val_loss: 100.3091\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24547.6504 - val_loss: 101.4579\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 42348.8203 - val_loss: 101.3966\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11669.9248 - val_loss: 99.6930\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51966.3164 - val_loss: 99.8303\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17697.2969 - val_loss: 100.7674\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36210.0938 - val_loss: 100.7318\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29991.4004 - val_loss: 99.8599\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 48911.6406 - val_loss: 99.5648\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20320.0625 - val_loss: 100.7454\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8589.1904 - val_loss: 101.3510\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5611.5610 - val_loss: 101.7580\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 8565.0811 - val_loss: 101.8613\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 4959.8223 - val_loss: 102.0639\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 19227.0273 - val_loss: 102.3698\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6550.7173 - val_loss: 104.1736\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 103304.8906 - val_loss: 104.3207\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 74136.8281 - val_loss: 103.2250\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16127.5195 - val_loss: 102.1801\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 49632.6875 - val_loss: 102.3672\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10855.1025 - val_loss: 103.4754\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13738.2881 - val_loss: 103.2740\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24732.5879 - val_loss: 103.6769\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3598.0007 - val_loss: 103.7797\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21925.7227 - val_loss: 103.6059\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9136.4824 - val_loss: 103.4040\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff782307670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:50.534 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff782307670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 35 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 58.2128 - val_loss: 34.6540\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25.7881 - val_loss: 6.3704\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23.7412 - val_loss: 25.7239\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19.0211 - val_loss: 28.9389\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.8906 - val_loss: 17.1466\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.9103 - val_loss: 12.7392\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.7634 - val_loss: 17.3348\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.3080 - val_loss: 15.0985\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.1090 - val_loss: 5.7425\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.0372 - val_loss: 12.3494\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.1637 - val_loss: 4.4947\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.7587 - val_loss: 9.6288\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.6621 - val_loss: 3.0265\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.2932 - val_loss: 5.9609\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5477 - val_loss: 4.3745\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2761 - val_loss: 6.0125\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.1071 - val_loss: 3.7254\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.1939 - val_loss: 4.7844\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.0069 - val_loss: 3.6364\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.0550 - val_loss: 5.1654\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8361 - val_loss: 3.5793\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7214 - val_loss: 5.1220\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7563 - val_loss: 3.4788\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.4509 - val_loss: 4.2225\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3610 - val_loss: 4.5521\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.2934 - val_loss: 4.2132\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1818 - val_loss: 3.0847\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0509 - val_loss: 4.1155\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0263 - val_loss: 3.3254\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9652 - val_loss: 3.4784\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9055 - val_loss: 3.1613\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1450 - val_loss: 4.3080\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.7243 - val_loss: 2.3503\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7224 - val_loss: 4.1900\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6751 - val_loss: 2.3742\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4541 - val_loss: 2.6454\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2812 - val_loss: 2.5930\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2150 - val_loss: 4.0434\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4809 - val_loss: 1.7319\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.2582 - val_loss: 4.4019\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.1974 - val_loss: 2.2161\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4026 - val_loss: 4.3641\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.6752 - val_loss: 2.6850\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5005 - val_loss: 3.8547\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.1648 - val_loss: 1.7469\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9651 - val_loss: 1.7350\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.0553 - val_loss: 3.1916\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.0761 - val_loss: 1.5877\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8550 - val_loss: 2.7974\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6766 - val_loss: 1.5388\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff77ca73f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:33:58.437 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff77ca73f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 36 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 75755.7969 - val_loss: 123.0369\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 155669.5469 - val_loss: 125.3602\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 85560.0078 - val_loss: 120.8722\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3222.6074 - val_loss: 116.5623\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8639.0908 - val_loss: 116.2720\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18898.9668 - val_loss: 116.4116\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21244.0703 - val_loss: 115.5867\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3802.2036 - val_loss: 114.7317\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7079.7705 - val_loss: 115.5433\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17979.0488 - val_loss: 114.8899\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6315.3945 - val_loss: 112.8828\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45689.2383 - val_loss: 112.9439\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33326.7031 - val_loss: 114.6895\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 26640.7559 - val_loss: 114.6106\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16814.7715 - val_loss: 112.4241\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29950.6719 - val_loss: 112.5731\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21577.7012 - val_loss: 114.0788\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33725.5898 - val_loss: 114.2231\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15937.8271 - val_loss: 112.7459\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21369.8418 - val_loss: 111.9929\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16621.1113 - val_loss: 112.9245\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13418.6670 - val_loss: 112.7215\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5532.5337 - val_loss: 110.9880\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 43162.8867 - val_loss: 110.7767\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29235.7305 - val_loss: 111.8780\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20990.3418 - val_loss: 112.4997\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10674.4180 - val_loss: 110.4699\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 49872.3477 - val_loss: 109.5849\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 43722.5703 - val_loss: 110.3565\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15705.3916 - val_loss: 112.0673\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33581.2812 - val_loss: 112.2099\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34394.6953 - val_loss: 110.9320\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1939.3818 - val_loss: 110.6538\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8119.0938 - val_loss: 110.1687\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12842.0840 - val_loss: 110.0260\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6164.4204 - val_loss: 111.8690\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50888.7539 - val_loss: 112.0161\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51389.2227 - val_loss: 110.8419\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20403.4473 - val_loss: 109.5074\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14837.3604 - val_loss: 108.9976\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11838.4639 - val_loss: 109.6041\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13232.4941 - val_loss: 109.6192\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2356.4680 - val_loss: 109.0050\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20421.0312 - val_loss: 108.2921\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21852.9531 - val_loss: 108.9555\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2028.7445 - val_loss: 108.6372\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9324.3613 - val_loss: 108.9989\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8450.2910 - val_loss: 108.7780\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5549.5845 - val_loss: 108.9046\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3701.0359 - val_loss: 108.4157\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff76354aca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:34:06.333 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff76354aca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 37 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 66.8006 - val_loss: 42.2837\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23.1086 - val_loss: 3.0991\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22.3701 - val_loss: 16.8073\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.8515 - val_loss: 26.9338\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.0453 - val_loss: 17.6627\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.0434 - val_loss: 10.0629\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.4067 - val_loss: 12.3916\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.1254 - val_loss: 15.6737\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.1165 - val_loss: 11.1019\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.5574 - val_loss: 5.8217\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.4625 - val_loss: 7.9505\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.5520 - val_loss: 2.7200\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.6002 - val_loss: 2.6862\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.1098 - val_loss: 2.5172\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9789 - val_loss: 2.2970\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2265 - val_loss: 2.7749\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.7879 - val_loss: 2.3733\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7190 - val_loss: 2.6041\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.5535 - val_loss: 2.5310\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.4549 - val_loss: 2.7408\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3554 - val_loss: 2.2939\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.4425 - val_loss: 2.3015\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2708 - val_loss: 2.7141\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.0828 - val_loss: 2.1076\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2416 - val_loss: 2.1325\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2249 - val_loss: 2.9829\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2384 - val_loss: 2.4107\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.9702 - val_loss: 2.1872\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.1392 - val_loss: 2.0011\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.8957 - val_loss: 2.1590\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.7344 - val_loss: 2.0052\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.7345 - val_loss: 2.0654\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.5225 - val_loss: 2.3797\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.5982 - val_loss: 2.0160\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.5549 - val_loss: 2.2487\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.4754 - val_loss: 2.2428\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.7113 - val_loss: 1.9138\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6200 - val_loss: 2.7825\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.5779 - val_loss: 2.3990\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.5870 - val_loss: 1.9085\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.4033 - val_loss: 1.7085\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.2410 - val_loss: 1.8984\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.0124 - val_loss: 2.0189\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.1794 - val_loss: 1.9285\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.0708 - val_loss: 2.4994\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.2315 - val_loss: 1.8697\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.1269 - val_loss: 1.7878\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.9184 - val_loss: 1.7705\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.9709 - val_loss: 2.1921\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.8132 - val_loss: 1.7576\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff80deb71f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:34:14.259 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff80deb71f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 38 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 107.4098 - val_loss: 91.2290\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 63.2762 - val_loss: 55.1414\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 44.6737 - val_loss: 31.4635\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37.1645 - val_loss: 37.7156\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30.4862 - val_loss: 32.6062\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.2001 - val_loss: 15.2807\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.4527 - val_loss: 20.7658\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.1627 - val_loss: 12.3565\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.4025 - val_loss: 3.6008\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.3901 - val_loss: 10.8083\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.8808 - val_loss: 4.7089\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.4002 - val_loss: 9.9070\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.8325 - val_loss: 4.1083\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.6383 - val_loss: 4.7962\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.0651 - val_loss: 6.7191\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.6762 - val_loss: 6.1189\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.2839 - val_loss: 7.1274\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.7681 - val_loss: 5.4155\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.2248 - val_loss: 5.4703\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.7667 - val_loss: 5.6799\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.3537 - val_loss: 6.5542\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.4606 - val_loss: 4.2957\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.8787 - val_loss: 5.1563\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.5714 - val_loss: 4.7289\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.3343 - val_loss: 4.1334\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.9663 - val_loss: 6.6394\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.1183 - val_loss: 3.2419\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4936 - val_loss: 5.8106\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7909 - val_loss: 4.6955\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.6949 - val_loss: 3.6257\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7030 - val_loss: 5.8263\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3504 - val_loss: 4.7871\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5270 - val_loss: 5.0903\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1201 - val_loss: 3.3975\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0963 - val_loss: 5.3745\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0383 - val_loss: 5.4735\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0386 - val_loss: 3.0289\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2377 - val_loss: 6.1016\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9505 - val_loss: 4.2666\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.1001 - val_loss: 4.6600\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3136 - val_loss: 3.4397\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.8793 - val_loss: 6.4671\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0671 - val_loss: 3.1329\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7468 - val_loss: 5.1741\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2198 - val_loss: 3.4006\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5555 - val_loss: 4.7178\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6537 - val_loss: 3.5819\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.3546 - val_loss: 3.7471\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9602 - val_loss: 3.7513\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9968 - val_loss: 4.3119\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff77777f700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:34:22.258 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff77777f700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 39 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 28699.6641 - val_loss: 74.1669\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 619617.2500 - val_loss: 73.7098\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 385302.4062 - val_loss: 79.1452\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 133107.7344 - val_loss: 87.1808\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 104646.4766 - val_loss: 92.4583\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 141404.7031 - val_loss: 91.4157\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8207.8799 - val_loss: 89.1742\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 61491.5938 - val_loss: 88.4674\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 117731.2734 - val_loss: 89.4030\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 76839.8516 - val_loss: 93.2471\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 102636.0938 - val_loss: 93.7046\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 87703.8125 - val_loss: 92.5146\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27085.0254 - val_loss: 90.9588\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 57917.5273 - val_loss: 92.1385\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11593.3838 - val_loss: 93.2299\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34764.1914 - val_loss: 96.1464\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 59792.4414 - val_loss: 96.4391\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19082.2246 - val_loss: 93.5920\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 180470.1250 - val_loss: 92.9288\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 143308.5469 - val_loss: 94.2270\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 41376.4258 - val_loss: 97.4184\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 109958.6328 - val_loss: 98.6574\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 129585.6016 - val_loss: 96.6224\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24176.5078 - val_loss: 95.5898\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27942.5918 - val_loss: 96.6130\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21217.0938 - val_loss: 97.1334\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4847.0479 - val_loss: 98.0888\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 41408.4570 - val_loss: 97.3947\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24831.9844 - val_loss: 96.7588\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 46127.6758 - val_loss: 98.3149\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33471.9336 - val_loss: 99.8120\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 65195.0273 - val_loss: 98.4130\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35091.4141 - val_loss: 95.6321\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 44442.3867 - val_loss: 95.6627\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5189.0215 - val_loss: 96.7168\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51419.5742 - val_loss: 96.8077\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21091.9355 - val_loss: 94.6129\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 96665.0234 - val_loss: 93.8723\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 92933.3516 - val_loss: 95.9240\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5901.2026 - val_loss: 97.1648\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23288.4473 - val_loss: 95.7798\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 38918.7656 - val_loss: 95.1280\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5956.0386 - val_loss: 95.4888\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5594.5532 - val_loss: 95.7013\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6582.6562 - val_loss: 96.5196\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 35930.8867 - val_loss: 97.4209\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12521.9023 - val_loss: 96.1744\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 58431.8008 - val_loss: 95.3856\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50707.5000 - val_loss: 97.0373\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22896.9688 - val_loss: 97.2340\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff729db2550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:34:30.237 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff729db2550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 40 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 95.4747 - val_loss: 81.1563\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 71.4557 - val_loss: 62.3914\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 53.9762 - val_loss: 38.5589\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 43.1774 - val_loss: 23.3612\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 40.9224 - val_loss: 27.3884\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 34.7299 - val_loss: 17.3812\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33.8308 - val_loss: 20.2540\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 29.4348 - val_loss: 16.6428\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25.3582 - val_loss: 5.3275\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23.4155 - val_loss: 9.5651\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20.8197 - val_loss: 6.2097\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 18.0269 - val_loss: 6.8930\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 18.6059 - val_loss: 6.4595\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.4192 - val_loss: 4.0509\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.6917 - val_loss: 7.4480\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.3949 - val_loss: 4.0819\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.2988 - val_loss: 6.1581\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.9418 - val_loss: 4.0439\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.6664 - val_loss: 6.2224\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.5264 - val_loss: 7.5212\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.4346 - val_loss: 4.5715\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.3349 - val_loss: 6.5912\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.2400 - val_loss: 5.6614\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7471 - val_loss: 3.2842\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.7461 - val_loss: 2.5527\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.9636 - val_loss: 8.1885\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.5128 - val_loss: 3.4969\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.0189 - val_loss: 2.7745\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.2007 - val_loss: 4.3912\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.2069 - val_loss: 4.9299\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.3351 - val_loss: 4.7870\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.3429 - val_loss: 2.8013\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.4197 - val_loss: 7.0209\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.2965 - val_loss: 4.0356\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.7796 - val_loss: 2.1967\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.1878 - val_loss: 4.4066\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.1450 - val_loss: 5.5046\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.3414 - val_loss: 2.3887\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4822 - val_loss: 4.6728\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.9292 - val_loss: 3.6023\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7858 - val_loss: 3.8685\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.1305 - val_loss: 3.3638\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.0259 - val_loss: 4.8769\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8914 - val_loss: 2.7253\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2916 - val_loss: 5.1606\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7078 - val_loss: 2.7597\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8591 - val_loss: 3.6974\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.8717 - val_loss: 4.5395\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.1419 - val_loss: 4.1873\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.2524 - val_loss: 2.1625\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff738cb8c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:34:38.337 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff738cb8c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 41 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 152539.3438 - val_loss: 82.8401\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19963.9629 - val_loss: 94.5461\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 169910.1250 - val_loss: 96.9971\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 156855.2656 - val_loss: 96.6802\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 118042.1562 - val_loss: 92.3306\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 11339.9717 - val_loss: 91.1259\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 13835.6611 - val_loss: 91.8447\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 25016.1699 - val_loss: 92.9407\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23188.0371 - val_loss: 92.3076\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4567.5283 - val_loss: 92.4109\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4111.3369 - val_loss: 91.2274\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 42037.9258 - val_loss: 91.2507\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24749.2344 - val_loss: 93.3726\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 41275.0508 - val_loss: 93.9645\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34454.9648 - val_loss: 92.7518\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8322.9775 - val_loss: 92.9719\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8687.3672 - val_loss: 92.9006\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11531.4033 - val_loss: 92.6641\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10009.5186 - val_loss: 93.6148\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32151.8652 - val_loss: 94.0525\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13379.7441 - val_loss: 93.2337\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33490.2734 - val_loss: 92.1077\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34058.5742 - val_loss: 93.2657\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5054.1504 - val_loss: 93.3567\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5726.8154 - val_loss: 94.2108\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27419.0469 - val_loss: 94.3475\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18667.2168 - val_loss: 92.1516\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46231.5547 - val_loss: 92.1116\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 51789.3711 - val_loss: 93.1135\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 22062.0547 - val_loss: 94.8298\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26371.3418 - val_loss: 95.1479\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23871.7969 - val_loss: 94.3290\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2779.3938 - val_loss: 94.7097\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7986.1421 - val_loss: 94.0989\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12925.9668 - val_loss: 94.5975\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7269.7812 - val_loss: 94.5742\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21409.3301 - val_loss: 94.0443\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 16701.6816 - val_loss: 95.1184\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12137.1475 - val_loss: 94.9603\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3105.2183 - val_loss: 94.4236\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 23513.4277 - val_loss: 94.2505\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16709.6934 - val_loss: 95.2971\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11734.6680 - val_loss: 95.1471\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5533.3696 - val_loss: 94.1393\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34967.2852 - val_loss: 94.1892\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20075.5195 - val_loss: 94.9833\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4055.2505 - val_loss: 96.8471\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 59093.4219 - val_loss: 97.2531\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 60725.5312 - val_loss: 96.8377\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30929.4746 - val_loss: 95.9677\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7d2d7d700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:34:46.730 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7d2d7d700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 42 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 64112.8281 - val_loss: 121.6655\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 102497.1875 - val_loss: 117.6201\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 42751.5117 - val_loss: 113.6473\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8050.2178 - val_loss: 106.0043\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 122718.7734 - val_loss: 103.6406\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 112993.8438 - val_loss: 104.4198\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 64854.5469 - val_loss: 107.0864\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10094.0010 - val_loss: 111.1444\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 68260.6328 - val_loss: 112.6431\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 74557.5625 - val_loss: 111.9861\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 63866.1055 - val_loss: 107.9186\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28952.3691 - val_loss: 106.4670\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26389.9316 - val_loss: 107.3038\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1237.0262 - val_loss: 109.7953\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39142.9062 - val_loss: 110.3103\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 41717.0234 - val_loss: 109.7567\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24976.0957 - val_loss: 106.4806\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 43327.6172 - val_loss: 105.0065\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42927.8789 - val_loss: 106.2012\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17396.1152 - val_loss: 108.2127\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27637.1504 - val_loss: 108.2058\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22891.0254 - val_loss: 107.0699\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 845.6049 - val_loss: 104.8745\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39762.0742 - val_loss: 104.4726\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39393.8789 - val_loss: 105.0081\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23985.2773 - val_loss: 106.9356\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8964.8955 - val_loss: 107.0759\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11927.1299 - val_loss: 106.0216\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11522.4814 - val_loss: 106.3014\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1416.1713 - val_loss: 105.9156\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12149.3945 - val_loss: 106.2150\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 777.7666 - val_loss: 107.4712\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31886.3594 - val_loss: 107.6244\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27404.8496 - val_loss: 106.1996\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6321.2852 - val_loss: 106.0736\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3760.4592 - val_loss: 105.8606\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7348.8076 - val_loss: 106.0497\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4318.0586 - val_loss: 105.8313\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15016.9717 - val_loss: 105.4622\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4191.7617 - val_loss: 105.9626\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3614.3015 - val_loss: 106.0635\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6736.5024 - val_loss: 105.9480\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17585.7344 - val_loss: 105.1476\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11611.8125 - val_loss: 106.5478\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14103.0820 - val_loss: 106.4946\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13928.3633 - val_loss: 105.7871\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6099.6567 - val_loss: 105.6772\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 843.8613 - val_loss: 106.9288\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28299.5195 - val_loss: 106.8047\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21193.5312 - val_loss: 105.9048\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff860721160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:34:55.196 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff860721160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 43 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 58.8942 - val_loss: 36.4346\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39.1912 - val_loss: 18.1340\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33.0459 - val_loss: 27.7270\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26.0515 - val_loss: 30.1894\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22.7793 - val_loss: 20.4447\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.2047 - val_loss: 10.0688\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.3308 - val_loss: 7.9509\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.2269 - val_loss: 6.0885\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.7017 - val_loss: 7.4887\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.3478 - val_loss: 5.6310\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.0946 - val_loss: 6.6579\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.3964 - val_loss: 4.2313\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.6475 - val_loss: 4.4349\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.5975 - val_loss: 2.6032\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1473 - val_loss: 4.8352\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.0646 - val_loss: 2.4334\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.1625 - val_loss: 3.7731\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.5304 - val_loss: 2.5438\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.9932 - val_loss: 2.9815\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.7191 - val_loss: 2.6452\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5856 - val_loss: 2.0452\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.1676 - val_loss: 3.1725\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8578 - val_loss: 2.1473\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.6904 - val_loss: 2.7902\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.5077 - val_loss: 2.1129\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5395 - val_loss: 1.9780\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.3950 - val_loss: 1.9032\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.1479 - val_loss: 2.2619\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1036 - val_loss: 1.4767\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7602 - val_loss: 4.0865\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.6029 - val_loss: 1.7849\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6029 - val_loss: 3.9518\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7425 - val_loss: 1.5012\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.1466 - val_loss: 1.4627\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.6529 - val_loss: 2.0118\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0185 - val_loss: 2.5484\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3835 - val_loss: 1.3600\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8835 - val_loss: 1.6124\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7420 - val_loss: 1.8327\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.5569 - val_loss: 1.5430\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7781 - val_loss: 1.3620\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7246 - val_loss: 2.6178\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.1302 - val_loss: 1.5378\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.7357 - val_loss: 1.4429\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.5018 - val_loss: 1.8703\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.5798 - val_loss: 2.3007\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2897 - val_loss: 1.7104\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.5053 - val_loss: 1.8694\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.6260 - val_loss: 1.3881\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3607 - val_loss: 1.4781\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff90835cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:35:03.275 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff90835cc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 44 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 59.9534 - val_loss: 40.4137\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 44.5432 - val_loss: 29.2744\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34.6229 - val_loss: 33.8818\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 29.6957 - val_loss: 23.4362\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 25.5329 - val_loss: 15.5975\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21.2455 - val_loss: 10.2825\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17.0516 - val_loss: 5.0765\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.1378 - val_loss: 7.0962\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13.4945 - val_loss: 4.4151\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 14.0407 - val_loss: 5.8079\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12.4196 - val_loss: 5.5797\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11.9721 - val_loss: 5.9316\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.8998 - val_loss: 4.4692\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.1756 - val_loss: 5.1161\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.4911 - val_loss: 6.4320\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.3757 - val_loss: 4.4930\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.8499 - val_loss: 3.8185\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.3942 - val_loss: 7.9930\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.2157 - val_loss: 3.6135\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.3644 - val_loss: 5.8690\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.7728 - val_loss: 5.2516\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8432 - val_loss: 5.6847\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0766 - val_loss: 5.1941\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7901 - val_loss: 5.5667\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5313 - val_loss: 3.6266\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4106 - val_loss: 5.8038\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.2636 - val_loss: 3.6591\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3071 - val_loss: 5.9294\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0503 - val_loss: 5.0975\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7236 - val_loss: 3.2074\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.6574 - val_loss: 7.2605\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4557 - val_loss: 3.2070\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7210 - val_loss: 5.4789\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4434 - val_loss: 5.4593\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0178 - val_loss: 3.0499\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7210 - val_loss: 6.8103\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.9206 - val_loss: 3.2886\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5384 - val_loss: 4.8982\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5134 - val_loss: 4.3507\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.0310 - val_loss: 3.2881\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.7904 - val_loss: 5.3591\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6167 - val_loss: 3.1048\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1440 - val_loss: 4.9440\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6060 - val_loss: 3.3373\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9030 - val_loss: 3.8074\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4874 - val_loss: 3.3255\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7860 - val_loss: 5.6154\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7219 - val_loss: 3.0793\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1186 - val_loss: 5.3414\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5014 - val_loss: 2.9406\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff72a41d4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:35:11.284 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff72a41d4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 45 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 165961.7656 - val_loss: 98.0486\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12377.5156 - val_loss: 103.0423\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2187.0779 - val_loss: 104.0423\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 24356.6797 - val_loss: 103.3971\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 17204.2695 - val_loss: 101.5916\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10526.1377 - val_loss: 102.6081\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 39671.7578 - val_loss: 104.7018\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37445.0977 - val_loss: 101.4794\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28873.3848 - val_loss: 100.4031\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 12836.7852 - val_loss: 104.3867\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 59415.5586 - val_loss: 105.4399\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 55232.3945 - val_loss: 103.6740\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17809.3496 - val_loss: 100.4256\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 37800.9258 - val_loss: 99.2349\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 34599.6406 - val_loss: 100.1973\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12883.6797 - val_loss: 103.2475\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31233.5098 - val_loss: 103.3993\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34480.1484 - val_loss: 101.6318\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2227.5342 - val_loss: 101.8218\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17840.2891 - val_loss: 101.7518\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4370.3315 - val_loss: 99.2923\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33316.8750 - val_loss: 99.4065\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30786.9766 - val_loss: 100.5533\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11390.1064 - val_loss: 103.2665\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 35575.1445 - val_loss: 103.2539\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 35165.8906 - val_loss: 102.4898\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7640.4473 - val_loss: 100.3889\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13250.5449 - val_loss: 99.9712\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 20140.5977 - val_loss: 101.3264\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3476.0649 - val_loss: 100.6187\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 13771.6582 - val_loss: 100.8600\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3045.3743 - val_loss: 100.8335\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4913.4829 - val_loss: 101.7171\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18920.6816 - val_loss: 101.7922\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12557.0684 - val_loss: 99.1397\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 57866.4023 - val_loss: 98.0101\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 48495.8867 - val_loss: 99.5342\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6908.7876 - val_loss: 101.1943\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22187.1445 - val_loss: 102.5408\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33584.7188 - val_loss: 102.4062\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26202.5742 - val_loss: 100.6410\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7294.1680 - val_loss: 100.6647\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6083.5630 - val_loss: 101.3537\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20061.0430 - val_loss: 101.7692\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12333.4355 - val_loss: 100.1449\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29576.9121 - val_loss: 99.4878\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26339.1113 - val_loss: 100.2713\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1216.1733 - val_loss: 100.7397\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2573.1423 - val_loss: 100.9962\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3701.0349 - val_loss: 100.3432\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73745f940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:35:19.118 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73745f940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 46 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 51034.6914 - val_loss: 119.2701\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 201937.8750 - val_loss: 123.3731\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 156423.6562 - val_loss: 116.4889\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31052.4863 - val_loss: 110.6581\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 108322.5781 - val_loss: 108.0028\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 101830.2422 - val_loss: 108.1912\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 86919.6406 - val_loss: 111.2797\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30744.7363 - val_loss: 111.9440\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30066.4941 - val_loss: 110.5222\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19202.5234 - val_loss: 110.0985\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4166.2061 - val_loss: 111.4331\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 42511.5977 - val_loss: 111.7771\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 32283.2500 - val_loss: 111.0918\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16143.2217 - val_loss: 109.7953\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6267.3926 - val_loss: 111.0812\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47804.0938 - val_loss: 111.8579\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29283.8555 - val_loss: 110.5111\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8326.4492 - val_loss: 109.8387\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7273.2466 - val_loss: 110.3408\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27416.5332 - val_loss: 110.8668\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4004.0288 - val_loss: 109.6623\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30419.8613 - val_loss: 108.5521\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 36322.5820 - val_loss: 109.2723\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11573.9229 - val_loss: 110.5607\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33555.1484 - val_loss: 110.7641\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28820.1211 - val_loss: 109.4474\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16766.5098 - val_loss: 109.1222\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3591.1477 - val_loss: 109.9148\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25835.0703 - val_loss: 110.2285\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22968.6543 - val_loss: 109.0443\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 21948.7891 - val_loss: 108.6449\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10972.0957 - val_loss: 110.1329\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 52050.2617 - val_loss: 110.8265\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36329.4492 - val_loss: 109.8800\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6481.3213 - val_loss: 108.6466\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2801.4395 - val_loss: 109.5149\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27886.3496 - val_loss: 109.4122\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19986.6367 - val_loss: 108.8071\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6943.7383 - val_loss: 108.3647\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 337.5904 - val_loss: 109.0780\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18805.3516 - val_loss: 108.9313\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15751.1055 - val_loss: 108.1467\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5244.9595 - val_loss: 108.3437\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4740.9014 - val_loss: 108.0854\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4857.5562 - val_loss: 108.2900\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4072.8577 - val_loss: 107.8787\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12925.0918 - val_loss: 108.0155\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1411.2234 - val_loss: 107.9622\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11120.2266 - val_loss: 107.7795\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9213.8271 - val_loss: 108.3078\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff719818550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:35:27.096 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff719818550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 47 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0000e+00 - val_loss: 96.2264\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71a4919d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:35:35.085 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71a4919d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 48 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 190043.3125 - val_loss: 82.0739\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31027.2129 - val_loss: 98.1984\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 271051.5312 - val_loss: 103.0323\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 225472.5781 - val_loss: 99.6093\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 143793.1875 - val_loss: 93.4633\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 11586.8135 - val_loss: 87.5047\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 63438.5586 - val_loss: 85.3311\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 83582.5156 - val_loss: 87.1667\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 31789.2031 - val_loss: 89.8968\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 33786.8711 - val_loss: 91.6879\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 24080.7969 - val_loss: 90.6000\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 4881.8916 - val_loss: 89.5752\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 13676.2930 - val_loss: 90.4192\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4551.7778 - val_loss: 89.8769\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7653.7827 - val_loss: 91.4605\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 28720.1641 - val_loss: 90.8890\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11605.8564 - val_loss: 87.9526\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 58572.1641 - val_loss: 87.7415\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 38288.4961 - val_loss: 89.1341\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23485.6719 - val_loss: 91.4893\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15614.3604 - val_loss: 89.0630\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28286.1445 - val_loss: 89.3642\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22081.4629 - val_loss: 90.4690\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3653.2842 - val_loss: 94.1191\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 84673.9453 - val_loss: 95.0227\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 80529.6094 - val_loss: 93.3835\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 29841.7246 - val_loss: 91.3138\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15638.6104 - val_loss: 90.3349\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15726.2354 - val_loss: 91.1721\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 23807.0059 - val_loss: 92.0533\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11230.5547 - val_loss: 89.4825\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 59382.2109 - val_loss: 88.4954\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 50779.5977 - val_loss: 89.3899\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 39121.9414 - val_loss: 93.1589\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 71381.3047 - val_loss: 94.8741\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 67998.7812 - val_loss: 93.0057\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4759.5503 - val_loss: 91.4780\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 20642.9746 - val_loss: 90.3763\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33172.3125 - val_loss: 91.3067\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7243.6167 - val_loss: 92.4578\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 23065.1738 - val_loss: 92.9891\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20173.1543 - val_loss: 91.6081\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26038.6914 - val_loss: 91.2071\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14550.9375 - val_loss: 93.3188\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32635.0586 - val_loss: 93.7401\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33218.2383 - val_loss: 92.7913\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 544.5937 - val_loss: 92.5969\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6101.7720 - val_loss: 91.9339\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9200.4531 - val_loss: 92.7415\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15220.3506 - val_loss: 92.6721\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71c2f8160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:35:46.193 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71c2f8160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 49 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 76.3545 - val_loss: 55.9232\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 43.8091 - val_loss: 28.1169\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 39.9917 - val_loss: 33.5701\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28.6477 - val_loss: 32.3816\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 24.8819 - val_loss: 23.0879\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 22.3792 - val_loss: 14.5356\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18.6170 - val_loss: 18.0886\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.9576 - val_loss: 6.2811\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.5280 - val_loss: 7.7131\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.9243 - val_loss: 4.2578\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.1382 - val_loss: 2.3044\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.2517 - val_loss: 5.2697\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.6615 - val_loss: 2.1323\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.2478 - val_loss: 4.1016\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.5928 - val_loss: 2.6082\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.8432 - val_loss: 2.9496\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.3287 - val_loss: 3.1015\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.0283 - val_loss: 2.4357\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.7244 - val_loss: 2.8893\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.4406 - val_loss: 2.8669\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.1100 - val_loss: 2.7319\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7602 - val_loss: 3.1603\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.6535 - val_loss: 2.4248\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.5401 - val_loss: 4.6472\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.3734 - val_loss: 2.4430\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0068 - val_loss: 3.5306\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.3752 - val_loss: 4.2136\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.4504 - val_loss: 2.7697\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.3780 - val_loss: 6.3304\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.1705 - val_loss: 3.1221\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9405 - val_loss: 4.7171\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.7169 - val_loss: 3.0461\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.2180 - val_loss: 5.0273\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.9646 - val_loss: 2.8711\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4147 - val_loss: 4.3718\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0947 - val_loss: 2.3848\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.1347 - val_loss: 3.4361\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.6248 - val_loss: 2.5629\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.7379 - val_loss: 3.5424\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.6811 - val_loss: 2.6403\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.2102 - val_loss: 2.6332\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9530 - val_loss: 3.3348\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8488 - val_loss: 2.9653\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.8737 - val_loss: 3.4348\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0980 - val_loss: 3.1734\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.6736 - val_loss: 2.9435\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.8082 - val_loss: 2.4696\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9245 - val_loss: 4.2635\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 5.4593 - val_loss: 2.1376\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 5.3424 - val_loss: 3.6296\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71de3aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:35:58.112 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71de3aa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 50 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 69.3250 - val_loss: 45.5344\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33.9108 - val_loss: 20.7038\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 28.9106 - val_loss: 29.8546\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23.8129 - val_loss: 28.9977\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18.6293 - val_loss: 13.6040\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16.2196 - val_loss: 15.1245\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.4963 - val_loss: 7.7661\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.9528 - val_loss: 3.5204\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.2544 - val_loss: 3.3068\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.7120 - val_loss: 3.3779\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.7465 - val_loss: 5.0256\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.7131 - val_loss: 4.8654\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.2486 - val_loss: 5.1110\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2115 - val_loss: 3.2415\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2740 - val_loss: 4.2402\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.9750 - val_loss: 5.3936\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.6570 - val_loss: 3.1261\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.5874 - val_loss: 4.1433\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.2315 - val_loss: 3.3734\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4094 - val_loss: 4.0661\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5585 - val_loss: 5.1744\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.0138 - val_loss: 3.1091\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4112 - val_loss: 3.5428\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2182 - val_loss: 4.4151\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9902 - val_loss: 2.9201\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1723 - val_loss: 4.1631\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.9446 - val_loss: 3.2396\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.6236 - val_loss: 2.9906\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.4330 - val_loss: 4.9516\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8926 - val_loss: 2.8323\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.6666 - val_loss: 2.9490\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3146 - val_loss: 3.9216\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4879 - val_loss: 2.9514\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7319 - val_loss: 2.7744\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.8651 - val_loss: 3.1543\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.9115 - val_loss: 2.7758\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7217 - val_loss: 2.7905\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7060 - val_loss: 2.7447\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6274 - val_loss: 2.7522\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5838 - val_loss: 2.7169\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5270 - val_loss: 3.1161\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6000 - val_loss: 2.6775\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5071 - val_loss: 2.8351\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3756 - val_loss: 2.6608\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3078 - val_loss: 2.6516\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4006 - val_loss: 2.8290\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6446 - val_loss: 2.6137\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.3213 - val_loss: 2.6095\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.5595 - val_loss: 3.1658\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.4138 - val_loss: 2.6473\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6fe9610d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:36:08.405 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6fe9610d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 51 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 207671.5469 - val_loss: 102.5120\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 530113.8125 - val_loss: 105.1986\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 390252.4375 - val_loss: 96.0595\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 147701.8594 - val_loss: 86.7010\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 253183.3438 - val_loss: 83.0206\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 194422.9531 - val_loss: 86.8594\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 76338.6484 - val_loss: 92.8830\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40187.9062 - val_loss: 94.1880\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 91781.4609 - val_loss: 94.0253\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 28229.1680 - val_loss: 89.2839\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 93538.9609 - val_loss: 87.8976\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 107684.2266 - val_loss: 90.1488\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27240.6367 - val_loss: 93.1611\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 40634.9375 - val_loss: 93.2733\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 43369.3594 - val_loss: 92.1823\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28197.6289 - val_loss: 91.3457\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14445.4971 - val_loss: 94.6690\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 128392.4219 - val_loss: 95.8601\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 78699.9609 - val_loss: 92.9475\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 28757.6445 - val_loss: 87.6423\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 166476.2969 - val_loss: 86.1184\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 168925.8594 - val_loss: 88.3711\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 74465.1953 - val_loss: 90.3807\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4211.3364 - val_loss: 95.0481\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 132939.3125 - val_loss: 97.1196\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 154179.5312 - val_loss: 96.7081\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 120756.3047 - val_loss: 93.7936\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13556.5498 - val_loss: 90.5619\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 74211.5391 - val_loss: 89.6129\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 88309.8672 - val_loss: 90.5693\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33463.1719 - val_loss: 92.5796\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 56940.6523 - val_loss: 94.1497\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32285.6738 - val_loss: 92.4971\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 49634.2305 - val_loss: 91.1549\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28876.9141 - val_loss: 93.2292\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33252.0312 - val_loss: 93.6117\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29433.8848 - val_loss: 93.0268\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12760.4785 - val_loss: 89.3667\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 98281.6875 - val_loss: 88.8871\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 120792.3828 - val_loss: 89.1425\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 92914.4688 - val_loss: 92.2883\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3390.9685 - val_loss: 93.0745\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13319.8691 - val_loss: 92.2774\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22368.8145 - val_loss: 92.5709\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6431.7808 - val_loss: 94.5016\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 61171.2305 - val_loss: 94.3767\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45753.8281 - val_loss: 92.8033\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 932.6382 - val_loss: 93.1306\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9043.9092 - val_loss: 91.8342\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 47039.3516 - val_loss: 91.7617\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff701521af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:36:17.938 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff701521af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 52 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 11660.7070 - val_loss: 111.4288\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 334708.0938 - val_loss: 114.6707\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 251408.2656 - val_loss: 108.6198\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 104641.0391 - val_loss: 102.6924\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 30168.0684 - val_loss: 98.7489\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7368.2959 - val_loss: 100.7064\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42801.2930 - val_loss: 102.6256\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51073.7227 - val_loss: 102.0765\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5755.6826 - val_loss: 98.3271\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 70450.5469 - val_loss: 95.9511\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 71824.8438 - val_loss: 97.6960\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27091.8242 - val_loss: 100.4137\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 40680.3047 - val_loss: 101.7747\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33001.3242 - val_loss: 100.6193\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13554.3438 - val_loss: 97.0766\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 56464.0781 - val_loss: 96.9707\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 61592.8242 - val_loss: 97.8268\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26190.4102 - val_loss: 100.2752\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 417.3961 - val_loss: 100.7990\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9936.1924 - val_loss: 99.2330\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 28279.4531 - val_loss: 99.5163\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1707.2365 - val_loss: 100.8774\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45272.6406 - val_loss: 102.6362\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46466.4336 - val_loss: 101.2728\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9208.4307 - val_loss: 99.0909\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 41199.8594 - val_loss: 98.3571\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 38009.3867 - val_loss: 99.8890\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2839.3367 - val_loss: 102.4002\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 64858.1523 - val_loss: 103.4025\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 61249.2578 - val_loss: 101.5975\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17881.0137 - val_loss: 99.3151\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39512.9141 - val_loss: 98.7519\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34578.4727 - val_loss: 99.6416\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2993.9695 - val_loss: 100.4864\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20114.7637 - val_loss: 99.9337\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3117.9119 - val_loss: 102.0290\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28669.1738 - val_loss: 102.2848\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34722.2812 - val_loss: 101.2509\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1827.2780 - val_loss: 99.7962\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44268.0547 - val_loss: 98.6552\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 36744.6367 - val_loss: 100.4658\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9771.3877 - val_loss: 100.8865\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3025.3384 - val_loss: 100.1083\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17010.6133 - val_loss: 100.0363\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10674.0625 - val_loss: 100.7784\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22679.1523 - val_loss: 101.4851\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9445.3496 - val_loss: 99.8391\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37228.7305 - val_loss: 99.0317\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35058.4961 - val_loss: 100.4490\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1263.1926 - val_loss: 102.2854\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70428b700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:36:26.153 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70428b700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 53 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 77.8021 - val_loss: 54.9343\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 29.6627 - val_loss: 4.4501\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31.4260 - val_loss: 15.3268\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.1705 - val_loss: 27.6199\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 19.4898 - val_loss: 24.6974\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 16.4207 - val_loss: 14.3540\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15.0117 - val_loss: 10.6954\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.9853 - val_loss: 12.6418\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.9897 - val_loss: 9.4878\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.8455 - val_loss: 5.8027\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.6782 - val_loss: 5.2496\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 10.0139 - val_loss: 4.0527\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.7313 - val_loss: 3.3814\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.2103 - val_loss: 3.8502\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.8193 - val_loss: 3.1932\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.5375 - val_loss: 3.4852\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2854 - val_loss: 3.4110\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.8375 - val_loss: 3.0174\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.7196 - val_loss: 3.0234\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.4086 - val_loss: 3.3159\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0040 - val_loss: 3.8276\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0226 - val_loss: 3.0983\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5628 - val_loss: 3.1771\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.5723 - val_loss: 3.4955\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3590 - val_loss: 3.7984\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1089 - val_loss: 4.4080\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.3499 - val_loss: 4.7842\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.4095 - val_loss: 3.3140\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5557 - val_loss: 3.1627\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7223 - val_loss: 3.6101\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7003 - val_loss: 3.0941\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5002 - val_loss: 4.7959\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5768 - val_loss: 3.7100\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9879 - val_loss: 2.9885\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3422 - val_loss: 3.5759\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4743 - val_loss: 3.0452\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.9952 - val_loss: 3.3868\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6958 - val_loss: 3.0877\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.8729 - val_loss: 3.7152\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9127 - val_loss: 4.2044\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.5322 - val_loss: 3.3334\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.8030 - val_loss: 3.3893\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3530 - val_loss: 3.0468\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7839 - val_loss: 3.4430\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.0109 - val_loss: 3.2145\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.8861 - val_loss: 3.5300\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8854 - val_loss: 2.9440\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8475 - val_loss: 3.0797\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.8331 - val_loss: 2.7126\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.8348 - val_loss: 2.7605\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70810bb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:36:34.378 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70810bb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 54 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 84878.0156 - val_loss: 95.3144\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12748.9414 - val_loss: 94.1808\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 17238.8613 - val_loss: 93.7680\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 12480.7500 - val_loss: 90.7641\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 51105.7773 - val_loss: 91.3295\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 45097.8320 - val_loss: 91.7466\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 12428.8301 - val_loss: 90.8302\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 22333.4160 - val_loss: 92.6687\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 45461.3438 - val_loss: 93.0883\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 14084.4854 - val_loss: 91.8917\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 31864.9512 - val_loss: 93.4005\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 16460.4414 - val_loss: 92.6214\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 30821.8594 - val_loss: 93.3839\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5774.6699 - val_loss: 93.1555\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 4898.8008 - val_loss: 94.1194\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 26101.1934 - val_loss: 92.8097\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 23828.4805 - val_loss: 93.4055\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 13556.2256 - val_loss: 95.1386\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 48406.1562 - val_loss: 94.5146\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1767.0715 - val_loss: 93.7024\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19259.3516 - val_loss: 95.0648\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28560.5254 - val_loss: 94.3406\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14257.7139 - val_loss: 94.7278\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 12817.8125 - val_loss: 94.4413\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8113.5674 - val_loss: 94.8172\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 25882.3438 - val_loss: 94.5878\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6957.4341 - val_loss: 94.6753\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25566.9004 - val_loss: 94.7099\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5367.1978 - val_loss: 93.7329\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20269.3691 - val_loss: 93.8819\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35520.1133 - val_loss: 95.8486\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 99290.6641 - val_loss: 95.8530\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 67637.7344 - val_loss: 94.0831\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 54790.4531 - val_loss: 93.6436\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36732.8867 - val_loss: 95.1794\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 34513.6523 - val_loss: 94.7027\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6368.8398 - val_loss: 92.4081\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 53294.5039 - val_loss: 92.6459\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16653.8086 - val_loss: 94.0957\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36742.6055 - val_loss: 94.9918\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33980.4219 - val_loss: 94.4627\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8077.5889 - val_loss: 94.1753\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5714.7236 - val_loss: 94.7525\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5007.9004 - val_loss: 93.9476\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 41029.9766 - val_loss: 94.5382\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14631.8242 - val_loss: 95.3953\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8254.7051 - val_loss: 95.1987\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11757.4189 - val_loss: 95.6252\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19109.0508 - val_loss: 94.8003\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25397.6797 - val_loss: 94.6211\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70b7b6280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:36:45.354 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70b7b6280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 55 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 8507.7578 - val_loss: 86.2809\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 294697.4062 - val_loss: 83.6608\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 213569.9844 - val_loss: 91.3058\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 87188.1797 - val_loss: 99.0893\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16208.0908 - val_loss: 101.5706\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28069.4531 - val_loss: 100.6282\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18268.3164 - val_loss: 100.7912\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4173.1699 - val_loss: 103.8791\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 86854.4062 - val_loss: 105.0493\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 54837.4688 - val_loss: 102.6691\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11489.5088 - val_loss: 98.8883\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 85204.8281 - val_loss: 97.6861\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 90024.5078 - val_loss: 98.4677\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 60091.9141 - val_loss: 101.0278\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10794.9844 - val_loss: 104.5358\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 62400.4336 - val_loss: 105.3356\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 70481.3984 - val_loss: 104.6860\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 55687.0742 - val_loss: 101.7316\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 24417.3867 - val_loss: 100.8102\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16440.7129 - val_loss: 101.9526\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19988.7617 - val_loss: 102.7601\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15263.9766 - val_loss: 100.7528\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 39122.4883 - val_loss: 100.3005\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 32642.8281 - val_loss: 102.4060\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 45ms/step - loss: 24976.8105 - val_loss: 102.9594\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14640.4668 - val_loss: 101.5109\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 28591.6504 - val_loss: 100.8401\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19000.6406 - val_loss: 101.9018\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21086.5879 - val_loss: 102.8424\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 894.0296 - val_loss: 101.5065\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 40849.3008 - val_loss: 99.9751\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 43467.9258 - val_loss: 100.9944\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8626.3203 - val_loss: 102.2537\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3021.6760 - val_loss: 102.9947\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17862.9961 - val_loss: 102.1001\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3148.2405 - val_loss: 102.6770\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11424.8516 - val_loss: 101.9117\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10412.9834 - val_loss: 102.0648\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6649.0161 - val_loss: 101.9921\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2547.9783 - val_loss: 102.8105\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20999.5703 - val_loss: 102.6861\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4537.2114 - val_loss: 101.0448\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35151.9023 - val_loss: 100.5015\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32232.8496 - val_loss: 101.7092\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 727.4324 - val_loss: 101.7830\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6728.0571 - val_loss: 102.6576\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16363.4521 - val_loss: 102.3398\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5975.9937 - val_loss: 102.1283\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20240.9492 - val_loss: 102.8596\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15645.2812 - val_loss: 100.8234\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ef007ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:36:58.469 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ef007ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 56 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 13742.3867 - val_loss: 86.3987\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 315711.5625 - val_loss: 80.8726\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 216712.0938 - val_loss: 88.5836\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 71527.9609 - val_loss: 95.3640\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 53338.5586 - val_loss: 100.9555\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 50931.3750 - val_loss: 98.9303\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9791.0635 - val_loss: 98.4163\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2921.4268 - val_loss: 97.5846\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31627.9961 - val_loss: 97.3843\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 22074.6094 - val_loss: 100.9654\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 68325.6484 - val_loss: 101.8318\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 64979.2031 - val_loss: 100.4604\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39973.3203 - val_loss: 97.3377\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 51033.4297 - val_loss: 96.0550\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33411.5312 - val_loss: 97.7183\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3272.8103 - val_loss: 100.8826\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 79260.8984 - val_loss: 102.2521\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 77389.2266 - val_loss: 101.4164\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 39898.7695 - val_loss: 99.5909\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26034.1836 - val_loss: 95.6317\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 52925.3633 - val_loss: 94.8148\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 74308.4531 - val_loss: 95.2863\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 51094.7188 - val_loss: 96.6758\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7773.4121 - val_loss: 98.6392\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 28877.0156 - val_loss: 100.2381\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 45739.0312 - val_loss: 100.3266\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17195.8301 - val_loss: 98.7363\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6140.0293 - val_loss: 97.6293\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 25331.9277 - val_loss: 97.7195\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17150.6465 - val_loss: 99.5706\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 27503.1953 - val_loss: 99.6755\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22732.9629 - val_loss: 98.7830\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7928.5288 - val_loss: 98.3971\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1238.3298 - val_loss: 99.7350\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 30402.4668 - val_loss: 99.7610\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23952.5039 - val_loss: 98.8553\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3258.8992 - val_loss: 98.6811\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3631.3022 - val_loss: 98.0246\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20301.5664 - val_loss: 97.9749\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13348.5518 - val_loss: 99.6547\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 27893.0098 - val_loss: 99.7276\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22418.0527 - val_loss: 98.8774\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11114.7178 - val_loss: 98.3617\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7821.4722 - val_loss: 98.8022\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5528.5444 - val_loss: 98.6914\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4504.2012 - val_loss: 98.4950\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14173.9355 - val_loss: 98.3033\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3984.8989 - val_loss: 99.4581\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16658.0684 - val_loss: 99.4465\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16115.8867 - val_loss: 98.5643\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff796348ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:37:59.839 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff796348ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 57 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 86.0332 - val_loss: 63.9106\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 37.9041 - val_loss: 12.8488\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34.0949 - val_loss: 21.7324\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 23.9635 - val_loss: 33.8524\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 23.6914 - val_loss: 28.1031\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19.6217 - val_loss: 16.2682\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17.9782 - val_loss: 14.1415\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.6078 - val_loss: 19.8986\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13.0223 - val_loss: 11.2342\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.0373 - val_loss: 8.8940\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2403 - val_loss: 3.4265\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.5682 - val_loss: 5.3920\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2254 - val_loss: 3.5866\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.6603 - val_loss: 4.5649\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1088 - val_loss: 3.2391\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7320 - val_loss: 6.0209\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9312 - val_loss: 3.3123\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.7899 - val_loss: 5.4077\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6215 - val_loss: 3.5685\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.3267 - val_loss: 3.8935\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9865 - val_loss: 4.5797\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8085 - val_loss: 3.5387\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6777 - val_loss: 3.7460\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6230 - val_loss: 3.1644\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.9116 - val_loss: 4.9556\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4750 - val_loss: 4.4104\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8137 - val_loss: 3.3075\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4955 - val_loss: 3.2710\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5233 - val_loss: 4.1142\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1716 - val_loss: 4.4340\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1637 - val_loss: 3.8955\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1167 - val_loss: 3.5581\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9813 - val_loss: 3.2768\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0697 - val_loss: 4.0735\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9311 - val_loss: 3.9428\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1721 - val_loss: 4.4861\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.0951 - val_loss: 3.2261\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.8731 - val_loss: 3.8702\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9881 - val_loss: 3.1797\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.8969 - val_loss: 3.1897\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.9745 - val_loss: 3.5994\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0045 - val_loss: 4.6608\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1104 - val_loss: 3.2832\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8623 - val_loss: 3.3239\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.7562 - val_loss: 3.1096\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6763 - val_loss: 3.2554\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6262 - val_loss: 3.1119\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6145 - val_loss: 3.0337\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7578 - val_loss: 3.4254\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7984 - val_loss: 2.9801\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8603b20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:38:10.461 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8603b20d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 58 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 5358757.5000 - val_loss: 106.4920\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4135115.2500 - val_loss: 104.7922\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4499786.5000 - val_loss: 103.5310\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4168862.2500 - val_loss: 102.4261\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1879636.2500 - val_loss: 101.4187\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1403630.5000 - val_loss: 100.4825\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1018160.4375 - val_loss: 99.5558\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 555464.1250 - val_loss: 98.9072\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 185208.9844 - val_loss: 98.1486\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 604341.2500 - val_loss: 97.4490\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 532795.5000 - val_loss: 96.6424\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 661007.9375 - val_loss: 95.9260\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 627757.6875 - val_loss: 95.2069\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 625795.8125 - val_loss: 94.4653\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 569004.7500 - val_loss: 93.9228\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 515246.0938 - val_loss: 93.2965\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 669980.7500 - val_loss: 92.6277\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 172522.1875 - val_loss: 92.0255\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 438403.2188 - val_loss: 91.3994\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 724549.1875 - val_loss: 90.8234\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 914295.2500 - val_loss: 90.2375\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 682890.5000 - val_loss: 89.7414\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 366540.4688 - val_loss: 89.2172\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 98479.8516 - val_loss: 88.6500\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 406879.0938 - val_loss: 88.0871\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 287318.7188 - val_loss: 87.5696\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 310682.3125 - val_loss: 87.0496\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 378732.2500 - val_loss: 86.5314\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 307165.8125 - val_loss: 86.0031\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 286470.0312 - val_loss: 85.5272\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 370538.9688 - val_loss: 84.9863\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 339560.2500 - val_loss: 84.4558\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 375202.6562 - val_loss: 84.0043\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 83499.7266 - val_loss: 83.5205\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 227589.9531 - val_loss: 83.0406\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 267889.2812 - val_loss: 82.5836\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 299212.5938 - val_loss: 82.0763\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 328374.7812 - val_loss: 81.5614\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 365276.5625 - val_loss: 81.1651\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 719628.7500 - val_loss: 80.7294\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1057206.2500 - val_loss: 80.3782\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 360404.7500 - val_loss: 79.9538\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 224840.0781 - val_loss: 79.5604\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 234660.5312 - val_loss: 79.2079\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 510534.7500 - val_loss: 78.7844\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 478686.3750 - val_loss: 78.4879\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 382262.4375 - val_loss: 78.1728\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 454238.7500 - val_loss: 77.8561\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 450803.1562 - val_loss: 77.4956\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 424012.4688 - val_loss: 77.2393\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7d54cb700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:38:19.801 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7d54cb700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 59 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 114ms/step - loss: 52.7212 - val_loss: 25.7910\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 28.1376 - val_loss: 13.6677\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23.7112 - val_loss: 28.0177\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20.2290 - val_loss: 19.5081\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.7277 - val_loss: 10.9559\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.0915 - val_loss: 16.9969\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.2038 - val_loss: 4.4763\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.1792 - val_loss: 9.1911\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.9335 - val_loss: 3.4879\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.4295 - val_loss: 7.8108\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.0767 - val_loss: 3.5244\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.4031 - val_loss: 6.4400\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.2336 - val_loss: 5.9378\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8460 - val_loss: 4.5270\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.3748 - val_loss: 5.5976\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2139 - val_loss: 3.8923\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.3766 - val_loss: 4.4245\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0893 - val_loss: 4.4359\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.8762 - val_loss: 4.0769\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6331 - val_loss: 3.9825\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4957 - val_loss: 3.6882\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4452 - val_loss: 6.1269\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.5214 - val_loss: 2.5999\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2323 - val_loss: 5.0538\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0057 - val_loss: 3.7540\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9953 - val_loss: 3.8057\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.7074 - val_loss: 4.9928\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7155 - val_loss: 2.1525\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9923 - val_loss: 5.0273\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6582 - val_loss: 4.0153\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3659 - val_loss: 2.6319\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5310 - val_loss: 5.4898\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7764 - val_loss: 1.8371\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5545 - val_loss: 7.0024\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.0424 - val_loss: 2.5201\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.8226 - val_loss: 6.5831\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7123 - val_loss: 1.7150\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.6564 - val_loss: 4.9776\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8837 - val_loss: 1.8007\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 4.7660 - val_loss: 4.0889\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 4.8795 - val_loss: 3.2152\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.8790 - val_loss: 1.6939\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 5.0157 - val_loss: 4.2694\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.5836 - val_loss: 2.2186\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.3939 - val_loss: 2.3793\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.0951 - val_loss: 3.4876\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 4.1210 - val_loss: 3.1230\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 3.9918 - val_loss: 2.5318\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 3.9964 - val_loss: 3.2060\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 4.1086 - val_loss: 2.4136\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff703c02ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:38:30.514 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff703c02ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 60 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 85.5786 - val_loss: 69.2406\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 44.2611 - val_loss: 28.7233\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 38.8022 - val_loss: 35.1608\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30.8766 - val_loss: 39.8060\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28.6880 - val_loss: 37.2000\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24.6094 - val_loss: 24.7644\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21.8487 - val_loss: 14.4900\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17.7821 - val_loss: 18.8935\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.1755 - val_loss: 8.6313\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.1008 - val_loss: 6.9144\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.0227 - val_loss: 3.0594\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.3792 - val_loss: 4.4821\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.5536 - val_loss: 5.4859\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.9715 - val_loss: 4.5241\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.8149 - val_loss: 4.2159\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.7690 - val_loss: 6.9313\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.6731 - val_loss: 3.2285\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1612 - val_loss: 5.3512\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1310 - val_loss: 3.9936\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.6135 - val_loss: 3.5499\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.1876 - val_loss: 3.5690\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.0727 - val_loss: 3.2984\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.0626 - val_loss: 4.6753\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.9732 - val_loss: 2.8926\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.4647 - val_loss: 3.3084\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.5628 - val_loss: 3.8883\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.2472 - val_loss: 3.0868\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.1445 - val_loss: 2.9842\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.2942 - val_loss: 3.8687\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.1770 - val_loss: 3.5455\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.5699 - val_loss: 2.9839\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0615 - val_loss: 3.0657\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8928 - val_loss: 4.6524\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.1474 - val_loss: 3.1786\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4778 - val_loss: 3.8817\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3595 - val_loss: 2.9239\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.2924 - val_loss: 2.8892\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.8253 - val_loss: 2.9901\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0324 - val_loss: 3.1839\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.7810 - val_loss: 2.8516\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4835 - val_loss: 2.7693\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4098 - val_loss: 2.7747\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4553 - val_loss: 2.6909\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2109 - val_loss: 2.5326\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0546 - val_loss: 2.8166\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0423 - val_loss: 2.6198\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9114 - val_loss: 2.5508\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7913 - val_loss: 2.4277\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8849 - val_loss: 2.5940\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7823 - val_loss: 2.7719\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71b8db8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:38:40.012 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71b8db8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 61 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 16572.3047 - val_loss: 88.1963\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 277387.3750 - val_loss: 90.6403\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 179919.3125 - val_loss: 96.4785\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 39359.8828 - val_loss: 102.0248\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 63406.0117 - val_loss: 106.6522\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 73726.1484 - val_loss: 105.5012\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 42928.7383 - val_loss: 101.6358\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 55796.9609 - val_loss: 100.8867\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 49141.6562 - val_loss: 103.1844\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 20287.4375 - val_loss: 103.6014\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6705.3901 - val_loss: 101.0914\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 48589.3086 - val_loss: 100.9166\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 40579.6367 - val_loss: 102.7227\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3648.2698 - val_loss: 106.0060\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 63897.8477 - val_loss: 105.9123\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 64056.8828 - val_loss: 104.9408\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 46151.0352 - val_loss: 101.4823\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 39830.6641 - val_loss: 100.6073\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36803.8359 - val_loss: 102.3571\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12477.1543 - val_loss: 102.2936\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6687.2495 - val_loss: 102.4460\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10530.7881 - val_loss: 102.0448\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2792.2241 - val_loss: 103.1553\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 30648.1406 - val_loss: 103.0589\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14697.9062 - val_loss: 100.7808\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 56972.9023 - val_loss: 99.6915\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37521.4766 - val_loss: 101.3590\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15371.2314 - val_loss: 104.6635\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 65135.7812 - val_loss: 105.2635\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 70585.7734 - val_loss: 104.6572\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 56900.0000 - val_loss: 102.0885\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4365.1880 - val_loss: 101.7175\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2369.6445 - val_loss: 102.3168\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32115.1738 - val_loss: 102.9493\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26391.0137 - val_loss: 100.7777\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22439.2871 - val_loss: 100.7894\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22436.1973 - val_loss: 101.8896\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1525.7953 - val_loss: 101.4010\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10006.3184 - val_loss: 102.2048\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19351.0332 - val_loss: 102.2553\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1124.6930 - val_loss: 101.9681\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26545.5977 - val_loss: 102.6583\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16952.1113 - val_loss: 100.7494\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42511.7656 - val_loss: 99.8303\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25193.6387 - val_loss: 100.8158\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12831.7686 - val_loss: 102.0154\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14749.3896 - val_loss: 101.1892\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9046.0801 - val_loss: 101.4358\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 630.6618 - val_loss: 100.7058\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16768.8047 - val_loss: 101.3181\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73e48d5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:38:50.706 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73e48d5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 62 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 92.2125 - val_loss: 64.1064\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 40.6516 - val_loss: 11.2989\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 31.1465 - val_loss: 25.7015\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.6202 - val_loss: 35.1118\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.2870 - val_loss: 24.6117\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21.2373 - val_loss: 15.5830\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19.7043 - val_loss: 21.8856\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.3625 - val_loss: 21.8509\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.4413 - val_loss: 14.7625\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.4292 - val_loss: 17.9039\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.6994 - val_loss: 12.8337\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.0881 - val_loss: 12.4675\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.0184 - val_loss: 9.4952\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.4782 - val_loss: 8.5575\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.2016 - val_loss: 4.1060\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.1662 - val_loss: 4.9467\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9530 - val_loss: 3.7477\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.8426 - val_loss: 3.7338\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8729 - val_loss: 3.3167\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.3628 - val_loss: 3.7251\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.8951 - val_loss: 3.3189\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.8357 - val_loss: 3.4982\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4039 - val_loss: 3.3409\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3058 - val_loss: 4.7156\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4581 - val_loss: 3.8649\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.9109 - val_loss: 4.4969\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5678 - val_loss: 3.2340\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.9782 - val_loss: 3.1145\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9281 - val_loss: 3.3474\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4561 - val_loss: 2.7883\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.1846 - val_loss: 2.8772\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3345 - val_loss: 2.8360\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.3109 - val_loss: 2.6479\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.9641 - val_loss: 2.7753\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.9504 - val_loss: 3.0604\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7524 - val_loss: 2.6083\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6329 - val_loss: 3.6556\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1724 - val_loss: 2.8720\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.9174 - val_loss: 2.4492\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6757 - val_loss: 2.5467\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7877 - val_loss: 2.7978\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2859 - val_loss: 2.5883\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4990 - val_loss: 2.7585\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2438 - val_loss: 2.4427\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6289 - val_loss: 2.3870\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5826 - val_loss: 3.6433\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3527 - val_loss: 2.7354\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5484 - val_loss: 2.5841\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6877 - val_loss: 2.9175\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7604 - val_loss: 2.8950\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ead31160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:39:01.551 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ead31160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 63 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 2889724.0000 - val_loss: 100.0805\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5191358.5000 - val_loss: 98.5350\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5384994.5000 - val_loss: 97.1907\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2378888.0000 - val_loss: 95.9077\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2430483.7500 - val_loss: 94.6927\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1608940.0000 - val_loss: 93.5500\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 907147.8750 - val_loss: 92.4471\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 702864.9375 - val_loss: 91.3503\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 852131.9375 - val_loss: 90.3671\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 324828.7500 - val_loss: 89.3825\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 220826.1875 - val_loss: 88.5221\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 274079.4375 - val_loss: 87.5952\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 248178.9531 - val_loss: 86.6611\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 262249.2500 - val_loss: 85.6960\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 718489.8125 - val_loss: 84.6930\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 816762.0625 - val_loss: 83.8037\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 212067.9219 - val_loss: 82.8641\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 531539.3750 - val_loss: 81.9515\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 787606.7500 - val_loss: 81.1375\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 471371.2188 - val_loss: 80.3849\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 812350.1875 - val_loss: 79.7189\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 635893.3750 - val_loss: 78.9801\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 669743.1875 - val_loss: 78.3765\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 542745.4375 - val_loss: 77.6006\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 726287.0000 - val_loss: 76.9272\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 276155.5938 - val_loss: 76.3236\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 179984.4062 - val_loss: 75.6221\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 234899.5000 - val_loss: 75.0766\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 186705.0469 - val_loss: 74.2872\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 210611.2031 - val_loss: 73.5693\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 224393.5781 - val_loss: 72.9491\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 175579.2188 - val_loss: 72.2558\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 168033.2969 - val_loss: 71.6186\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 228869.5312 - val_loss: 71.0020\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 193604.8750 - val_loss: 70.4296\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 204107.0000 - val_loss: 69.8598\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 223414.3906 - val_loss: 69.2171\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 153228.7344 - val_loss: 68.5441\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 226389.1094 - val_loss: 67.9259\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 562954.8125 - val_loss: 67.4600\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 606863.6875 - val_loss: 66.9945\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 273230.0938 - val_loss: 66.6642\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 303916.9375 - val_loss: 66.2140\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 288866.0312 - val_loss: 65.7241\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 299281.6875 - val_loss: 65.2350\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 610424.3750 - val_loss: 64.9040\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 827110.7500 - val_loss: 64.6223\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 784091.3750 - val_loss: 64.4807\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 357943.2188 - val_loss: 64.3225\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 368162.4375 - val_loss: 63.9802\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73dd97430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:39:10.777 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73dd97430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 64 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 89123.6797 - val_loss: 116.7243\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 140566.1719 - val_loss: 116.5512\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 99307.3516 - val_loss: 110.0744\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 57087.8086 - val_loss: 107.5255\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35586.5234 - val_loss: 110.3463\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 58499.9141 - val_loss: 110.7523\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 48358.4492 - val_loss: 109.5977\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1735.1918 - val_loss: 107.8895\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1160.2418 - val_loss: 107.0748\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 42013.2812 - val_loss: 106.2288\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34189.1562 - val_loss: 108.7329\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 54963.3281 - val_loss: 109.4792\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50497.4883 - val_loss: 107.7352\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3841.4861 - val_loss: 107.5665\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8026.6670 - val_loss: 106.8329\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23264.0684 - val_loss: 106.7648\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1255.5774 - val_loss: 107.9601\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37481.4258 - val_loss: 108.4639\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33772.3438 - val_loss: 107.7398\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5071.2710 - val_loss: 105.8830\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 49010.7227 - val_loss: 105.1139\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 48523.3438 - val_loss: 105.5139\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22180.7793 - val_loss: 107.0097\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11379.2783 - val_loss: 107.1475\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17724.4551 - val_loss: 106.1512\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15032.9209 - val_loss: 106.2378\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6574.2046 - val_loss: 106.2998\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7550.0688 - val_loss: 106.1432\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3461.3042 - val_loss: 107.0416\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22369.9258 - val_loss: 106.7070\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14213.6875 - val_loss: 105.5115\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27561.1992 - val_loss: 105.4235\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19471.4316 - val_loss: 106.6148\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26802.2168 - val_loss: 106.6705\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14007.3408 - val_loss: 106.0698\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4199.4907 - val_loss: 103.8917\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 57957.9727 - val_loss: 103.4521\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 68939.9609 - val_loss: 103.8629\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 51379.5938 - val_loss: 104.9929\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13415.6172 - val_loss: 106.4102\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32165.7656 - val_loss: 106.4298\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35919.6133 - val_loss: 105.9220\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11448.4180 - val_loss: 105.0205\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22197.0781 - val_loss: 104.4211\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20074.3906 - val_loss: 104.8921\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4576.2788 - val_loss: 106.0667\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 49099.8672 - val_loss: 106.4180\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 44601.4219 - val_loss: 105.7746\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16306.4971 - val_loss: 104.8661\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6180.3867 - val_loss: 104.5056\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff72b175430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:39:19.684 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff72b175430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 65 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 70.9509 - val_loss: 46.6845\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30.2527 - val_loss: 10.8272\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26.8470 - val_loss: 19.3186\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.2995 - val_loss: 24.9900\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.4777 - val_loss: 14.4277\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17.0758 - val_loss: 10.9912\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.9881 - val_loss: 13.7184\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.4396 - val_loss: 7.5651\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.3153 - val_loss: 8.3035\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.0623 - val_loss: 4.4772\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.4895 - val_loss: 3.6496\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.2672 - val_loss: 3.2554\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.6619 - val_loss: 4.1450\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.1831 - val_loss: 3.8011\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.0444 - val_loss: 3.1360\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8868 - val_loss: 3.9940\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.8392 - val_loss: 2.6845\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.9183 - val_loss: 5.4625\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4384 - val_loss: 2.9089\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0869 - val_loss: 2.9449\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.8796 - val_loss: 3.4104\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.8705 - val_loss: 2.8343\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.6709 - val_loss: 3.7161\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4656 - val_loss: 3.1224\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3629 - val_loss: 3.0083\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4448 - val_loss: 3.8043\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.1591 - val_loss: 2.3878\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1477 - val_loss: 3.9037\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.2500 - val_loss: 2.6874\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.8673 - val_loss: 3.0212\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.6422 - val_loss: 2.5876\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5336 - val_loss: 3.4114\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3194 - val_loss: 2.3660\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3231 - val_loss: 3.5753\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1476 - val_loss: 2.2111\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1057 - val_loss: 3.1421\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9890 - val_loss: 2.2323\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.2663 - val_loss: 4.5986\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8429 - val_loss: 2.3329\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6833 - val_loss: 3.6902\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3693 - val_loss: 2.3864\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6055 - val_loss: 4.1641\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5926 - val_loss: 2.4858\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6608 - val_loss: 3.1544\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3817 - val_loss: 2.5324\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3058 - val_loss: 2.4819\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2439 - val_loss: 3.7626\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4636 - val_loss: 2.4464\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6626 - val_loss: 3.4856\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.4373 - val_loss: 3.2116\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff726f90040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:39:28.447 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff726f90040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 66 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 72.5575 - val_loss: 53.8499\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 29.4843 - val_loss: 15.2956\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29.9928 - val_loss: 24.8191\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21.5966 - val_loss: 32.9611\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.4402 - val_loss: 25.1831\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.3283 - val_loss: 15.0013\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.3399 - val_loss: 15.2799\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.4509 - val_loss: 14.0748\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.6390 - val_loss: 10.5878\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.1246 - val_loss: 5.7729\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.6515 - val_loss: 9.6245\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.7755 - val_loss: 6.1115\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.4993 - val_loss: 8.4842\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.2958 - val_loss: 8.3335\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.0787 - val_loss: 7.6629\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.5069 - val_loss: 7.4971\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.4640 - val_loss: 7.9684\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.3271 - val_loss: 5.4935\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.2316 - val_loss: 5.0793\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.0465 - val_loss: 7.5204\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.5158 - val_loss: 3.1246\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.0165 - val_loss: 9.7344\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.3534 - val_loss: 2.7196\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.1073 - val_loss: 7.0192\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.5774 - val_loss: 4.6338\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1723 - val_loss: 5.4180\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9743 - val_loss: 4.3185\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.9349 - val_loss: 6.3415\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.9980 - val_loss: 3.3593\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0014 - val_loss: 5.7811\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.4981 - val_loss: 4.0935\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3775 - val_loss: 3.1249\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.2895 - val_loss: 4.4137\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3058 - val_loss: 5.5997\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3344 - val_loss: 2.4874\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0539 - val_loss: 4.7663\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9097 - val_loss: 4.6205\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6646 - val_loss: 3.3378\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5840 - val_loss: 2.1186\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.6614 - val_loss: 3.7326\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3161 - val_loss: 4.2180\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3162 - val_loss: 3.0190\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1058 - val_loss: 1.9868\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0950 - val_loss: 2.4067\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.9606 - val_loss: 3.7841\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4229 - val_loss: 1.8311\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8940 - val_loss: 1.7662\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8648 - val_loss: 2.5129\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6494 - val_loss: 2.7669\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5462 - val_loss: 2.3497\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b453bb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:39:37.345 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b453bb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 67 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 94.1599 - val_loss: 72.7123\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 43.5856 - val_loss: 21.4881\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29.8295 - val_loss: 10.3153\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23.1010 - val_loss: 29.1302\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22.1221 - val_loss: 28.8303\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.3567 - val_loss: 14.8855\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.9224 - val_loss: 11.9473\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15.1389 - val_loss: 17.3538\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.4929 - val_loss: 13.5304\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.8392 - val_loss: 9.9727\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.0472 - val_loss: 8.5226\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9100 - val_loss: 7.5760\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.1666 - val_loss: 6.3724\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4082 - val_loss: 6.5439\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0264 - val_loss: 6.1667\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5441 - val_loss: 6.3426\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3466 - val_loss: 5.9041\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4149 - val_loss: 6.7632\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3153 - val_loss: 6.0797\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1943 - val_loss: 6.3223\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7865 - val_loss: 6.1826\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9178 - val_loss: 6.0481\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4957 - val_loss: 6.6292\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.5601 - val_loss: 6.2243\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4925 - val_loss: 5.8608\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.4717 - val_loss: 6.6894\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1181 - val_loss: 5.8136\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.0863 - val_loss: 5.8318\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.2927 - val_loss: 6.3677\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.0850 - val_loss: 6.4322\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.9893 - val_loss: 5.8725\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7593 - val_loss: 5.7204\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0798 - val_loss: 6.9740\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0860 - val_loss: 6.2449\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0041 - val_loss: 5.6136\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7813 - val_loss: 6.0754\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6297 - val_loss: 5.8291\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5708 - val_loss: 5.5744\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7642 - val_loss: 5.9543\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3898 - val_loss: 5.7176\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2130 - val_loss: 5.6637\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2152 - val_loss: 5.8840\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1930 - val_loss: 5.3338\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2106 - val_loss: 5.9902\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3007 - val_loss: 5.5118\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0640 - val_loss: 5.5406\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9826 - val_loss: 5.1346\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1866 - val_loss: 5.2206\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0371 - val_loss: 5.7056\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.0119 - val_loss: 5.2344\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff76341c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:39:45.847 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff76341c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 68 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 76.2092 - val_loss: 53.1620\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28.7552 - val_loss: 3.2890\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 25.6667 - val_loss: 9.8628\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17.0679 - val_loss: 22.3266\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.3352 - val_loss: 19.8295\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.0248 - val_loss: 12.1828\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.8363 - val_loss: 8.5323\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.1174 - val_loss: 14.4818\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.7444 - val_loss: 8.8922\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.0560 - val_loss: 6.1939\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.4504 - val_loss: 6.8465\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5910 - val_loss: 3.1604\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.8886 - val_loss: 3.2834\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.5820 - val_loss: 3.5088\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2338 - val_loss: 3.4813\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8892 - val_loss: 3.5651\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.7761 - val_loss: 3.5290\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6598 - val_loss: 4.9115\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.2397 - val_loss: 3.4537\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6114 - val_loss: 3.6435\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3371 - val_loss: 3.9006\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3262 - val_loss: 3.5625\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.6349 - val_loss: 4.0485\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.2844 - val_loss: 3.9409\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.1885 - val_loss: 3.4743\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3629 - val_loss: 3.4559\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3946 - val_loss: 3.6896\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0380 - val_loss: 3.5908\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8842 - val_loss: 4.0599\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9677 - val_loss: 3.5632\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9691 - val_loss: 3.4512\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.1424 - val_loss: 4.6946\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4730 - val_loss: 3.5956\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9457 - val_loss: 3.6507\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.7175 - val_loss: 3.3023\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0770 - val_loss: 3.3354\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7186 - val_loss: 3.4427\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6820 - val_loss: 3.4351\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5929 - val_loss: 3.3623\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7850 - val_loss: 3.3819\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7505 - val_loss: 3.5142\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5379 - val_loss: 3.3387\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5407 - val_loss: 3.7126\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7015 - val_loss: 3.5214\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5602 - val_loss: 3.3587\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5983 - val_loss: 3.2065\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.5062 - val_loss: 3.5835\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.4217 - val_loss: 3.3550\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4915 - val_loss: 3.3446\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4973 - val_loss: 3.2298\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff77ca73670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:39:54.980 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff77ca73670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 69 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 72.8614 - val_loss: 54.9106\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 32.1491 - val_loss: 10.2314\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 22.2361 - val_loss: 9.2822\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 17.6052 - val_loss: 24.0536\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 16.5356 - val_loss: 20.3655\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.5952 - val_loss: 9.3353\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 13.4439 - val_loss: 10.8536\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 11.7722 - val_loss: 14.7429\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.6522 - val_loss: 9.0445\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.2939 - val_loss: 9.6487\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.2847 - val_loss: 5.6109\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.1587 - val_loss: 3.9328\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3060 - val_loss: 5.3883\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3267 - val_loss: 5.7010\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.3706 - val_loss: 4.9871\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5341 - val_loss: 4.1967\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.1613 - val_loss: 4.2928\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8628 - val_loss: 4.6598\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6890 - val_loss: 4.3908\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7996 - val_loss: 4.5446\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6034 - val_loss: 4.6743\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4695 - val_loss: 4.4690\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7098 - val_loss: 4.5533\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5580 - val_loss: 4.7691\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.3755 - val_loss: 4.9080\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.8690 - val_loss: 4.6746\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2520 - val_loss: 4.2578\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3599 - val_loss: 4.3584\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1696 - val_loss: 5.4853\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.8473 - val_loss: 4.3939\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.9653 - val_loss: 4.2932\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9603 - val_loss: 4.1867\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.8186 - val_loss: 4.2018\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7587 - val_loss: 4.2038\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7158 - val_loss: 4.1186\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7279 - val_loss: 4.2451\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.7495 - val_loss: 4.1913\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.9147 - val_loss: 4.1464\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.5799 - val_loss: 4.1126\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6808 - val_loss: 3.9367\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4887 - val_loss: 3.9031\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6103 - val_loss: 3.9452\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8887 - val_loss: 4.6433\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1213 - val_loss: 3.8398\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5814 - val_loss: 4.3113\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7826 - val_loss: 3.7326\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.3644 - val_loss: 3.7864\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4800 - val_loss: 3.6600\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.2772 - val_loss: 3.6224\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.1834 - val_loss: 3.6609\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff786376310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:40:05.458 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff786376310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 70 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 1654910.7500 - val_loss: 79.6947\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 373117.7188 - val_loss: 104.0030\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1042499.6875 - val_loss: 110.7912\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 919474.7500 - val_loss: 104.6564\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 320706.9062 - val_loss: 99.0855\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 47958.2266 - val_loss: 91.3006\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 385243.0312 - val_loss: 90.5323\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 334438.7188 - val_loss: 93.6989\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 42255.7812 - val_loss: 97.2820\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 123093.8438 - val_loss: 96.5855\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 76174.9375 - val_loss: 94.1994\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 107156.9141 - val_loss: 96.2397\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 49377.3555 - val_loss: 95.6580\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 105504.3203 - val_loss: 93.4803\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 136495.8750 - val_loss: 96.1144\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 118490.7891 - val_loss: 97.9049\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 89649.8906 - val_loss: 95.5296\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 58799.3242 - val_loss: 97.3163\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 76997.7578 - val_loss: 97.4647\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 41290.0938 - val_loss: 95.2373\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 82480.4766 - val_loss: 96.5146\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 57467.2695 - val_loss: 100.5007\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 291212.8125 - val_loss: 100.7031\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 199835.1562 - val_loss: 97.2685\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 39593.5078 - val_loss: 96.5509\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17006.0410 - val_loss: 97.1672\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33115.1875 - val_loss: 97.2461\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3430.6411 - val_loss: 96.7252\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 86791.2656 - val_loss: 96.2554\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 66436.0312 - val_loss: 99.5059\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 102129.3125 - val_loss: 99.3446\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 87216.6875 - val_loss: 97.2293\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 53018.0117 - val_loss: 97.5336\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16340.9736 - val_loss: 100.6855\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 244947.3125 - val_loss: 101.4922\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 224451.5312 - val_loss: 97.1651\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 123089.0547 - val_loss: 96.5233\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 45844.1875 - val_loss: 98.4250\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 156134.7031 - val_loss: 100.2239\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 109546.2266 - val_loss: 98.7507\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 92467.2812 - val_loss: 95.4023\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 204883.7969 - val_loss: 95.7930\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 52427.3867 - val_loss: 98.7761\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 104627.8281 - val_loss: 101.1687\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 158531.5938 - val_loss: 99.9361\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 49765.7578 - val_loss: 98.2956\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15322.4453 - val_loss: 100.3131\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 139922.0312 - val_loss: 100.5034\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 53677.6367 - val_loss: 98.8751\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 50283.9883 - val_loss: 98.1343\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79761a8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:40:17.156 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79761a8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 71 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 21768.2754 - val_loss: 126.8760\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 369909.3438 - val_loss: 124.9676\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 216985.0781 - val_loss: 113.6520\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 119681.0391 - val_loss: 95.1094\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16263.2529 - val_loss: 76.8612\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 221482.3438 - val_loss: 68.5952\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 213248.5156 - val_loss: 76.7524\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 95786.9688 - val_loss: 82.5576\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 56957.3164 - val_loss: 93.3524\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 91335.3906 - val_loss: 99.0165\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 77730.0469 - val_loss: 96.9706\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39023.3438 - val_loss: 90.0442\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 48265.1641 - val_loss: 86.5767\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 51463.8047 - val_loss: 88.6671\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10575.8887 - val_loss: 92.0915\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17104.2148 - val_loss: 94.6059\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 31580.2500 - val_loss: 92.8875\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11032.5947 - val_loss: 88.2434\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37435.7773 - val_loss: 88.6474\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31966.9043 - val_loss: 90.3450\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7418.6392 - val_loss: 92.0369\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13651.9092 - val_loss: 91.3280\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3203.4292 - val_loss: 95.1764\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33509.0664 - val_loss: 94.8496\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 28468.6621 - val_loss: 93.2094\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5629.9165 - val_loss: 92.0093\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 190.0624 - val_loss: 89.8665\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33329.4219 - val_loss: 89.6055\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4579.2690 - val_loss: 94.0612\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20777.3691 - val_loss: 95.2428\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35966.8867 - val_loss: 93.8978\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5956.5537 - val_loss: 90.6257\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24341.4902 - val_loss: 89.6421\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26080.3379 - val_loss: 91.5909\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5846.6084 - val_loss: 96.8188\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 55511.8867 - val_loss: 97.3428\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 51089.1172 - val_loss: 96.1816\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8555.0596 - val_loss: 91.5395\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 45916.5547 - val_loss: 87.2577\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 56076.5742 - val_loss: 88.2481\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 28500.3789 - val_loss: 90.7209\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12320.5840 - val_loss: 93.6968\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13746.2627 - val_loss: 91.2179\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19109.0332 - val_loss: 91.5594\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5894.2251 - val_loss: 94.9520\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 47340.2617 - val_loss: 96.5157\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 42240.4453 - val_loss: 92.0711\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3932.7854 - val_loss: 92.0905\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3781.5403 - val_loss: 93.3056\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27174.4590 - val_loss: 94.5772\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7a4cef700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:40:27.040 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7a4cef700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 72 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 125ms/step - loss: 178183.3750 - val_loss: 106.2929\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 195477.6719 - val_loss: 108.8510\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 184617.1406 - val_loss: 105.7147\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 68836.1562 - val_loss: 105.9802\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 94687.8516 - val_loss: 102.5463\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 193387.6875 - val_loss: 102.2790\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 155135.2969 - val_loss: 107.0078\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 506923.7812 - val_loss: 108.2289\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 388927.4062 - val_loss: 105.1255\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 44526.7148 - val_loss: 103.8875\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28245.1719 - val_loss: 104.1422\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 66219.8359 - val_loss: 103.6330\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 141884.5156 - val_loss: 100.7530\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 266499.6562 - val_loss: 101.3527\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 49921.4336 - val_loss: 102.9280\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 163810.0469 - val_loss: 104.4487\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 221785.3906 - val_loss: 103.6222\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 98209.3516 - val_loss: 101.2382\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 225343.6562 - val_loss: 100.6762\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 192649.1094 - val_loss: 101.7935\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 77372.8047 - val_loss: 103.9858\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 154426.7656 - val_loss: 103.1743\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10925.7686 - val_loss: 102.2865\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13342.5811 - val_loss: 102.8871\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32739.5918 - val_loss: 102.6452\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4973.6582 - val_loss: 103.2116\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32156.5371 - val_loss: 101.9287\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 165296.1094 - val_loss: 101.9590\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 58036.2695 - val_loss: 103.6989\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 103357.4531 - val_loss: 104.2094\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 121015.5391 - val_loss: 102.8322\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 71267.7266 - val_loss: 102.7064\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 58923.5938 - val_loss: 103.4934\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 44800.2617 - val_loss: 102.0766\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 87470.4688 - val_loss: 102.1121\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11200.7061 - val_loss: 103.1589\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 154158.3438 - val_loss: 103.4480\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 97312.3125 - val_loss: 102.3732\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19370.3848 - val_loss: 102.0848\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10667.3311 - val_loss: 103.5835\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 206285.4219 - val_loss: 103.9482\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 128592.7344 - val_loss: 102.4840\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 42381.5625 - val_loss: 101.9201\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 44152.8711 - val_loss: 102.5568\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 66145.6016 - val_loss: 102.6927\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16362.2314 - val_loss: 101.6888\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 140513.9531 - val_loss: 101.2701\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 81176.2500 - val_loss: 102.4658\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 78765.8672 - val_loss: 102.9052\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 45143.0117 - val_loss: 102.1217\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7cde90d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:40:36.663 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7cde90d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 73 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 5367.4370 - val_loss: 111.9919\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 258802.0000 - val_loss: 111.7036\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 194000.4375 - val_loss: 105.2234\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 54802.5859 - val_loss: 99.9878\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 65158.6719 - val_loss: 98.1420\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 54511.7188 - val_loss: 98.4108\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2671.5071 - val_loss: 100.7774\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 62332.5781 - val_loss: 101.8475\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 71668.2188 - val_loss: 101.2638\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 37989.0547 - val_loss: 99.5484\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 2099.2141 - val_loss: 96.7603\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 81037.5703 - val_loss: 95.8762\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 83141.9062 - val_loss: 96.4835\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50634.5938 - val_loss: 98.0995\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1164.2015 - val_loss: 100.2157\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 37859.2852 - val_loss: 101.0416\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 49337.5742 - val_loss: 100.3221\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 29836.6211 - val_loss: 98.5794\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25603.1211 - val_loss: 98.2963\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17562.2676 - val_loss: 99.2362\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9898.4453 - val_loss: 99.2360\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 2697.7603 - val_loss: 98.3419\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 36620.6016 - val_loss: 97.7859\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 26523.9062 - val_loss: 99.3909\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 17555.7559 - val_loss: 99.6146\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 17026.7090 - val_loss: 98.5504\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19744.6348 - val_loss: 98.3848\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10166.0957 - val_loss: 99.6567\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 26003.1934 - val_loss: 99.7220\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19986.0566 - val_loss: 99.2771\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9826.1211 - val_loss: 98.4731\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7954.7720 - val_loss: 98.9621\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9298.3730 - val_loss: 98.9575\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1120.9918 - val_loss: 98.4910\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34183.1133 - val_loss: 97.5632\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26291.8320 - val_loss: 98.5717\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1203.1324 - val_loss: 98.7033\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1834.5872 - val_loss: 99.2307\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16979.1660 - val_loss: 99.0849\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4511.3516 - val_loss: 98.5602\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30537.7363 - val_loss: 97.6354\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21743.8789 - val_loss: 98.3851\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2172.4482 - val_loss: 100.0062\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 48238.1133 - val_loss: 100.5158\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 48765.5469 - val_loss: 100.1198\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 26781.0547 - val_loss: 99.0514\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7763.3257 - val_loss: 98.3613\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7204.0771 - val_loss: 99.1726\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18143.8047 - val_loss: 98.9924\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7494.3140 - val_loss: 98.2503\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7eb32a940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:40:46.938 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7eb32a940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 74 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 97.0671 - val_loss: 76.6973\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48.5329 - val_loss: 35.3846\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37.7153 - val_loss: 18.6795\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 30.7235 - val_loss: 28.4640\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 25.6216 - val_loss: 31.1579\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21.4104 - val_loss: 17.2106\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17.6179 - val_loss: 11.1886\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.8252 - val_loss: 13.2824\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.5163 - val_loss: 4.1766\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1116 - val_loss: 4.2506\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3268 - val_loss: 3.9484\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.7069 - val_loss: 3.5585\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0509 - val_loss: 5.1916\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5836 - val_loss: 6.5018\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.5823 - val_loss: 2.8977\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.0707 - val_loss: 4.4778\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.8463 - val_loss: 3.7670\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4560 - val_loss: 3.3725\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9494 - val_loss: 3.3271\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5045 - val_loss: 3.3752\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3196 - val_loss: 3.5819\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1604 - val_loss: 5.1627\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.8339 - val_loss: 3.6283\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.2888 - val_loss: 3.8505\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.2175 - val_loss: 2.9029\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.0587 - val_loss: 2.9727\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5780 - val_loss: 4.3547\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5419 - val_loss: 3.3104\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.6287 - val_loss: 2.9795\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.3654 - val_loss: 3.3961\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3680 - val_loss: 3.0931\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4876 - val_loss: 3.2082\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.5347 - val_loss: 2.8339\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3257 - val_loss: 2.9832\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1424 - val_loss: 3.2247\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9441 - val_loss: 3.0394\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0063 - val_loss: 3.3972\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2092 - val_loss: 3.1583\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9377 - val_loss: 3.2064\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1173 - val_loss: 3.5959\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2366 - val_loss: 2.8998\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9349 - val_loss: 3.2717\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9475 - val_loss: 3.0928\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8598 - val_loss: 3.8427\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0249 - val_loss: 3.0409\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.7933 - val_loss: 3.4204\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.1728 - val_loss: 4.3888\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4442 - val_loss: 3.4583\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8422 - val_loss: 3.0634\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6774 - val_loss: 3.5027\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7feae4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:40:56.127 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7feae4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 75 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 89.1860 - val_loss: 72.8245\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40.0264 - val_loss: 28.7325\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 38.1467 - val_loss: 33.7463\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 30.0921 - val_loss: 43.4743\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27.9804 - val_loss: 37.7365\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23.8964 - val_loss: 25.0500\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21.4820 - val_loss: 19.9238\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17.9605 - val_loss: 21.9038\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.9180 - val_loss: 11.8527\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.4962 - val_loss: 10.0868\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.0787 - val_loss: 6.1797\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.8430 - val_loss: 7.0618\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.2609 - val_loss: 7.5907\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.4063 - val_loss: 4.9497\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.5617 - val_loss: 5.5249\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.2097 - val_loss: 5.1813\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.4632 - val_loss: 5.8883\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.7888 - val_loss: 4.3165\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.8812 - val_loss: 6.9917\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5840 - val_loss: 3.8619\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.8223 - val_loss: 5.0656\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.0504 - val_loss: 4.3808\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.3737 - val_loss: 4.5419\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.1171 - val_loss: 4.8231\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.2066 - val_loss: 4.3699\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.0876 - val_loss: 4.3655\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.7569 - val_loss: 4.4287\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.2745 - val_loss: 4.5950\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.1153 - val_loss: 4.7211\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9611 - val_loss: 4.6922\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.0324 - val_loss: 4.6402\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6678 - val_loss: 4.9566\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.4654 - val_loss: 4.5287\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2469 - val_loss: 5.0050\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9087 - val_loss: 4.8201\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0696 - val_loss: 4.5104\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.7602 - val_loss: 4.0224\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1785 - val_loss: 4.6695\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.6432 - val_loss: 4.1630\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.1787 - val_loss: 3.7789\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9779 - val_loss: 3.5758\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9411 - val_loss: 4.2268\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.6755 - val_loss: 3.9920\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.0227 - val_loss: 4.5737\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.9994 - val_loss: 3.1564\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1938 - val_loss: 3.3907\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7926 - val_loss: 3.0270\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6736 - val_loss: 3.1328\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4334 - val_loss: 3.5705\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.7647 - val_loss: 2.8798\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8a4f7e5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:41:05.170 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8a4f7e5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 76 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 16000.9785 - val_loss: 104.9568\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 271727.8438 - val_loss: 102.7813\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 160471.9062 - val_loss: 98.8155\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 84148.2188 - val_loss: 91.0132\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24184.6953 - val_loss: 88.5826\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 53446.3984 - val_loss: 88.5786\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9829.1719 - val_loss: 92.0071\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 51598.0234 - val_loss: 93.7753\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 57233.4531 - val_loss: 92.9432\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33753.7969 - val_loss: 90.1184\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37883.6094 - val_loss: 90.0327\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 31966.0156 - val_loss: 91.4840\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18007.1270 - val_loss: 92.3761\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11041.7061 - val_loss: 89.4522\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48495.8086 - val_loss: 89.4809\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 48882.3867 - val_loss: 90.6143\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10174.2275 - val_loss: 92.5821\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 40999.9961 - val_loss: 93.8841\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37573.4688 - val_loss: 93.1413\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8670.1689 - val_loss: 90.9260\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 27165.8438 - val_loss: 90.6830\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 34319.8008 - val_loss: 92.1022\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3744.1243 - val_loss: 92.2218\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9301.4150 - val_loss: 92.7531\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12287.1895 - val_loss: 92.6011\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3068.5603 - val_loss: 92.9672\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16721.9434 - val_loss: 92.9405\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12599.3418 - val_loss: 92.0996\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7525.3398 - val_loss: 93.6411\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 42135.5000 - val_loss: 94.3280\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17545.8340 - val_loss: 92.8540\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17570.1113 - val_loss: 91.5352\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29482.3359 - val_loss: 91.6817\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14075.9922 - val_loss: 94.2481\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 61363.7031 - val_loss: 95.5570\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 52122.0742 - val_loss: 94.2724\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21532.5176 - val_loss: 92.1143\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19664.1289 - val_loss: 91.9307\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 27408.5879 - val_loss: 92.6635\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1223.4716 - val_loss: 93.1158\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9549.3926 - val_loss: 93.1459\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1488.6721 - val_loss: 93.9230\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21957.7266 - val_loss: 94.2170\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17439.8691 - val_loss: 92.7978\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11804.0703 - val_loss: 93.1145\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1310.5665 - val_loss: 93.6590\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14743.1250 - val_loss: 94.1765\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18798.6836 - val_loss: 93.4160\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2483.1086 - val_loss: 93.7052\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11637.6309 - val_loss: 93.5533\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff84a22ae50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:41:14.706 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff84a22ae50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 77 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 72.6493 - val_loss: 63.5846\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 60.9443 - val_loss: 46.2721\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 45.0686 - val_loss: 45.3909\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 35.0187 - val_loss: 27.6764\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31.7132 - val_loss: 17.1710\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26.8610 - val_loss: 23.6204\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24.1929 - val_loss: 13.1084\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21.6906 - val_loss: 10.4544\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18.9366 - val_loss: 6.5616\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17.9721 - val_loss: 4.1454\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16.9005 - val_loss: 3.6116\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.9980 - val_loss: 3.3588\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.9743 - val_loss: 4.6857\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.8439 - val_loss: 3.2989\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.7780 - val_loss: 3.0263\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13.5799 - val_loss: 3.8869\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.6705 - val_loss: 3.1956\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.0711 - val_loss: 3.7724\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.1621 - val_loss: 4.1040\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.4220 - val_loss: 3.8031\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.0310 - val_loss: 5.0365\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.0348 - val_loss: 4.5012\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.0820 - val_loss: 4.0826\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.1800 - val_loss: 4.4976\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.1636 - val_loss: 3.5891\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.8665 - val_loss: 4.2442\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.9881 - val_loss: 3.5062\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.0386 - val_loss: 3.4049\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3733 - val_loss: 4.0747\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.3320 - val_loss: 3.8626\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.1202 - val_loss: 3.5054\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.0117 - val_loss: 4.9513\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2304 - val_loss: 3.8340\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.4986 - val_loss: 7.1354\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.3472 - val_loss: 3.4042\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.3036 - val_loss: 7.5011\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.3385 - val_loss: 3.9287\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.3972 - val_loss: 5.2878\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.8957 - val_loss: 3.3271\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.7011 - val_loss: 3.2287\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2930 - val_loss: 3.3342\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.0569 - val_loss: 3.1798\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.0360 - val_loss: 3.4481\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.8394 - val_loss: 3.1250\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8482 - val_loss: 4.1303\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3596 - val_loss: 3.4168\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.8662 - val_loss: 3.7904\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7829 - val_loss: 3.4655\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.0188 - val_loss: 3.4585\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.6091 - val_loss: 4.1871\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff806fa9d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:41:23.755 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff806fa9d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 78 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 92.2250 - val_loss: 71.1192\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 42.1558 - val_loss: 28.0180\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32.3156 - val_loss: 15.3461\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26.3809 - val_loss: 26.9618\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 22.8499 - val_loss: 32.6809\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 22.4045 - val_loss: 25.1752\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 18.6614 - val_loss: 13.7939\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17.7272 - val_loss: 14.4489\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.6273 - val_loss: 19.1665\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.8111 - val_loss: 12.4227\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.5230 - val_loss: 8.4874\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.2850 - val_loss: 4.8406\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.6097 - val_loss: 2.5589\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2335 - val_loss: 2.7318\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4439 - val_loss: 2.6968\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.9056 - val_loss: 3.7737\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.1323 - val_loss: 2.7952\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.5982 - val_loss: 3.8130\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2720 - val_loss: 4.1090\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9668 - val_loss: 3.1037\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.2501 - val_loss: 3.1226\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.9218 - val_loss: 3.8965\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4563 - val_loss: 3.6779\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6123 - val_loss: 5.5563\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.5325 - val_loss: 4.1469\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9199 - val_loss: 4.4914\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.2111 - val_loss: 4.0068\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.9189 - val_loss: 4.1077\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.9700 - val_loss: 4.0486\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9534 - val_loss: 3.7605\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8444 - val_loss: 3.6323\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8499 - val_loss: 4.2468\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8936 - val_loss: 4.1952\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1348 - val_loss: 3.5850\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2610 - val_loss: 5.6520\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4908 - val_loss: 6.2800\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0998 - val_loss: 6.1136\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3271 - val_loss: 3.6607\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0074 - val_loss: 5.1114\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0383 - val_loss: 3.7281\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8057 - val_loss: 4.3488\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8302 - val_loss: 3.7430\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4704 - val_loss: 5.2610\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1680 - val_loss: 3.5945\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0556 - val_loss: 4.5409\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.6665 - val_loss: 3.6744\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7587 - val_loss: 4.7999\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7452 - val_loss: 3.5342\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4354 - val_loss: 3.4999\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4116 - val_loss: 3.4834\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ec23a550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:41:32.784 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7ec23a550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 79 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 3012075.2500 - val_loss: 103.8336\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5387458.0000 - val_loss: 100.5213\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5346187.0000 - val_loss: 98.8318\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1539426.8750 - val_loss: 96.7690\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 849850.7500 - val_loss: 95.4366\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 859398.7500 - val_loss: 94.0388\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 760261.8125 - val_loss: 93.0941\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 197691.5469 - val_loss: 92.1473\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 216440.8281 - val_loss: 91.2262\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 256292.7031 - val_loss: 90.0267\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 223730.4219 - val_loss: 89.0932\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 270479.5312 - val_loss: 87.9748\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 244014.8906 - val_loss: 86.8515\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 234424.6250 - val_loss: 85.8430\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 232683.7656 - val_loss: 84.7643\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 237383.0469 - val_loss: 83.5469\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 249178.2031 - val_loss: 82.5160\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 217555.8125 - val_loss: 81.5158\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 219495.3750 - val_loss: 80.4777\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 237491.9531 - val_loss: 79.5263\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 258370.6094 - val_loss: 78.5424\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 197322.9688 - val_loss: 77.5159\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 197826.8750 - val_loss: 76.4407\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 255251.9688 - val_loss: 75.5500\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 257251.3125 - val_loss: 74.5904\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 182652.7969 - val_loss: 73.6205\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 226718.1562 - val_loss: 72.6279\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 209417.3438 - val_loss: 71.6801\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 319819.4062 - val_loss: 70.7320\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 753208.4375 - val_loss: 70.1126\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 510225.5312 - val_loss: 69.3588\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 424706.9688 - val_loss: 68.5901\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 764070.7500 - val_loss: 68.0163\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 994302.6250 - val_loss: 67.5297\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 671936.5625 - val_loss: 67.2636\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 437453.4062 - val_loss: 66.8204\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 771128.9375 - val_loss: 66.4533\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1121517.8750 - val_loss: 66.3925\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 376101.2500 - val_loss: 66.1833\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 246052.5469 - val_loss: 65.8051\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 265942.2500 - val_loss: 65.4019\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 249863.7969 - val_loss: 65.0638\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 191916.5469 - val_loss: 64.5574\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 335705.0625 - val_loss: 64.0664\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 728428.5000 - val_loss: 63.7518\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 946861.5625 - val_loss: 63.5776\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 384365.7500 - val_loss: 63.4364\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 200885.7500 - val_loss: 63.1660\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 172367.6875 - val_loss: 62.8582\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 324951.4062 - val_loss: 62.5712\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff790d88b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:41:43.303 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff790d88b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 80 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 95.8255 - val_loss: 76.9134\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 48.4013 - val_loss: 37.4337\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 31.3785 - val_loss: 15.8636\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28.9974 - val_loss: 24.7931\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 22.7965 - val_loss: 31.9897\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20.7929 - val_loss: 23.4179\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17.3262 - val_loss: 12.8808\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16.0095 - val_loss: 17.9639\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.2877 - val_loss: 16.2732\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12.0953 - val_loss: 7.7680\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11.0990 - val_loss: 10.6076\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3864 - val_loss: 5.1063\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6199 - val_loss: 5.8072\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.3519 - val_loss: 4.1282\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.9170 - val_loss: 3.6927\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6068 - val_loss: 3.4897\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.5497 - val_loss: 5.5626\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.6739 - val_loss: 2.6663\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.2085 - val_loss: 6.6792\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.3102 - val_loss: 2.8169\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.5812 - val_loss: 5.2825\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7851 - val_loss: 2.7827\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.7446 - val_loss: 4.3520\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2442 - val_loss: 2.8721\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9223 - val_loss: 2.8465\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.7309 - val_loss: 4.2030\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.6569 - val_loss: 2.6733\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3879 - val_loss: 3.8366\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.5079 - val_loss: 3.3766\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9443 - val_loss: 3.2804\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3286 - val_loss: 2.2150\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.0768 - val_loss: 2.4590\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.7626 - val_loss: 4.4412\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.9958 - val_loss: 3.6243\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.7258 - val_loss: 5.3538\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.9092 - val_loss: 5.4413\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0574 - val_loss: 4.6530\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5878 - val_loss: 3.6173\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0624 - val_loss: 3.5844\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8805 - val_loss: 2.8312\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6115 - val_loss: 3.2767\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4504 - val_loss: 3.0137\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0814 - val_loss: 2.9891\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0804 - val_loss: 2.3244\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.8479 - val_loss: 2.1874\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.8096 - val_loss: 2.0474\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7458 - val_loss: 2.1443\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.7718 - val_loss: 2.1618\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6467 - val_loss: 2.0486\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4837 - val_loss: 2.1729\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79208f160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:41:55.677 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79208f160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 81 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 130ms/step - loss: 5410484.0000 - val_loss: 93.1726\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4237229.5000 - val_loss: 92.7690\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 3590593.2500 - val_loss: 92.4166\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1488002.3750 - val_loss: 91.8438\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 877580.0000 - val_loss: 91.4062\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 380830.3125 - val_loss: 90.7917\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 830515.1875 - val_loss: 90.0871\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 610127.3750 - val_loss: 89.4820\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 210944.6719 - val_loss: 88.8171\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 248303.4688 - val_loss: 88.2605\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 183715.0000 - val_loss: 87.6395\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 233806.7969 - val_loss: 86.9600\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 193274.3438 - val_loss: 86.3245\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 210660.2344 - val_loss: 85.6785\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 215238.1719 - val_loss: 85.0343\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 244955.7969 - val_loss: 84.5455\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 651363.3125 - val_loss: 83.9877\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 560302.9375 - val_loss: 83.5602\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 644958.9375 - val_loss: 83.0284\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 622194.6250 - val_loss: 82.5757\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 522454.5938 - val_loss: 82.0778\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 611083.1875 - val_loss: 81.6324\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 565647.8750 - val_loss: 81.2511\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 548667.9375 - val_loss: 80.8511\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 581924.2500 - val_loss: 80.4589\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 468986.0938 - val_loss: 80.1348\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 549752.1875 - val_loss: 79.7721\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 523849.8438 - val_loss: 79.4496\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 501958.1562 - val_loss: 79.2295\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 404502.6875 - val_loss: 78.9045\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 544260.1875 - val_loss: 78.6596\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 513920.0938 - val_loss: 78.3734\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 393233.1875 - val_loss: 78.1861\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 470218.9375 - val_loss: 77.9161\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 487318.8438 - val_loss: 77.6633\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 481131.5938 - val_loss: 77.4732\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 357386.3438 - val_loss: 77.2869\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 466093.6250 - val_loss: 77.0804\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 438058.0312 - val_loss: 76.8226\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 472600.7812 - val_loss: 76.6357\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 337426.7188 - val_loss: 76.5154\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 441306.0625 - val_loss: 76.3216\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 429466.0000 - val_loss: 76.2103\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 372286.5000 - val_loss: 76.0903\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 102903.1328 - val_loss: 75.9603\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 160737.9219 - val_loss: 75.7395\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 331433.7500 - val_loss: 75.5264\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 503557.6875 - val_loss: 75.4539\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 160657.9688 - val_loss: 75.2962\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 144233.7969 - val_loss: 75.1036\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79a3011f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:42:06.610 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79a3011f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 82 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 522.8430 - val_loss: 114.1402\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 324161.2500 - val_loss: 116.2333\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 236971.1250 - val_loss: 109.2126\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 129223.8438 - val_loss: 102.0774\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7259.7539 - val_loss: 95.6276\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 148399.3906 - val_loss: 92.4801\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 118944.9453 - val_loss: 94.5603\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48954.5312 - val_loss: 98.2321\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 36654.4414 - val_loss: 101.3595\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36497.9258 - val_loss: 100.1467\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11740.6230 - val_loss: 100.0826\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 225.9345 - val_loss: 99.6424\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24316.3984 - val_loss: 99.8094\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4952.7979 - val_loss: 101.5801\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 55194.3633 - val_loss: 102.6604\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 50198.9102 - val_loss: 100.6355\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5680.7651 - val_loss: 100.5373\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 176.5481 - val_loss: 100.1026\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18209.6641 - val_loss: 100.2343\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10195.3828 - val_loss: 101.7676\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 30781.7070 - val_loss: 101.6372\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18638.0840 - val_loss: 100.5392\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21679.0879 - val_loss: 100.0529\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 16712.3164 - val_loss: 101.6925\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 35911.2422 - val_loss: 102.0900\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 22900.7324 - val_loss: 101.0862\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 19084.0996 - val_loss: 100.2345\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10406.9248 - val_loss: 101.9989\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 32859.1523 - val_loss: 102.2182\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 29017.5527 - val_loss: 101.7652\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3930.8721 - val_loss: 99.5308\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32307.9043 - val_loss: 99.0156\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 49453.3203 - val_loss: 99.2608\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24718.6816 - val_loss: 100.4102\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6001.2881 - val_loss: 101.4798\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13458.4482 - val_loss: 100.9858\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11087.3604 - val_loss: 100.8305\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1075.1836 - val_loss: 100.6026\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17570.0000 - val_loss: 100.6045\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 678.5655 - val_loss: 101.7113\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15903.2930 - val_loss: 101.8510\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19080.2441 - val_loss: 100.8381\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13637.2100 - val_loss: 100.7441\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6633.4917 - val_loss: 102.3587\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 36166.3867 - val_loss: 102.3983\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 29968.1582 - val_loss: 101.5374\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2277.8591 - val_loss: 99.9715\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24863.9062 - val_loss: 99.7527\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 30669.3320 - val_loss: 100.4992\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 737.4344 - val_loss: 101.4367\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7e00e5f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:42:17.003 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7e00e5f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 83 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 88.4264 - val_loss: 60.9730\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34.7098 - val_loss: 5.5770\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18.4798 - val_loss: 7.2843\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.0256 - val_loss: 20.5123\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.3276 - val_loss: 14.1395\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.4079 - val_loss: 6.8058\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.3152 - val_loss: 11.8612\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.7700 - val_loss: 13.3250\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.3162 - val_loss: 7.1570\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.6410 - val_loss: 8.9585\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.0326 - val_loss: 10.2486\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6314 - val_loss: 7.1112\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0285 - val_loss: 6.6860\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3256 - val_loss: 6.2809\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8164 - val_loss: 4.8828\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.3986 - val_loss: 3.9933\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.8553 - val_loss: 6.1727\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2442 - val_loss: 3.3787\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9421 - val_loss: 3.4820\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7677 - val_loss: 3.7695\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6511 - val_loss: 3.5692\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.8050 - val_loss: 6.5251\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1762 - val_loss: 3.8070\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8537 - val_loss: 6.2938\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1060 - val_loss: 3.2719\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.4570 - val_loss: 4.5534\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4263 - val_loss: 3.3968\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.3861 - val_loss: 3.2668\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.2757 - val_loss: 4.2007\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.3820 - val_loss: 3.2156\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.2892 - val_loss: 3.2955\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.3995 - val_loss: 4.3492\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.3964 - val_loss: 3.2969\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.3215 - val_loss: 3.9788\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.1659 - val_loss: 3.2857\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0624 - val_loss: 4.0059\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.0823 - val_loss: 3.2825\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.1206 - val_loss: 3.2603\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0918 - val_loss: 3.9547\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.0949 - val_loss: 3.1636\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.9325 - val_loss: 3.4548\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.0130 - val_loss: 3.1533\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.0945 - val_loss: 3.2576\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.9120 - val_loss: 3.5883\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.0772 - val_loss: 3.2251\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.9081 - val_loss: 3.1879\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.1210 - val_loss: 4.8914\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7747 - val_loss: 3.2998\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8692 - val_loss: 3.0719\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4452 - val_loss: 3.8530\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b162e310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:42:26.252 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7b162e310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 84 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 84.6005 - val_loss: 68.2835\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 43.3169 - val_loss: 29.0809\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 42.8929 - val_loss: 25.8300\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33.3000 - val_loss: 35.0985\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30.8322 - val_loss: 32.6915\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26.5834 - val_loss: 19.8155\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23.3389 - val_loss: 14.3134\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21.0954 - val_loss: 16.8813\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18.0770 - val_loss: 9.9014\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.4347 - val_loss: 11.0159\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.3526 - val_loss: 9.1254\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.7280 - val_loss: 11.4646\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15.1685 - val_loss: 9.1443\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.9640 - val_loss: 9.4665\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.3099 - val_loss: 9.5800\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.0860 - val_loss: 9.1608\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.7434 - val_loss: 9.7082\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.7294 - val_loss: 10.3586\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.4991 - val_loss: 9.6972\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13.2507 - val_loss: 9.9090\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.0097 - val_loss: 9.5521\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.1827 - val_loss: 10.1944\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.8539 - val_loss: 9.4238\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.9848 - val_loss: 10.6513\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.1458 - val_loss: 9.1257\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.6162 - val_loss: 9.4247\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.9036 - val_loss: 9.4494\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.3719 - val_loss: 10.4005\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.3314 - val_loss: 9.5593\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.4142 - val_loss: 10.0426\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.0757 - val_loss: 9.4781\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.9841 - val_loss: 9.4902\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.8861 - val_loss: 9.2611\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.0562 - val_loss: 10.0816\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.8912 - val_loss: 9.3663\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.9221 - val_loss: 9.4990\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.7275 - val_loss: 9.2240\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.2655 - val_loss: 9.7208\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.2430 - val_loss: 9.0425\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.7327 - val_loss: 9.0349\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.3617 - val_loss: 8.8022\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.2830 - val_loss: 9.0914\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.5545 - val_loss: 9.1491\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.2681 - val_loss: 9.1320\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.4888 - val_loss: 9.3836\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.7087 - val_loss: 8.9990\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.5784 - val_loss: 9.3649\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.7929 - val_loss: 9.0580\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.8079 - val_loss: 9.1839\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.7709 - val_loss: 8.8039\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7812b8af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:42:35.340 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7812b8af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 85 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 78.1884 - val_loss: 57.0842\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 31.7689 - val_loss: 9.7896\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 27.2412 - val_loss: 16.9217\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20.4396 - val_loss: 28.1428\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19.9942 - val_loss: 24.2757\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.7820 - val_loss: 12.5360\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.5512 - val_loss: 12.2679\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.0106 - val_loss: 16.5779\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.5932 - val_loss: 11.3343\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.8931 - val_loss: 9.0481\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.9056 - val_loss: 6.0837\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.4869 - val_loss: 5.8588\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.5174 - val_loss: 6.3147\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0648 - val_loss: 6.2831\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3168 - val_loss: 8.8458\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.2680 - val_loss: 6.4127\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8506 - val_loss: 7.4542\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.8638 - val_loss: 6.0853\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.1365 - val_loss: 5.7638\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8298 - val_loss: 6.0564\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7527 - val_loss: 6.7034\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8521 - val_loss: 6.2505\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.4742 - val_loss: 6.3041\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1714 - val_loss: 6.2553\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.0745 - val_loss: 6.2338\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9467 - val_loss: 6.1727\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1938 - val_loss: 6.4152\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1795 - val_loss: 6.0707\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.8219 - val_loss: 6.7908\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3008 - val_loss: 7.4236\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4286 - val_loss: 6.5845\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.8601 - val_loss: 6.7289\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4148 - val_loss: 6.2627\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4170 - val_loss: 6.2801\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.8929 - val_loss: 6.1511\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 4.0512 - val_loss: 6.0730\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 3.7858 - val_loss: 6.5292\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.6990 - val_loss: 6.5326\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.3248 - val_loss: 6.3479\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.3814 - val_loss: 6.8854\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4687 - val_loss: 6.1553\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.2634 - val_loss: 6.0578\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8264 - val_loss: 6.5559\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5802 - val_loss: 5.9398\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.4041 - val_loss: 6.1983\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.5228 - val_loss: 5.7907\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0407 - val_loss: 5.9257\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0284 - val_loss: 5.8269\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.9368 - val_loss: 5.9536\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.8446 - val_loss: 5.7925\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7647bb1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:42:44.658 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7647bb1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 86 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 83.5819 - val_loss: 62.8666\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 34.9291 - val_loss: 17.9751\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 30.2856 - val_loss: 11.6861\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 23.0027 - val_loss: 27.6379\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 20.8890 - val_loss: 27.7811\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 18.5827 - val_loss: 16.5893\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 16.0352 - val_loss: 13.3932\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 14.2617 - val_loss: 15.3762\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 12.5380 - val_loss: 12.4144\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11.4363 - val_loss: 9.0943\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.1459 - val_loss: 6.8304\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.6944 - val_loss: 6.6962\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.1602 - val_loss: 6.4927\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.8698 - val_loss: 6.4554\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2394 - val_loss: 6.5528\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8596 - val_loss: 6.5764\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.7102 - val_loss: 6.7579\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.8079 - val_loss: 6.6434\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3313 - val_loss: 6.7264\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.5217 - val_loss: 6.6825\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.3352 - val_loss: 6.5423\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.5522 - val_loss: 6.4055\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.4207 - val_loss: 7.3426\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.8638 - val_loss: 6.7077\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.6898 - val_loss: 6.5070\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0852 - val_loss: 6.3782\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.8651 - val_loss: 6.3075\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9775 - val_loss: 6.2112\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9714 - val_loss: 6.1886\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.8111 - val_loss: 6.2274\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.6538 - val_loss: 6.0026\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4332 - val_loss: 6.0555\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3984 - val_loss: 6.2068\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.3197 - val_loss: 6.2182\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.4670 - val_loss: 6.0216\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7263 - val_loss: 6.6308\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.2507 - val_loss: 5.8736\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4323 - val_loss: 6.2951\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.6764 - val_loss: 5.7934\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3975 - val_loss: 5.6963\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.1500 - val_loss: 6.0934\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.1385 - val_loss: 5.4871\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9595 - val_loss: 5.8665\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8326 - val_loss: 5.5847\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7902 - val_loss: 5.9810\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0635 - val_loss: 5.4076\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9507 - val_loss: 5.2211\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.1995 - val_loss: 6.9292\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.7812 - val_loss: 5.1351\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8377 - val_loss: 6.5963\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7560101f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:42:56.727 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7560101f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 87 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 253947.6562 - val_loss: 68.7182\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 117183.1719 - val_loss: 86.3387\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 96802.4766 - val_loss: 93.6331\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 97023.0312 - val_loss: 91.7699\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37782.2969 - val_loss: 86.7212\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 32681.6680 - val_loss: 84.3857\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34123.4492 - val_loss: 85.8477\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 18821.4902 - val_loss: 87.4958\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4390.9307 - val_loss: 87.3652\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10407.2715 - val_loss: 85.9117\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20247.2500 - val_loss: 86.9720\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13091.1973 - val_loss: 87.1817\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7764.2188 - val_loss: 86.8494\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3764.0840 - val_loss: 89.8111\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 39861.0117 - val_loss: 88.9100\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20806.7969 - val_loss: 86.6229\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10657.5293 - val_loss: 87.0656\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2896.7698 - val_loss: 88.5574\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 39701.8984 - val_loss: 89.6775\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24533.8574 - val_loss: 87.7642\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23989.5469 - val_loss: 86.4869\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16832.0508 - val_loss: 89.2932\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 29441.0723 - val_loss: 89.3164\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16797.9688 - val_loss: 87.3649\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19600.5469 - val_loss: 87.4612\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12091.0566 - val_loss: 90.2754\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 36417.9609 - val_loss: 90.0528\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24649.5195 - val_loss: 87.3692\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 42248.8008 - val_loss: 86.5345\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 28424.3125 - val_loss: 90.1796\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 45032.9805 - val_loss: 91.7360\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 40737.0742 - val_loss: 90.0159\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9533.9785 - val_loss: 86.8608\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 67599.5859 - val_loss: 85.9473\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 62084.4609 - val_loss: 88.0930\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 31312.5430 - val_loss: 91.3587\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 41658.8438 - val_loss: 92.5406\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 34177.4062 - val_loss: 91.9347\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6128.8999 - val_loss: 90.5997\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 305.0580 - val_loss: 92.5140\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 50722.6094 - val_loss: 93.4310\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 41550.4453 - val_loss: 91.5137\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16175.4922 - val_loss: 90.6313\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2876.3940 - val_loss: 92.0724\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26591.5215 - val_loss: 92.7851\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25088.1309 - val_loss: 91.7541\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 435.0313 - val_loss: 91.9953\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8798.9609 - val_loss: 91.3114\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14973.2285 - val_loss: 91.2903\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5069.8452 - val_loss: 92.9765\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff75c301b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:43:09.336 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff75c301b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 88 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 155ms/step - loss: 80630.2500 - val_loss: 109.4801\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 153768.6250 - val_loss: 110.4081\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 85303.8516 - val_loss: 106.5534\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11224.5410 - val_loss: 104.0827\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3630.9531 - val_loss: 103.1970\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36420.9961 - val_loss: 102.8867\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6598.2373 - val_loss: 104.1797\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 55381.9492 - val_loss: 106.0486\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 55033.8828 - val_loss: 105.3646\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 28211.6445 - val_loss: 101.6429\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 43550.0117 - val_loss: 101.0312\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 61088.7969 - val_loss: 101.7866\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 33653.6055 - val_loss: 104.1157\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48186.8984 - val_loss: 105.2120\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22483.1934 - val_loss: 103.7475\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17013.5781 - val_loss: 102.4132\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 25809.9863 - val_loss: 102.9204\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7745.1802 - val_loss: 104.8676\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 64141.4023 - val_loss: 105.4427\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 55198.8281 - val_loss: 104.3025\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17165.6562 - val_loss: 102.4962\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 42404.5312 - val_loss: 101.5817\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40270.8242 - val_loss: 102.5624\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 180.5218 - val_loss: 102.7978\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14081.3672 - val_loss: 102.8303\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4750.4092 - val_loss: 104.4511\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 46534.5938 - val_loss: 104.4594\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 40277.3203 - val_loss: 103.5936\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19967.0840 - val_loss: 101.7934\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 35713.0859 - val_loss: 101.5130\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34080.7773 - val_loss: 102.2372\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4719.8428 - val_loss: 102.5271\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6530.4819 - val_loss: 102.5381\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2676.9648 - val_loss: 102.2843\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16459.1934 - val_loss: 102.2377\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7354.6069 - val_loss: 103.2012\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25359.5156 - val_loss: 103.1867\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 17097.0430 - val_loss: 102.6539\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4047.3137 - val_loss: 101.0910\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 49935.8438 - val_loss: 100.8728\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 49743.7148 - val_loss: 101.5749\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 29014.1387 - val_loss: 102.9639\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17227.3691 - val_loss: 103.0072\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12633.3545 - val_loss: 102.6141\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8891.0811 - val_loss: 102.0759\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9164.0391 - val_loss: 102.7859\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13140.4775 - val_loss: 102.4437\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 893.3293 - val_loss: 102.4788\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11283.5020 - val_loss: 102.6181\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9976.8232 - val_loss: 102.0970\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79d070310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:43:20.194 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff79d070310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 89 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 98.6601 - val_loss: 81.3595\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 52.9241 - val_loss: 44.1778\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32.8014 - val_loss: 15.2882\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30.5593 - val_loss: 22.3346\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23.3534 - val_loss: 28.1718\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 21.2487 - val_loss: 21.4015\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18.4363 - val_loss: 12.0336\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16.7597 - val_loss: 16.8427\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15.3605 - val_loss: 11.3546\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.2898 - val_loss: 10.6319\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.2526 - val_loss: 10.9526\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.4517 - val_loss: 7.4387\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.7025 - val_loss: 8.3433\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.2068 - val_loss: 6.4647\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.9927 - val_loss: 7.8009\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.3182 - val_loss: 6.5737\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.3784 - val_loss: 8.6359\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.7683 - val_loss: 6.9267\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.5383 - val_loss: 6.2589\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.2861 - val_loss: 7.6313\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.9468 - val_loss: 6.0455\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7786 - val_loss: 7.3365\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.6087 - val_loss: 5.9194\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.4881 - val_loss: 7.3439\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.5566 - val_loss: 5.5707\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.3928 - val_loss: 6.7354\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.2358 - val_loss: 5.7452\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.4297 - val_loss: 6.6403\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7491 - val_loss: 6.0704\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5512 - val_loss: 5.2219\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.9271 - val_loss: 7.0932\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8406 - val_loss: 5.0628\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3775 - val_loss: 6.7623\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.6678 - val_loss: 4.8954\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2635 - val_loss: 5.8374\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2544 - val_loss: 5.2062\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1358 - val_loss: 5.2991\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.3638 - val_loss: 4.9838\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.1857 - val_loss: 4.7589\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.6226 - val_loss: 5.3091\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8078 - val_loss: 4.6326\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.5749 - val_loss: 5.0298\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3654 - val_loss: 4.6961\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.3665 - val_loss: 4.4326\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4939 - val_loss: 4.6793\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2171 - val_loss: 4.1558\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1003 - val_loss: 4.3859\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0977 - val_loss: 3.8260\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3752 - val_loss: 5.4287\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4186 - val_loss: 4.3120\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7a9567430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:43:29.732 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7a9567430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 90 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 118515.9844 - val_loss: 101.5775\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 152657.2500 - val_loss: 103.2702\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 71331.2734 - val_loss: 99.2120\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6223.3618 - val_loss: 95.5711\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16497.9082 - val_loss: 97.6036\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26494.5000 - val_loss: 97.0275\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3491.3640 - val_loss: 97.1632\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10526.1611 - val_loss: 97.2991\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12758.6484 - val_loss: 94.7237\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 46839.7031 - val_loss: 94.1539\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 34922.2188 - val_loss: 98.6305\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31527.4941 - val_loss: 98.2366\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24681.8906 - val_loss: 95.0986\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23332.9297 - val_loss: 96.1738\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5873.0459 - val_loss: 99.1922\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 44968.8438 - val_loss: 99.0907\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30057.1680 - val_loss: 96.7902\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10782.8486 - val_loss: 96.7977\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 235.0643 - val_loss: 95.4878\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 45216.8594 - val_loss: 94.6740\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20946.5820 - val_loss: 98.5701\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33912.8828 - val_loss: 99.6187\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37381.7695 - val_loss: 97.8487\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7618.0815 - val_loss: 94.1494\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 74613.8125 - val_loss: 93.0628\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 63131.1445 - val_loss: 94.7250\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35566.4219 - val_loss: 98.6214\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36646.1992 - val_loss: 99.8741\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36341.5117 - val_loss: 99.0297\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21441.2871 - val_loss: 95.6503\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 57120.3008 - val_loss: 94.5562\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 51329.7852 - val_loss: 96.1381\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17169.3555 - val_loss: 98.8088\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 38594.0195 - val_loss: 100.1693\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37079.1094 - val_loss: 98.4283\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1976.7325 - val_loss: 96.0861\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 58574.1914 - val_loss: 94.9611\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 54133.3203 - val_loss: 97.3877\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11584.9609 - val_loss: 100.2384\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 63261.6641 - val_loss: 101.4183\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 59443.0508 - val_loss: 99.1829\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1283.1941 - val_loss: 97.3904\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 39438.2500 - val_loss: 95.5725\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 43672.5234 - val_loss: 96.4821\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19504.0547 - val_loss: 98.1273\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13740.3379 - val_loss: 99.1733\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9857.5732 - val_loss: 98.4851\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2000.7184 - val_loss: 98.6221\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1214.3464 - val_loss: 97.9025\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23924.9707 - val_loss: 97.6157\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff728195b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:43:40.347 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff728195b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 91 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 77.4914 - val_loss: 62.8572\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 55.9053 - val_loss: 47.9596\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 41.2530 - val_loss: 40.9564\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 31.1976 - val_loss: 26.1921\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 25.0184 - val_loss: 16.8882\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15.5722 - val_loss: 8.2916\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13.8046 - val_loss: 6.8150\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 15.6781 - val_loss: 7.4119\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 13.7320 - val_loss: 12.2540\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 14.1553 - val_loss: 5.3965\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 13.0404 - val_loss: 7.4545\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12.1373 - val_loss: 6.2978\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 11.1014 - val_loss: 5.8313\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10.8311 - val_loss: 6.1448\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10.7497 - val_loss: 5.5918\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10.7932 - val_loss: 5.7916\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10.7665 - val_loss: 5.5756\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2711 - val_loss: 5.3809\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.5007 - val_loss: 6.4391\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.0359 - val_loss: 5.2416\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.9989 - val_loss: 6.1226\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.4314 - val_loss: 4.9249\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.8502 - val_loss: 6.3715\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.1490 - val_loss: 4.8832\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.1549 - val_loss: 5.2718\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.6500 - val_loss: 4.7727\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.9736 - val_loss: 5.4593\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.8681 - val_loss: 4.5337\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.9618 - val_loss: 4.9634\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.4221 - val_loss: 4.6983\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.2338 - val_loss: 4.7689\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.1806 - val_loss: 4.5444\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4184 - val_loss: 4.3190\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.5692 - val_loss: 4.5311\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.6800 - val_loss: 4.6774\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.6718 - val_loss: 4.2723\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.9882 - val_loss: 4.4052\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.4690 - val_loss: 4.4970\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.2665 - val_loss: 4.0874\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.6224 - val_loss: 4.2125\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.1207 - val_loss: 4.1999\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.9835 - val_loss: 4.0313\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.5442 - val_loss: 4.1968\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.6546 - val_loss: 4.0703\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.2195 - val_loss: 4.5864\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.1935 - val_loss: 4.1867\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.1410 - val_loss: 4.4272\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.1884 - val_loss: 4.5161\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.0943 - val_loss: 3.9420\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.6038 - val_loss: 3.9735\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73c8681f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:43:51.274 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff73c8681f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 92 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 90.2320 - val_loss: 69.4825\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 45.6176 - val_loss: 23.8177\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 27.7233 - val_loss: 3.5690\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 25.6670 - val_loss: 16.3061\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 20.5691 - val_loss: 25.1666\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 18.0984 - val_loss: 13.3378\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 16.2019 - val_loss: 7.2268\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15.0028 - val_loss: 11.3618\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.7209 - val_loss: 8.6836\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.8992 - val_loss: 7.6469\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.8830 - val_loss: 4.6045\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.0854 - val_loss: 4.5708\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3394 - val_loss: 4.3600\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.7594 - val_loss: 4.4253\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.3853 - val_loss: 4.5872\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.1625 - val_loss: 4.7158\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.9984 - val_loss: 4.5037\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.9701 - val_loss: 4.6334\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.4972 - val_loss: 4.4213\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.3508 - val_loss: 4.4883\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 8.0088 - val_loss: 4.2193\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.6671 - val_loss: 4.4176\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.6527 - val_loss: 4.1569\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.5981 - val_loss: 4.7226\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.7832 - val_loss: 4.1737\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2799 - val_loss: 3.9936\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.0960 - val_loss: 3.9696\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.8043 - val_loss: 4.4314\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6328 - val_loss: 4.0108\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9114 - val_loss: 3.8565\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.7560 - val_loss: 3.8193\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.5566 - val_loss: 3.6773\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3476 - val_loss: 4.0338\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.0475 - val_loss: 3.5362\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5190 - val_loss: 3.7345\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4548 - val_loss: 3.6432\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.2613 - val_loss: 3.4065\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5004 - val_loss: 3.6949\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6168 - val_loss: 3.3919\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0639 - val_loss: 3.2162\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8937 - val_loss: 3.2358\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.3072 - val_loss: 3.9934\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9260 - val_loss: 3.6959\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6857 - val_loss: 4.3675\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4125 - val_loss: 3.2935\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.2845 - val_loss: 5.5185\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.1162 - val_loss: 3.1212\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3566 - val_loss: 3.5452\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.2722 - val_loss: 3.3228\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.1387 - val_loss: 3.0476\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff711b88550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:44:04.159 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff711b88550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 93 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 133ms/step - loss: 60.9108 - val_loss: 40.1425\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 46.8958 - val_loss: 33.2845\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 36.4506 - val_loss: 42.9069\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34.0563 - val_loss: 37.2809\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 28.8076 - val_loss: 21.7647\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 25.5295 - val_loss: 13.6218\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20.5783 - val_loss: 16.2527\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17.5150 - val_loss: 3.3155\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 18.5318 - val_loss: 8.1861\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.1854 - val_loss: 7.1718\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.6894 - val_loss: 5.2542\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.0216 - val_loss: 5.4422\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.7927 - val_loss: 4.4328\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.2579 - val_loss: 3.0080\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13.3141 - val_loss: 4.4162\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.6056 - val_loss: 3.1649\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.3522 - val_loss: 2.7944\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.7192 - val_loss: 3.2817\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.3673 - val_loss: 3.1015\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.3263 - val_loss: 2.9722\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.8928 - val_loss: 2.9486\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5610 - val_loss: 3.5517\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.3135 - val_loss: 2.9696\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.0094 - val_loss: 3.3044\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5602 - val_loss: 3.2857\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.5071 - val_loss: 3.1051\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4867 - val_loss: 4.2660\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.3346 - val_loss: 3.1091\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.7203 - val_loss: 3.8911\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.1194 - val_loss: 3.5249\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9.1197 - val_loss: 3.9490\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.8783 - val_loss: 2.6067\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.9194 - val_loss: 2.3591\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5594 - val_loss: 2.9445\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.1337 - val_loss: 2.5108\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9554 - val_loss: 4.3654\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.0011 - val_loss: 2.2478\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.8241 - val_loss: 3.6738\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.4735 - val_loss: 2.1789\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.5954 - val_loss: 3.3172\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.4645 - val_loss: 3.4459\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.9237 - val_loss: 2.1118\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9382 - val_loss: 3.1115\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0382 - val_loss: 2.3701\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9291 - val_loss: 2.9836\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.4747 - val_loss: 2.0029\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.5537 - val_loss: 3.5175\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.7569 - val_loss: 2.1733\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3464 - val_loss: 2.9317\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4059 - val_loss: 2.0608\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7aba8ef70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:44:14.622 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7aba8ef70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 94 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 81.6240 - val_loss: 59.9384\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36.5882 - val_loss: 9.5019\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31.5941 - val_loss: 9.6463\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23.5354 - val_loss: 22.8796\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21.8902 - val_loss: 21.0932\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18.2335 - val_loss: 7.9505\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.8408 - val_loss: 5.9650\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 14.0128 - val_loss: 6.9506\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.0521 - val_loss: 9.0292\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.6771 - val_loss: 6.3082\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.4899 - val_loss: 6.2960\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5802 - val_loss: 6.4166\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.1724 - val_loss: 6.2820\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.3128 - val_loss: 6.1494\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.8366 - val_loss: 6.8711\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 8.0187 - val_loss: 5.7557\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 7.6560 - val_loss: 5.5583\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.4706 - val_loss: 6.7553\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.7617 - val_loss: 5.4147\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8.4457 - val_loss: 5.3852\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.9910 - val_loss: 6.2473\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.6886 - val_loss: 5.3268\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.5302 - val_loss: 8.9510\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.0459 - val_loss: 5.3773\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.2306 - val_loss: 5.1331\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.3286 - val_loss: 6.6208\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.2254 - val_loss: 4.9880\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.0822 - val_loss: 5.1369\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.5036 - val_loss: 5.1071\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.0371 - val_loss: 6.3245\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.9772 - val_loss: 4.8965\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7103 - val_loss: 4.6850\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.8750 - val_loss: 4.6936\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.7335 - val_loss: 4.8495\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4142 - val_loss: 4.4764\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5547 - val_loss: 4.6912\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.3169 - val_loss: 4.3825\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.5258 - val_loss: 4.3494\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4793 - val_loss: 5.3487\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4840 - val_loss: 4.2690\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.4375 - val_loss: 4.4703\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8603 - val_loss: 4.8745\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8116 - val_loss: 4.2731\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9077 - val_loss: 4.4863\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0609 - val_loss: 4.0932\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9448 - val_loss: 4.9907\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.1707 - val_loss: 4.0728\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0766 - val_loss: 4.3582\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.9608 - val_loss: 4.2076\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0491 - val_loss: 3.8622\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff759033700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:44:25.311 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff759033700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 95 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 68.2502 - val_loss: 49.3699\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37.4478 - val_loss: 15.8561\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 32.4136 - val_loss: 24.3951\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24.1626 - val_loss: 23.7154\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 18.7665 - val_loss: 10.4209\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.7549 - val_loss: 11.6951\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.4892 - val_loss: 5.8079\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.5955 - val_loss: 5.5291\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12.6186 - val_loss: 6.2627\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.0143 - val_loss: 5.2208\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.8233 - val_loss: 5.5889\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.5190 - val_loss: 5.2784\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.2723 - val_loss: 4.8259\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 11.2747 - val_loss: 5.0493\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 11.0837 - val_loss: 6.2002\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.2923 - val_loss: 4.5897\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.2251 - val_loss: 6.4142\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.0166 - val_loss: 4.5716\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.1589 - val_loss: 5.8070\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2937 - val_loss: 4.6096\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.7855 - val_loss: 4.7653\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.8038 - val_loss: 4.2323\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.7052 - val_loss: 4.1558\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.5475 - val_loss: 5.8128\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.5646 - val_loss: 4.0890\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4370 - val_loss: 4.5340\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2749 - val_loss: 6.6701\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2611 - val_loss: 4.1082\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4487 - val_loss: 5.6308\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.7860 - val_loss: 4.1561\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.8066 - val_loss: 4.1508\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3706 - val_loss: 3.8495\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5272 - val_loss: 4.3973\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.2895 - val_loss: 4.2902\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.0886 - val_loss: 3.9517\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.9947 - val_loss: 3.7855\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9981 - val_loss: 4.2112\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7120 - val_loss: 3.7107\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.8894 - val_loss: 3.8794\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 7.7008 - val_loss: 4.8051\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8880 - val_loss: 3.5687\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.3623 - val_loss: 4.2235\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2565 - val_loss: 3.5925\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.0017 - val_loss: 3.4765\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1551 - val_loss: 4.2192\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8914 - val_loss: 3.5422\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8884 - val_loss: 3.4629\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9554 - val_loss: 3.6006\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9628 - val_loss: 4.4329\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0495 - val_loss: 4.1984\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6f160a4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:44:35.122 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6f160a4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 96 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 75.6273 - val_loss: 51.0691\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27.1510 - val_loss: 3.3192\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 29.8115 - val_loss: 17.2696\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 17.3795 - val_loss: 33.3386\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 18.2776 - val_loss: 20.3745\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 13.9937 - val_loss: 13.5838\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 14.2850 - val_loss: 17.1763\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.6633 - val_loss: 21.0696\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 10.6763 - val_loss: 12.2471\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.2141 - val_loss: 13.8337\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.0232 - val_loss: 7.8330\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.1965 - val_loss: 9.7495\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.1161 - val_loss: 5.2657\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.6599 - val_loss: 8.3847\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.5218 - val_loss: 3.5884\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.7396 - val_loss: 8.6339\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4022 - val_loss: 5.9653\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9562 - val_loss: 6.0317\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8894 - val_loss: 4.9786\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9130 - val_loss: 4.1949\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2443 - val_loss: 8.0990\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0915 - val_loss: 3.1783\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.0021 - val_loss: 7.0318\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3908 - val_loss: 3.0282\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9679 - val_loss: 4.8984\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.7865 - val_loss: 2.9927\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4511 - val_loss: 3.1064\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4723 - val_loss: 3.7879\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2299 - val_loss: 2.9695\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0416 - val_loss: 3.4830\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0248 - val_loss: 2.9666\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.9679 - val_loss: 2.9017\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.7846 - val_loss: 2.9223\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7036 - val_loss: 2.9698\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6495 - val_loss: 3.0763\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.5983 - val_loss: 3.1312\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.5177 - val_loss: 2.9112\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5478 - val_loss: 4.0807\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.7379 - val_loss: 2.8672\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6615 - val_loss: 2.9307\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8120 - val_loss: 3.4092\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2868 - val_loss: 2.9668\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4212 - val_loss: 2.7769\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.5146 - val_loss: 4.1656\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9626 - val_loss: 3.8190\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0625 - val_loss: 3.4554\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.2782 - val_loss: 4.0396\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.3268 - val_loss: 2.5563\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2863 - val_loss: 3.3041\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.0372 - val_loss: 2.7854\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70b2b4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:44:46.024 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff70b2b4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 97 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 61.1322 - val_loss: 32.1548\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18.3909 - val_loss: 8.1380\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19.4327 - val_loss: 8.2305\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.8094 - val_loss: 19.1334\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14.5245 - val_loss: 11.5246\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.0034 - val_loss: 4.9467\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.5838 - val_loss: 3.5688\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.5228 - val_loss: 8.5215\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5929 - val_loss: 5.6537\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1245 - val_loss: 2.2969\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4954 - val_loss: 2.4842\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0388 - val_loss: 3.4163\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5260 - val_loss: 3.6374\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.6322 - val_loss: 3.3980\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8515 - val_loss: 3.2098\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2540 - val_loss: 3.0132\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9717 - val_loss: 2.7224\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.1621 - val_loss: 4.2848\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2406 - val_loss: 2.3791\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8595 - val_loss: 2.6759\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6391 - val_loss: 2.9616\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.6907 - val_loss: 3.2564\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6872 - val_loss: 4.1512\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8409 - val_loss: 2.6160\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.1463 - val_loss: 2.8558\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6754 - val_loss: 5.5681\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9956 - val_loss: 2.3988\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8816 - val_loss: 3.4078\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.8613 - val_loss: 5.6671\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.7365 - val_loss: 3.3509\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.5179 - val_loss: 5.4929\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5336 - val_loss: 2.8171\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.9956 - val_loss: 4.2836\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.4969 - val_loss: 2.5871\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.5145 - val_loss: 2.9017\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6519 - val_loss: 5.0902\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7881 - val_loss: 2.4074\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.0925 - val_loss: 6.7499\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.5023 - val_loss: 3.2979\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4501 - val_loss: 4.0765\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.4762 - val_loss: 2.5065\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4570 - val_loss: 5.4470\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.5629 - val_loss: 2.2524\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.5232 - val_loss: 5.2506\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.6062 - val_loss: 2.4268\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.3221 - val_loss: 3.0176\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.1032 - val_loss: 2.9885\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.9684 - val_loss: 2.8463\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.0322 - val_loss: 2.9370\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.1654 - val_loss: 4.2770\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7158840d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:44:58.511 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7158840d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 98 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 93.3975 - val_loss: 68.1052\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 50.2662 - val_loss: 26.6147\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 39.9212 - val_loss: 20.2748\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31.4546 - val_loss: 27.6527\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26.8201 - val_loss: 26.5628\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19.8515 - val_loss: 13.5293\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20.1795 - val_loss: 14.4822\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 18.4223 - val_loss: 21.0116\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16.3379 - val_loss: 11.4069\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16.1944 - val_loss: 11.7368\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15.5122 - val_loss: 16.0733\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.2461 - val_loss: 11.5980\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.8678 - val_loss: 11.2470\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.8494 - val_loss: 14.2744\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.1904 - val_loss: 11.5022\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.2555 - val_loss: 11.7357\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13.4118 - val_loss: 12.1739\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.5998 - val_loss: 10.4260\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.7455 - val_loss: 11.4081\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.3956 - val_loss: 10.3876\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.9075 - val_loss: 10.3672\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.6981 - val_loss: 9.9264\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.5548 - val_loss: 10.0229\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.3708 - val_loss: 9.7281\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.9393 - val_loss: 9.7260\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.8554 - val_loss: 9.8948\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.6576 - val_loss: 9.8425\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5255 - val_loss: 9.7119\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.6800 - val_loss: 9.4150\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.7362 - val_loss: 9.2361\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.8332 - val_loss: 9.3949\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.8482 - val_loss: 9.1281\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.6084 - val_loss: 9.1831\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.3878 - val_loss: 9.4137\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.5630 - val_loss: 8.9051\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1341 - val_loss: 8.8826\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6077 - val_loss: 10.4455\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5862 - val_loss: 9.4378\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.5516 - val_loss: 8.7605\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.9903 - val_loss: 8.6745\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7756 - val_loss: 8.4861\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.0311 - val_loss: 11.2364\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.1030 - val_loss: 8.5258\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5831 - val_loss: 8.3640\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1552 - val_loss: 8.3656\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.4543 - val_loss: 8.2463\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6917 - val_loss: 8.7567\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.0282 - val_loss: 8.1857\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.6260 - val_loss: 8.1817\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.3168 - val_loss: 8.0710\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff726727e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:45:08.842 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff726727e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 99 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 9125.9053 - val_loss: 90.6525\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 307258.8750 - val_loss: 89.8260\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 200736.9844 - val_loss: 93.9724\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 168610.0156 - val_loss: 103.5292\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45684.3008 - val_loss: 106.0744\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 47531.0117 - val_loss: 104.6058\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2883.7625 - val_loss: 100.6376\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 76318.2734 - val_loss: 99.2124\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 74382.1484 - val_loss: 101.4118\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10307.4668 - val_loss: 103.7439\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 41726.1250 - val_loss: 104.7911\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39075.3633 - val_loss: 103.5964\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19368.7129 - val_loss: 99.6837\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 90596.9141 - val_loss: 98.0053\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 78951.2969 - val_loss: 100.4821\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5470.2788 - val_loss: 103.0651\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48376.5117 - val_loss: 104.3054\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 52786.7031 - val_loss: 103.1640\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14443.4951 - val_loss: 101.0619\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20459.8555 - val_loss: 100.3504\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24462.5996 - val_loss: 100.9986\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2692.2349 - val_loss: 103.7148\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 46105.8477 - val_loss: 103.8650\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 51055.6211 - val_loss: 103.1639\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29563.7441 - val_loss: 100.4768\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35338.5508 - val_loss: 99.6915\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31267.2129 - val_loss: 100.2714\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10655.3584 - val_loss: 103.4411\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 66524.3125 - val_loss: 104.5043\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 68807.7266 - val_loss: 103.0524\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 25931.4355 - val_loss: 101.7361\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13555.6064 - val_loss: 98.1474\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 61008.5508 - val_loss: 97.4448\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 76781.0938 - val_loss: 98.1645\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 54959.4727 - val_loss: 99.8848\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14400.2266 - val_loss: 102.1576\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36488.7969 - val_loss: 102.6227\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 39021.6367 - val_loss: 101.6447\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4083.7949 - val_loss: 100.4830\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34305.7383 - val_loss: 99.1483\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 36621.6445 - val_loss: 99.9759\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9933.0938 - val_loss: 101.4024\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33377.0547 - val_loss: 102.0881\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 30821.9004 - val_loss: 100.7251\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3112.2729 - val_loss: 100.8750\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9949.7236 - val_loss: 100.7785\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9393.8867 - val_loss: 100.3296\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5299.7002 - val_loss: 100.9625\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12392.2715 - val_loss: 100.9969\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6365.4814 - val_loss: 100.1017\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7070b2e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:45:18.461 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7070b2e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 100 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 81.6325 - val_loss: 61.3906\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36.3871 - val_loss: 9.7775\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20.4176 - val_loss: 5.4624\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.4361 - val_loss: 21.8795\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.3197 - val_loss: 19.3308\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.3458 - val_loss: 8.1972\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.6913 - val_loss: 8.8648\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2091 - val_loss: 13.3221\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.9111 - val_loss: 11.9186\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.0976 - val_loss: 8.2333\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6943 - val_loss: 8.0510\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9552 - val_loss: 8.7335\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3690 - val_loss: 7.6076\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6198 - val_loss: 6.3151\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8004 - val_loss: 4.7424\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.2838 - val_loss: 3.9191\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.8220 - val_loss: 3.8246\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6484 - val_loss: 3.8675\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.6186 - val_loss: 4.1917\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.7942 - val_loss: 4.1870\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.7350 - val_loss: 3.7654\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.5035 - val_loss: 3.8427\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.5805 - val_loss: 3.8447\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.4385 - val_loss: 3.6173\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.2276 - val_loss: 4.3349\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.3113 - val_loss: 3.5854\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.0853 - val_loss: 3.7749\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0908 - val_loss: 3.7014\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.0629 - val_loss: 3.5497\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.9956 - val_loss: 3.5283\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0938 - val_loss: 3.7225\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.4770 - val_loss: 3.8162\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.3315 - val_loss: 4.0662\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.1653 - val_loss: 3.4060\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.1103 - val_loss: 3.3861\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.8725 - val_loss: 3.9006\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.9633 - val_loss: 3.4347\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0485 - val_loss: 3.4640\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2.7333 - val_loss: 3.4237\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.7534 - val_loss: 3.3140\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.6970 - val_loss: 3.3578\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.7383 - val_loss: 3.3779\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.6641 - val_loss: 3.5056\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.7350 - val_loss: 3.5304\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2.8969 - val_loss: 3.6477\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.0221 - val_loss: 3.5389\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.1913 - val_loss: 3.5225\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4277 - val_loss: 3.9137\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0997 - val_loss: 3.2941\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.6436 - val_loss: 3.7793\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6f395b670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:45:27.958 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6f395b670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 101 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 80.4531 - val_loss: 57.6324\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 38.0985 - val_loss: 17.7518\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33.0229 - val_loss: 29.7450\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26.0693 - val_loss: 34.9383\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23.0370 - val_loss: 21.0752\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 18.5240 - val_loss: 11.1777\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.8545 - val_loss: 14.3819\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.3828 - val_loss: 5.7737\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.6508 - val_loss: 8.0622\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.1031 - val_loss: 6.1235\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.0376 - val_loss: 5.6723\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.9971 - val_loss: 4.3231\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.8052 - val_loss: 5.9683\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.9254 - val_loss: 2.7828\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.5660 - val_loss: 4.2184\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.4607 - val_loss: 2.1734\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4301 - val_loss: 2.0036\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.0546 - val_loss: 4.3030\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.4538 - val_loss: 2.1015\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5281 - val_loss: 2.3151\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.0775 - val_loss: 2.3819\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9686 - val_loss: 1.7989\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.7574 - val_loss: 2.2024\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7943 - val_loss: 2.6472\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3935 - val_loss: 1.7116\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.4096 - val_loss: 1.5649\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9609 - val_loss: 2.0441\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.8622 - val_loss: 1.6087\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8669 - val_loss: 1.7782\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6914 - val_loss: 1.8830\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3953 - val_loss: 1.6465\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1869 - val_loss: 1.9198\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3458 - val_loss: 1.9659\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0226 - val_loss: 1.6509\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8126 - val_loss: 2.3562\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0267 - val_loss: 5.3623\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.2494 - val_loss: 2.4206\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9201 - val_loss: 2.8906\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8298 - val_loss: 1.5895\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.9386 - val_loss: 2.1396\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4253 - val_loss: 2.4188\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2629 - val_loss: 1.8980\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.1983 - val_loss: 1.8783\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.1782 - val_loss: 1.9208\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4231 - val_loss: 1.9899\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8479 - val_loss: 2.3264\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.3593 - val_loss: 2.1423\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2793 - val_loss: 1.7231\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0765 - val_loss: 3.1863\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0433 - val_loss: 1.8297\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6f4808310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 11:45:37.643 WARNING tensorflow: 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6f4808310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 102 finished\n"
     ]
    }
   ],
   "source": [
    "zipcodes = nv_zipcodes\n",
    "dict_mape = {}\n",
    "dict_pred = {}\n",
    "\n",
    "for zipcode in range(len(zipcodes)):\n",
    "\n",
    "    # init a RMM model\n",
    "    rnn_model = Sequential()\n",
    "    # add 4 layers of RNN and a last layer\n",
    "\n",
    "    # we define shape on first layer, (60,1) because we use 60 inputs per prediction\n",
    "    rnn_model.add(LSTM(units= 60, return_sequences = False, input_shape=((60,1))))\n",
    "    #rnn_model.add(Dropout(.1))\n",
    "\n",
    "    # 3 other layers\n",
    "    #rnn_model.add(LSTM(units= 30, return_sequences = True))\n",
    "    #rnn_model.add(Dropout(.1))\n",
    "\n",
    "    # return_sequence is False because we want only 1 output after this layer\n",
    "    #rnn_model.add(LSTM(units= 60, return_sequences = False))\n",
    "    #rnn_model.add(Dropout(.1))\n",
    "\n",
    "    # last layer \n",
    "\n",
    "    rnn_model.add(Dense(units=1))\n",
    "\n",
    "    # compile - because this is a regression model we want to minimize MSE\n",
    "\n",
    "    rnn_model.compile(optimizer='adam', loss='mean_absolute_percentage_error')\n",
    "\n",
    "    # We get only the specific column(Zipcode from our train and test datas)\n",
    "    train_data = train.iloc[:,zipcode:zipcode+1].values.astype(int)\n",
    "    test_data = test.iloc[:,zipcode:zipcode+1].values.astype(int)\n",
    "    \n",
    "    # We are using normalizaion rather than standascaler. \n",
    "    # In a upward trending timeseries it is better to not start from negative\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    train_data_scaled = scaler.fit_transform(train_data)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "    # Because we are using 60 previous values to model and predict the next value, \n",
    "    # We set X_train from arrays of 60 for each y_train value\n",
    "    # Same idea for test data sets\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(60,len(train_data_scaled)):\n",
    "        X_train.append(train_data_scaled[i-60:i])\n",
    "        y_train.append(train_data_scaled[i])\n",
    "\n",
    "    data_total = pd.concat((train.iloc[:,zipcode:zipcode+1], test.iloc[:,zipcode:zipcode+1]),axis=0)\n",
    "    inputs = data_total[len(train)-60:].values\n",
    "    inputs = scaler.transform(inputs)\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(60,len(inputs)):\n",
    "        X_test.append(inputs[i-60:i])\n",
    "        y_test.append(inputs[i])\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(test_data)\n",
    "\n",
    "    # We need numpy arrays for our model\n",
    "    X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "    \n",
    "    # We fit our data to our zipcode specific data\n",
    "    rnn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, scaler.transform(y_test)))\n",
    "\n",
    "    # Make predictions on the data\n",
    "\n",
    "    y_hat_raw = rnn_model.predict(X_test)\n",
    "    y_hat = scaler.inverse_transform(y_hat_raw)\n",
    "\n",
    "    # Use the score on unseen test data to calculate the MAPE\n",
    "\n",
    "    dict_mape[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_test)/y_test))      \n",
    "\n",
    "    # We get the last 60 values from our test data which is basically last 60 values in the data set\n",
    "    last_60 = df_time_series.iloc[-60:,zipcode:zipcode+1].values.astype(int)\n",
    "    \n",
    "    # Before we use our data we scale it\n",
    "    last_60 = scaler.transform(last_60)\n",
    "    \n",
    "    # Our input should be in (x,60,1) format\n",
    "    x_new_pred = last_60[-60:].reshape(1,60,1)\n",
    "\n",
    "    # make a prediction, add to the last_60 for the next prediction and \n",
    "    y_pred = rnn_model.predict(x_new_pred)\n",
    "\n",
    "    # We add our predition to our list of predictions for zipcode specific predictions list\n",
    "    dict_pred[zipcodes[zipcode]]=scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    print(f'Iteration number {zipcode} finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({95804: 0.5315432539077938,\n",
       "  95817: 0.5124504760512157,\n",
       "  95813: 0.006508317999600115,\n",
       "  95785: 0.011907401208689664,\n",
       "  95819: 0.015536161737748074,\n",
       "  95770: 0.42053615682015694,\n",
       "  95806: 0.464530955529028,\n",
       "  95790: 0.019576824831043088,\n",
       "  95799: 0.4051437563413147,\n",
       "  95844: 0.011921927187956908,\n",
       "  95843: 0.3578688945218865,\n",
       "  95815: 0.45184636587182475,\n",
       "  95825: 0.38152389087246624,\n",
       "  95818: 0.47189824704030975,\n",
       "  95811: 0.5359893680570037,\n",
       "  95931: 0.46396687898825445,\n",
       "  95753: 0.37366140791782015,\n",
       "  95827: 0.00895477237798403,\n",
       "  95937: 0.009821427202861922,\n",
       "  95914: 0.4272126786573001,\n",
       "  95754: 0.43064616749097345,\n",
       "  95824: 0.440302258067788,\n",
       "  95945: 0.010256275363664994,\n",
       "  95800: 0.5305922613642685,\n",
       "  95751: 0.007425901315001419,\n",
       "  95769: 0.26342204915554407,\n",
       "  95909: 0.5198084595761232,\n",
       "  95771: 0.42753879892302804,\n",
       "  95935: 0.4897754347779582,\n",
       "  95798: 0.48917108238629414,\n",
       "  95835: 0.418018812329609,\n",
       "  95845: 0.33577368618525744,\n",
       "  95865: 0.0110690572587036,\n",
       "  95809: 0.011654768341110282,\n",
       "  95944: 0.008558108160527085,\n",
       "  399671: 0.41329741837904654,\n",
       "  95831: 0.006651789800896116,\n",
       "  95803: 0.5802952756637934,\n",
       "  95939: 0.009029875327944208,\n",
       "  399665: 0.019039065051074132,\n",
       "  95826: 0.3949046864749507,\n",
       "  95830: 0.0073360364440262955,\n",
       "  95932: 0.45838941378528913,\n",
       "  95792: 0.4345692122840352,\n",
       "  95837: 0.006044799581819219,\n",
       "  95750: 0.010446251405969591,\n",
       "  95838: 0.4894882677253689,\n",
       "  95912: 0.5057961304955354,\n",
       "  95940: 0.2805008634494203,\n",
       "  95841: 0.4147297907958212,\n",
       "  95793: 0.015069971634264911,\n",
       "  95952: 0.009627691418112018,\n",
       "  95963: 0.09653037612012168,\n",
       "  95816: 0.44930872253417053,\n",
       "  95779: 0.01201222938531869,\n",
       "  95852: 0.4597349541972556,\n",
       "  95783: 0.3269234793667096,\n",
       "  95814: 0.3946809297740382,\n",
       "  95957: 0.012648814447446477,\n",
       "  95888: 0.1944524503417647,\n",
       "  95861: 0.009314070802342379,\n",
       "  95840: 0.012093233526937303,\n",
       "  95842: 0.4111398599841763,\n",
       "  95766: 0.009595125929397595,\n",
       "  95883: 0.14768182415836287,\n",
       "  95911: 0.5157942464604007,\n",
       "  95744: 0.013211295332702235,\n",
       "  95834: 0.010689107687849992,\n",
       "  95928: 0.023264470133830754,\n",
       "  95901: 0.015223403588790091,\n",
       "  95890: 0.017252290547685005,\n",
       "  95966: 0.080908905573775,\n",
       "  95768: 0.17421942895165352,\n",
       "  95805: 0.34578284410224114,\n",
       "  399673: 0.4418318198042554,\n",
       "  95954: 0.013361069592473332,\n",
       "  399672: 0.012682461079055251,\n",
       "  95787: 0.33420324142661967,\n",
       "  95839: 0.016610426019921406,\n",
       "  399674: 0.01448856868280435,\n",
       "  95922: 0.10902085387207897,\n",
       "  95866: 0.008831624167013196,\n",
       "  95907: 0.17842917879099413,\n",
       "  95788: 0.40405433579751987,\n",
       "  95926: 0.022248930422989654,\n",
       "  95930: 0.040854529060869595,\n",
       "  95956: 0.02375934649383698,\n",
       "  95938: 0.03127377551835743,\n",
       "  95795: 0.36771163337627305,\n",
       "  95923: 0.33600002056274914,\n",
       "  95955: 0.019477369977372778,\n",
       "  95924: 0.2447872081794937,\n",
       "  95775: 0.012375250106412763,\n",
       "  95919: 0.0166802913672708,\n",
       "  95794: 0.006933209341087386,\n",
       "  399666: 0.012521922598262882,\n",
       "  95760: 0.014186489218696195,\n",
       "  95916: 0.014117730579004917,\n",
       "  95891: 0.02114140271512981,\n",
       "  95820: 0.027295736637503107,\n",
       "  95917: 0.41677012505855277,\n",
       "  95893: 0.026252750053946686,\n",
       "  95851: 0.007814146002114407},\n",
       " {95804: array([[71147.016]], dtype=float32),\n",
       "  95817: array([[74570.94]], dtype=float32),\n",
       "  95813: array([[343974.38]], dtype=float32),\n",
       "  95785: array([[421430.6]], dtype=float32),\n",
       "  95819: array([[300689.84]], dtype=float32),\n",
       "  95770: array([[110922.31]], dtype=float32),\n",
       "  95806: array([[83814.88]], dtype=float32),\n",
       "  95790: array([[307788.3]], dtype=float32),\n",
       "  95799: array([[114296.914]], dtype=float32),\n",
       "  95844: array([[300981.]], dtype=float32),\n",
       "  95843: array([[145523.36]], dtype=float32),\n",
       "  95815: array([[99764.06]], dtype=float32),\n",
       "  95825: array([[142873.53]], dtype=float32),\n",
       "  95818: array([[86009.234]], dtype=float32),\n",
       "  95811: array([[60160.09]], dtype=float32),\n",
       "  95931: array([[109074.81]], dtype=float32),\n",
       "  95753: array([[150251.08]], dtype=float32),\n",
       "  95827: array([[335509.97]], dtype=float32),\n",
       "  95937: array([[445513.62]], dtype=float32),\n",
       "  95914: array([[171823.5]], dtype=float32),\n",
       "  95754: array([[109016.86]], dtype=float32),\n",
       "  95824: array([[121932.87]], dtype=float32),\n",
       "  95945: array([[385576.97]], dtype=float32),\n",
       "  95800: array([[69398.484]], dtype=float32),\n",
       "  95751: array([[347375.03]], dtype=float32),\n",
       "  95769: array([[75937.83]], dtype=float32),\n",
       "  95909: array([[87111.25]], dtype=float32),\n",
       "  95771: array([[102949.195]], dtype=float32),\n",
       "  95935: array([[108195.69]], dtype=float32),\n",
       "  95798: array([[84419.15]], dtype=float32),\n",
       "  95835: array([[131565.7]], dtype=float32),\n",
       "  95845: array([[174963.27]], dtype=float32),\n",
       "  95865: array([[297710.22]], dtype=float32),\n",
       "  95809: array([[303476.56]], dtype=float32),\n",
       "  95944: array([[421025.47]], dtype=float32),\n",
       "  399671: array([[123586.94]], dtype=float32),\n",
       "  95831: array([[426460.8]], dtype=float32),\n",
       "  95803: array([[55615.95]], dtype=float32),\n",
       "  95939: array([[653762.94]], dtype=float32),\n",
       "  399665: array([[304289.7]], dtype=float32),\n",
       "  95826: array([[130668.18]], dtype=float32),\n",
       "  95830: array([[320893.88]], dtype=float32),\n",
       "  95932: array([[128294.625]], dtype=float32),\n",
       "  95792: array([[107936.016]], dtype=float32),\n",
       "  95837: array([[319459.62]], dtype=float32),\n",
       "  95750: array([[268785.22]], dtype=float32),\n",
       "  95838: array([[80123.92]], dtype=float32),\n",
       "  95912: array([[110661.71]], dtype=float32),\n",
       "  95940: array([[115599.984]], dtype=float32),\n",
       "  95841: array([[112396.69]], dtype=float32),\n",
       "  95793: array([[294322.38]], dtype=float32),\n",
       "  95952: array([[265371.1]], dtype=float32),\n",
       "  95963: array([[211279.48]], dtype=float32),\n",
       "  95816: array([[109329.63]], dtype=float32),\n",
       "  95779: array([[353250.16]], dtype=float32),\n",
       "  95852: array([[86105.78]], dtype=float32),\n",
       "  95783: array([[107253.29]], dtype=float32),\n",
       "  95814: array([[130083.]], dtype=float32),\n",
       "  95957: array([[264841.97]], dtype=float32),\n",
       "  95888: array([[141728.98]], dtype=float32),\n",
       "  95861: array([[287390.16]], dtype=float32),\n",
       "  95840: array([[351977.47]], dtype=float32),\n",
       "  95842: array([[150103.39]], dtype=float32),\n",
       "  95766: array([[240971.58]], dtype=float32),\n",
       "  95883: array([[176819.56]], dtype=float32),\n",
       "  95911: array([[95302.664]], dtype=float32),\n",
       "  95744: array([[313836.94]], dtype=float32),\n",
       "  95834: array([[436590.38]], dtype=float32),\n",
       "  95928: array([[318956.97]], dtype=float32),\n",
       "  95901: array([[399089.94]], dtype=float32),\n",
       "  95890: array([[370209.84]], dtype=float32),\n",
       "  95966: array([[215876.78]], dtype=float32),\n",
       "  95768: array([[132021.52]], dtype=float32),\n",
       "  95805: array([[152292.73]], dtype=float32),\n",
       "  399673: array([[130023.29]], dtype=float32),\n",
       "  95954: array([[398123.3]], dtype=float32),\n",
       "  399672: array([[418916.88]], dtype=float32),\n",
       "  95787: array([[79144.22]], dtype=float32),\n",
       "  95839: array([[279416.53]], dtype=float32),\n",
       "  399674: array([[535701.8]], dtype=float32),\n",
       "  95922: array([[117771.57]], dtype=float32),\n",
       "  95866: array([[305283.75]], dtype=float32),\n",
       "  95907: array([[107502.836]], dtype=float32),\n",
       "  95788: array([[102076.516]], dtype=float32),\n",
       "  95926: array([[919457.94]], dtype=float32),\n",
       "  95930: array([[342691.06]], dtype=float32),\n",
       "  95956: array([[316519.03]], dtype=float32),\n",
       "  95938: array([[431528.7]], dtype=float32),\n",
       "  95795: array([[143493.78]], dtype=float32),\n",
       "  95923: array([[400056.88]], dtype=float32),\n",
       "  95955: array([[429283.8]], dtype=float32),\n",
       "  95924: array([[252192.67]], dtype=float32),\n",
       "  95775: array([[209203.08]], dtype=float32),\n",
       "  95919: array([[281477.12]], dtype=float32),\n",
       "  95794: array([[327327.62]], dtype=float32),\n",
       "  399666: array([[328454.3]], dtype=float32),\n",
       "  95760: array([[304591.88]], dtype=float32),\n",
       "  95916: array([[449193.4]], dtype=float32),\n",
       "  95891: array([[663938.7]], dtype=float32),\n",
       "  95820: array([[334986.]], dtype=float32),\n",
       "  95917: array([[100870.75]], dtype=float32),\n",
       "  95893: array([[2045979.5]], dtype=float32),\n",
       "  95851: array([[355222.]], dtype=float32)})"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_mape, dict_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_keys = list(dict_mape.keys())\n",
    "rnn_mape = list(dict_mape.values())\n",
    "rnn_pred = []\n",
    "rnn_dict = {}\n",
    "for zipcode in dict_pred.keys():\n",
    "    rnn_pred.append(dict_pred[zipcode].astype(int)[0][0])\n",
    "for zc in rnn_keys:\n",
    "    a = []\n",
    "    a.append(dict_mape[zc])\n",
    "    a.append(dict_pred[zc].astype(float)[0][0])\n",
    "    rnn_dict[zc] = a\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{95804: [0.5315432539077938, 71147.015625],\n",
       " 95817: [0.5124504760512157, 74570.9375],\n",
       " 95813: [0.006508317999600115, 343974.375],\n",
       " 95785: [0.011907401208689664, 421430.59375],\n",
       " 95819: [0.015536161737748074, 300689.84375],\n",
       " 95770: [0.42053615682015694, 110922.3125],\n",
       " 95806: [0.464530955529028, 83814.8828125],\n",
       " 95790: [0.019576824831043088, 307788.3125],\n",
       " 95799: [0.4051437563413147, 114296.9140625],\n",
       " 95844: [0.011921927187956908, 300981.0],\n",
       " 95843: [0.3578688945218865, 145523.359375],\n",
       " 95815: [0.45184636587182475, 99764.0625],\n",
       " 95825: [0.38152389087246624, 142873.53125],\n",
       " 95818: [0.47189824704030975, 86009.234375],\n",
       " 95811: [0.5359893680570037, 60160.08984375],\n",
       " 95931: [0.46396687898825445, 109074.8125],\n",
       " 95753: [0.37366140791782015, 150251.078125],\n",
       " 95827: [0.00895477237798403, 335509.96875],\n",
       " 95937: [0.009821427202861922, 445513.625],\n",
       " 95914: [0.4272126786573001, 171823.5],\n",
       " 95754: [0.43064616749097345, 109016.859375],\n",
       " 95824: [0.440302258067788, 121932.8671875],\n",
       " 95945: [0.010256275363664994, 385576.96875],\n",
       " 95800: [0.5305922613642685, 69398.484375],\n",
       " 95751: [0.007425901315001419, 347375.03125],\n",
       " 95769: [0.26342204915554407, 75937.828125],\n",
       " 95909: [0.5198084595761232, 87111.25],\n",
       " 95771: [0.42753879892302804, 102949.1953125],\n",
       " 95935: [0.4897754347779582, 108195.6875],\n",
       " 95798: [0.48917108238629414, 84419.1484375],\n",
       " 95835: [0.418018812329609, 131565.703125],\n",
       " 95845: [0.33577368618525744, 174963.265625],\n",
       " 95865: [0.0110690572587036, 297710.21875],\n",
       " 95809: [0.011654768341110282, 303476.5625],\n",
       " 95944: [0.008558108160527085, 421025.46875],\n",
       " 399671: [0.41329741837904654, 123586.9375],\n",
       " 95831: [0.006651789800896116, 426460.8125],\n",
       " 95803: [0.5802952756637934, 55615.94921875],\n",
       " 95939: [0.009029875327944208, 653762.9375],\n",
       " 399665: [0.019039065051074132, 304289.6875],\n",
       " 95826: [0.3949046864749507, 130668.1796875],\n",
       " 95830: [0.0073360364440262955, 320893.875],\n",
       " 95932: [0.45838941378528913, 128294.625],\n",
       " 95792: [0.4345692122840352, 107936.015625],\n",
       " 95837: [0.006044799581819219, 319459.625],\n",
       " 95750: [0.010446251405969591, 268785.21875],\n",
       " 95838: [0.4894882677253689, 80123.921875],\n",
       " 95912: [0.5057961304955354, 110661.7109375],\n",
       " 95940: [0.2805008634494203, 115599.984375],\n",
       " 95841: [0.4147297907958212, 112396.6875],\n",
       " 95793: [0.015069971634264911, 294322.375],\n",
       " 95952: [0.009627691418112018, 265371.09375],\n",
       " 95963: [0.09653037612012168, 211279.484375],\n",
       " 95816: [0.44930872253417053, 109329.6328125],\n",
       " 95779: [0.01201222938531869, 353250.15625],\n",
       " 95852: [0.4597349541972556, 86105.78125],\n",
       " 95783: [0.3269234793667096, 107253.2890625],\n",
       " 95814: [0.3946809297740382, 130083.0],\n",
       " 95957: [0.012648814447446477, 264841.96875],\n",
       " 95888: [0.1944524503417647, 141728.984375],\n",
       " 95861: [0.009314070802342379, 287390.15625],\n",
       " 95840: [0.012093233526937303, 351977.46875],\n",
       " 95842: [0.4111398599841763, 150103.390625],\n",
       " 95766: [0.009595125929397595, 240971.578125],\n",
       " 95883: [0.14768182415836287, 176819.5625],\n",
       " 95911: [0.5157942464604007, 95302.6640625],\n",
       " 95744: [0.013211295332702235, 313836.9375],\n",
       " 95834: [0.010689107687849992, 436590.375],\n",
       " 95928: [0.023264470133830754, 318956.96875],\n",
       " 95901: [0.015223403588790091, 399089.9375],\n",
       " 95890: [0.017252290547685005, 370209.84375],\n",
       " 95966: [0.080908905573775, 215876.78125],\n",
       " 95768: [0.17421942895165352, 132021.515625],\n",
       " 95805: [0.34578284410224114, 152292.734375],\n",
       " 399673: [0.4418318198042554, 130023.2890625],\n",
       " 95954: [0.013361069592473332, 398123.3125],\n",
       " 399672: [0.012682461079055251, 418916.875],\n",
       " 95787: [0.33420324142661967, 79144.21875],\n",
       " 95839: [0.016610426019921406, 279416.53125],\n",
       " 399674: [0.01448856868280435, 535701.8125],\n",
       " 95922: [0.10902085387207897, 117771.5703125],\n",
       " 95866: [0.008831624167013196, 305283.75],\n",
       " 95907: [0.17842917879099413, 107502.8359375],\n",
       " 95788: [0.40405433579751987, 102076.515625],\n",
       " 95926: [0.022248930422989654, 919457.9375],\n",
       " 95930: [0.040854529060869595, 342691.0625],\n",
       " 95956: [0.02375934649383698, 316519.03125],\n",
       " 95938: [0.03127377551835743, 431528.6875],\n",
       " 95795: [0.36771163337627305, 143493.78125],\n",
       " 95923: [0.33600002056274914, 400056.875],\n",
       " 95955: [0.019477369977372778, 429283.8125],\n",
       " 95924: [0.2447872081794937, 252192.671875],\n",
       " 95775: [0.012375250106412763, 209203.078125],\n",
       " 95919: [0.0166802913672708, 281477.125],\n",
       " 95794: [0.006933209341087386, 327327.625],\n",
       " 399666: [0.012521922598262882, 328454.3125],\n",
       " 95760: [0.014186489218696195, 304591.875],\n",
       " 95916: [0.014117730579004917, 449193.40625],\n",
       " 95891: [0.02114140271512981, 663938.6875],\n",
       " 95820: [0.027295736637503107, 334986.0],\n",
       " 95917: [0.41677012505855277, 100870.75],\n",
       " 95893: [0.026252750053946686, 2045979.5],\n",
       " 95851: [0.007814146002114407, 355222.0]}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dict = {}\n",
    "for zc in rnn_keys:\n",
    "    a = []\n",
    "    a.append(0.1)\n",
    "    a.append(1000)\n",
    "    a.append('Dummy')\n",
    "    dummy_dict[zc] = a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{95804: [0.1, 1000],\n",
       " 95817: [0.1, 1000],\n",
       " 95813: [0.1, 1000],\n",
       " 95785: [0.1, 1000],\n",
       " 95819: [0.1, 1000],\n",
       " 95770: [0.1, 1000],\n",
       " 95806: [0.1, 1000],\n",
       " 95790: [0.1, 1000],\n",
       " 95799: [0.1, 1000],\n",
       " 95844: [0.1, 1000],\n",
       " 95843: [0.1, 1000],\n",
       " 95815: [0.1, 1000],\n",
       " 95825: [0.1, 1000],\n",
       " 95818: [0.1, 1000],\n",
       " 95811: [0.1, 1000],\n",
       " 95931: [0.1, 1000],\n",
       " 95753: [0.1, 1000],\n",
       " 95827: [0.1, 1000],\n",
       " 95937: [0.1, 1000],\n",
       " 95914: [0.1, 1000],\n",
       " 95754: [0.1, 1000],\n",
       " 95824: [0.1, 1000],\n",
       " 95945: [0.1, 1000],\n",
       " 95800: [0.1, 1000],\n",
       " 95751: [0.1, 1000],\n",
       " 95769: [0.1, 1000],\n",
       " 95909: [0.1, 1000],\n",
       " 95771: [0.1, 1000],\n",
       " 95935: [0.1, 1000],\n",
       " 95798: [0.1, 1000],\n",
       " 95835: [0.1, 1000],\n",
       " 95845: [0.1, 1000],\n",
       " 95865: [0.1, 1000],\n",
       " 95809: [0.1, 1000],\n",
       " 95944: [0.1, 1000],\n",
       " 399671: [0.1, 1000],\n",
       " 95831: [0.1, 1000],\n",
       " 95803: [0.1, 1000],\n",
       " 95939: [0.1, 1000],\n",
       " 399665: [0.1, 1000],\n",
       " 95826: [0.1, 1000],\n",
       " 95830: [0.1, 1000],\n",
       " 95932: [0.1, 1000],\n",
       " 95792: [0.1, 1000],\n",
       " 95837: [0.1, 1000],\n",
       " 95750: [0.1, 1000],\n",
       " 95838: [0.1, 1000],\n",
       " 95912: [0.1, 1000],\n",
       " 95940: [0.1, 1000],\n",
       " 95841: [0.1, 1000],\n",
       " 95793: [0.1, 1000],\n",
       " 95952: [0.1, 1000],\n",
       " 95963: [0.1, 1000],\n",
       " 95816: [0.1, 1000],\n",
       " 95779: [0.1, 1000],\n",
       " 95852: [0.1, 1000],\n",
       " 95783: [0.1, 1000],\n",
       " 95814: [0.1, 1000],\n",
       " 95957: [0.1, 1000],\n",
       " 95888: [0.1, 1000],\n",
       " 95861: [0.1, 1000],\n",
       " 95840: [0.1, 1000],\n",
       " 95842: [0.1, 1000],\n",
       " 95766: [0.1, 1000],\n",
       " 95883: [0.1, 1000],\n",
       " 95911: [0.1, 1000],\n",
       " 95744: [0.1, 1000],\n",
       " 95834: [0.1, 1000],\n",
       " 95928: [0.1, 1000],\n",
       " 95901: [0.1, 1000],\n",
       " 95890: [0.1, 1000],\n",
       " 95966: [0.1, 1000],\n",
       " 95768: [0.1, 1000],\n",
       " 95805: [0.1, 1000],\n",
       " 399673: [0.1, 1000],\n",
       " 95954: [0.1, 1000],\n",
       " 399672: [0.1, 1000],\n",
       " 95787: [0.1, 1000],\n",
       " 95839: [0.1, 1000],\n",
       " 399674: [0.1, 1000],\n",
       " 95922: [0.1, 1000],\n",
       " 95866: [0.1, 1000],\n",
       " 95907: [0.1, 1000],\n",
       " 95788: [0.1, 1000],\n",
       " 95926: [0.1, 1000],\n",
       " 95930: [0.1, 1000],\n",
       " 95956: [0.1, 1000],\n",
       " 95938: [0.1, 1000],\n",
       " 95795: [0.1, 1000],\n",
       " 95923: [0.1, 1000],\n",
       " 95955: [0.1, 1000],\n",
       " 95924: [0.1, 1000],\n",
       " 95775: [0.1, 1000],\n",
       " 95919: [0.1, 1000],\n",
       " 95794: [0.1, 1000],\n",
       " 399666: [0.1, 1000],\n",
       " 95760: [0.1, 1000],\n",
       " 95916: [0.1, 1000],\n",
       " 95891: [0.1, 1000],\n",
       " 95820: [0.1, 1000],\n",
       " 95917: [0.1, 1000],\n",
       " 95893: [0.1, 1000],\n",
       " 95851: [0.1, 1000]}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rnn_dict, dummy_dict]\n",
    "best_model_dict = {}\n",
    "for zipcode in dict_pred.keys():\n",
    "    best_model = [1,1]\n",
    "    for model in models:\n",
    "        if model[zipcode][0]<best_model[0]:\n",
    "            best_model = model[zipcode]\n",
    "    best_model_dict[zipcode] = best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{95804: [0.1, 1000],\n",
       " 95817: [0.1, 1000],\n",
       " 95813: [0.006508317999600115, 343974.375],\n",
       " 95785: [0.011907401208689664, 421430.59375],\n",
       " 95819: [0.015536161737748074, 300689.84375],\n",
       " 95770: [0.1, 1000],\n",
       " 95806: [0.1, 1000],\n",
       " 95790: [0.019576824831043088, 307788.3125],\n",
       " 95799: [0.1, 1000],\n",
       " 95844: [0.011921927187956908, 300981.0],\n",
       " 95843: [0.1, 1000],\n",
       " 95815: [0.1, 1000],\n",
       " 95825: [0.1, 1000],\n",
       " 95818: [0.1, 1000],\n",
       " 95811: [0.1, 1000],\n",
       " 95931: [0.1, 1000],\n",
       " 95753: [0.1, 1000],\n",
       " 95827: [0.00895477237798403, 335509.96875],\n",
       " 95937: [0.009821427202861922, 445513.625],\n",
       " 95914: [0.1, 1000],\n",
       " 95754: [0.1, 1000],\n",
       " 95824: [0.1, 1000],\n",
       " 95945: [0.010256275363664994, 385576.96875],\n",
       " 95800: [0.1, 1000],\n",
       " 95751: [0.007425901315001419, 347375.03125],\n",
       " 95769: [0.1, 1000],\n",
       " 95909: [0.1, 1000],\n",
       " 95771: [0.1, 1000],\n",
       " 95935: [0.1, 1000],\n",
       " 95798: [0.1, 1000],\n",
       " 95835: [0.1, 1000],\n",
       " 95845: [0.1, 1000],\n",
       " 95865: [0.0110690572587036, 297710.21875],\n",
       " 95809: [0.011654768341110282, 303476.5625],\n",
       " 95944: [0.008558108160527085, 421025.46875],\n",
       " 399671: [0.1, 1000],\n",
       " 95831: [0.006651789800896116, 426460.8125],\n",
       " 95803: [0.1, 1000],\n",
       " 95939: [0.009029875327944208, 653762.9375],\n",
       " 399665: [0.019039065051074132, 304289.6875],\n",
       " 95826: [0.1, 1000],\n",
       " 95830: [0.0073360364440262955, 320893.875],\n",
       " 95932: [0.1, 1000],\n",
       " 95792: [0.1, 1000],\n",
       " 95837: [0.006044799581819219, 319459.625],\n",
       " 95750: [0.010446251405969591, 268785.21875],\n",
       " 95838: [0.1, 1000],\n",
       " 95912: [0.1, 1000],\n",
       " 95940: [0.1, 1000],\n",
       " 95841: [0.1, 1000],\n",
       " 95793: [0.015069971634264911, 294322.375],\n",
       " 95952: [0.009627691418112018, 265371.09375],\n",
       " 95963: [0.09653037612012168, 211279.484375],\n",
       " 95816: [0.1, 1000],\n",
       " 95779: [0.01201222938531869, 353250.15625],\n",
       " 95852: [0.1, 1000],\n",
       " 95783: [0.1, 1000],\n",
       " 95814: [0.1, 1000],\n",
       " 95957: [0.012648814447446477, 264841.96875],\n",
       " 95888: [0.1, 1000],\n",
       " 95861: [0.009314070802342379, 287390.15625],\n",
       " 95840: [0.012093233526937303, 351977.46875],\n",
       " 95842: [0.1, 1000],\n",
       " 95766: [0.009595125929397595, 240971.578125],\n",
       " 95883: [0.1, 1000],\n",
       " 95911: [0.1, 1000],\n",
       " 95744: [0.013211295332702235, 313836.9375],\n",
       " 95834: [0.010689107687849992, 436590.375],\n",
       " 95928: [0.023264470133830754, 318956.96875],\n",
       " 95901: [0.015223403588790091, 399089.9375],\n",
       " 95890: [0.017252290547685005, 370209.84375],\n",
       " 95966: [0.080908905573775, 215876.78125],\n",
       " 95768: [0.1, 1000],\n",
       " 95805: [0.1, 1000],\n",
       " 399673: [0.1, 1000],\n",
       " 95954: [0.013361069592473332, 398123.3125],\n",
       " 399672: [0.012682461079055251, 418916.875],\n",
       " 95787: [0.1, 1000],\n",
       " 95839: [0.016610426019921406, 279416.53125],\n",
       " 399674: [0.01448856868280435, 535701.8125],\n",
       " 95922: [0.1, 1000],\n",
       " 95866: [0.008831624167013196, 305283.75],\n",
       " 95907: [0.1, 1000],\n",
       " 95788: [0.1, 1000],\n",
       " 95926: [0.022248930422989654, 919457.9375],\n",
       " 95930: [0.040854529060869595, 342691.0625],\n",
       " 95956: [0.02375934649383698, 316519.03125],\n",
       " 95938: [0.03127377551835743, 431528.6875],\n",
       " 95795: [0.1, 1000],\n",
       " 95923: [0.1, 1000],\n",
       " 95955: [0.019477369977372778, 429283.8125],\n",
       " 95924: [0.1, 1000],\n",
       " 95775: [0.012375250106412763, 209203.078125],\n",
       " 95919: [0.0166802913672708, 281477.125],\n",
       " 95794: [0.006933209341087386, 327327.625],\n",
       " 399666: [0.012521922598262882, 328454.3125],\n",
       " 95760: [0.014186489218696195, 304591.875],\n",
       " 95916: [0.014117730579004917, 449193.40625],\n",
       " 95891: [0.02114140271512981, 663938.6875],\n",
       " 95820: [0.027295736637503107, 334986.0],\n",
       " 95917: [0.1, 1000],\n",
       " 95893: [0.026252750053946686, 2045979.5],\n",
       " 95851: [0.007814146002114407, 355222.0]}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20640588672801072"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rnn_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e81d9768a2937af9514d7fa33aa30feb69a40df5b58c34cfa60b871c6c10885"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
