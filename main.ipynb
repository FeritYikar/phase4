{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "from fbprophet.diagnostics import performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionID</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Metro</th>\n",
       "      <th>CountyName</th>\n",
       "      <th>SizeRank</th>\n",
       "      <th>1996-04</th>\n",
       "      <th>1996-05</th>\n",
       "      <th>1996-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-07</th>\n",
       "      <th>2017-08</th>\n",
       "      <th>2017-09</th>\n",
       "      <th>2017-10</th>\n",
       "      <th>2017-11</th>\n",
       "      <th>2017-12</th>\n",
       "      <th>2018-01</th>\n",
       "      <th>2018-02</th>\n",
       "      <th>2018-03</th>\n",
       "      <th>2018-04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84654</td>\n",
       "      <td>60657</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>1</td>\n",
       "      <td>334200.0</td>\n",
       "      <td>335400.0</td>\n",
       "      <td>336500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1005500</td>\n",
       "      <td>1007500</td>\n",
       "      <td>1007800</td>\n",
       "      <td>1009600</td>\n",
       "      <td>1013300</td>\n",
       "      <td>1018700</td>\n",
       "      <td>1024400</td>\n",
       "      <td>1030700</td>\n",
       "      <td>1033800</td>\n",
       "      <td>1030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90668</td>\n",
       "      <td>75070</td>\n",
       "      <td>McKinney</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas-Fort Worth</td>\n",
       "      <td>Collin</td>\n",
       "      <td>2</td>\n",
       "      <td>235700.0</td>\n",
       "      <td>236900.0</td>\n",
       "      <td>236700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>308000</td>\n",
       "      <td>310000</td>\n",
       "      <td>312500</td>\n",
       "      <td>314100</td>\n",
       "      <td>315000</td>\n",
       "      <td>316600</td>\n",
       "      <td>318100</td>\n",
       "      <td>319600</td>\n",
       "      <td>321100</td>\n",
       "      <td>321800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91982</td>\n",
       "      <td>77494</td>\n",
       "      <td>Katy</td>\n",
       "      <td>TX</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Harris</td>\n",
       "      <td>3</td>\n",
       "      <td>210400.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>212200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>321000</td>\n",
       "      <td>320600</td>\n",
       "      <td>320200</td>\n",
       "      <td>320400</td>\n",
       "      <td>320800</td>\n",
       "      <td>321200</td>\n",
       "      <td>321200</td>\n",
       "      <td>323000</td>\n",
       "      <td>326900</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84616</td>\n",
       "      <td>60614</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>4</td>\n",
       "      <td>498100.0</td>\n",
       "      <td>500900.0</td>\n",
       "      <td>503100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1289800</td>\n",
       "      <td>1287700</td>\n",
       "      <td>1287400</td>\n",
       "      <td>1291500</td>\n",
       "      <td>1296600</td>\n",
       "      <td>1299000</td>\n",
       "      <td>1302700</td>\n",
       "      <td>1306400</td>\n",
       "      <td>1308500</td>\n",
       "      <td>1307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93144</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>TX</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>5</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>77300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>119100</td>\n",
       "      <td>119400</td>\n",
       "      <td>120000</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120300</td>\n",
       "      <td>120500</td>\n",
       "      <td>121000</td>\n",
       "      <td>121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14718</th>\n",
       "      <td>58333</td>\n",
       "      <td>1338</td>\n",
       "      <td>Ashfield</td>\n",
       "      <td>MA</td>\n",
       "      <td>Greenfield Town</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>14719</td>\n",
       "      <td>94600.0</td>\n",
       "      <td>94300.0</td>\n",
       "      <td>94000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>216800</td>\n",
       "      <td>217700</td>\n",
       "      <td>218600</td>\n",
       "      <td>218500</td>\n",
       "      <td>218100</td>\n",
       "      <td>216400</td>\n",
       "      <td>213100</td>\n",
       "      <td>209800</td>\n",
       "      <td>209200</td>\n",
       "      <td>209300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14719</th>\n",
       "      <td>59107</td>\n",
       "      <td>3293</td>\n",
       "      <td>Woodstock</td>\n",
       "      <td>NH</td>\n",
       "      <td>Claremont</td>\n",
       "      <td>Grafton</td>\n",
       "      <td>14720</td>\n",
       "      <td>92700.0</td>\n",
       "      <td>92500.0</td>\n",
       "      <td>92400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>202100</td>\n",
       "      <td>208400</td>\n",
       "      <td>212200</td>\n",
       "      <td>215200</td>\n",
       "      <td>214300</td>\n",
       "      <td>213100</td>\n",
       "      <td>213700</td>\n",
       "      <td>218300</td>\n",
       "      <td>222700</td>\n",
       "      <td>225800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14720</th>\n",
       "      <td>75672</td>\n",
       "      <td>40404</td>\n",
       "      <td>Berea</td>\n",
       "      <td>KY</td>\n",
       "      <td>Richmond</td>\n",
       "      <td>Madison</td>\n",
       "      <td>14721</td>\n",
       "      <td>57100.0</td>\n",
       "      <td>57300.0</td>\n",
       "      <td>57500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121800</td>\n",
       "      <td>122800</td>\n",
       "      <td>124600</td>\n",
       "      <td>126700</td>\n",
       "      <td>128800</td>\n",
       "      <td>130600</td>\n",
       "      <td>131700</td>\n",
       "      <td>132500</td>\n",
       "      <td>133000</td>\n",
       "      <td>133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14721</th>\n",
       "      <td>93733</td>\n",
       "      <td>81225</td>\n",
       "      <td>Mount Crested Butte</td>\n",
       "      <td>CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gunnison</td>\n",
       "      <td>14722</td>\n",
       "      <td>191100.0</td>\n",
       "      <td>192400.0</td>\n",
       "      <td>193700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>662800</td>\n",
       "      <td>671200</td>\n",
       "      <td>682400</td>\n",
       "      <td>695600</td>\n",
       "      <td>695500</td>\n",
       "      <td>694700</td>\n",
       "      <td>706400</td>\n",
       "      <td>705300</td>\n",
       "      <td>681500</td>\n",
       "      <td>664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14722</th>\n",
       "      <td>95851</td>\n",
       "      <td>89155</td>\n",
       "      <td>Mesquite</td>\n",
       "      <td>NV</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>Clark</td>\n",
       "      <td>14723</td>\n",
       "      <td>176400.0</td>\n",
       "      <td>176300.0</td>\n",
       "      <td>176100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>333800</td>\n",
       "      <td>336400</td>\n",
       "      <td>339700</td>\n",
       "      <td>343800</td>\n",
       "      <td>346800</td>\n",
       "      <td>348900</td>\n",
       "      <td>350400</td>\n",
       "      <td>353000</td>\n",
       "      <td>356000</td>\n",
       "      <td>357200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14723 rows × 272 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       RegionID  RegionName                 City State              Metro  \\\n",
       "0         84654       60657              Chicago    IL            Chicago   \n",
       "1         90668       75070             McKinney    TX  Dallas-Fort Worth   \n",
       "2         91982       77494                 Katy    TX            Houston   \n",
       "3         84616       60614              Chicago    IL            Chicago   \n",
       "4         93144       79936              El Paso    TX            El Paso   \n",
       "...         ...         ...                  ...   ...                ...   \n",
       "14718     58333        1338             Ashfield    MA    Greenfield Town   \n",
       "14719     59107        3293            Woodstock    NH          Claremont   \n",
       "14720     75672       40404                Berea    KY           Richmond   \n",
       "14721     93733       81225  Mount Crested Butte    CO                NaN   \n",
       "14722     95851       89155             Mesquite    NV          Las Vegas   \n",
       "\n",
       "      CountyName  SizeRank   1996-04   1996-05   1996-06  ...  2017-07  \\\n",
       "0           Cook         1  334200.0  335400.0  336500.0  ...  1005500   \n",
       "1         Collin         2  235700.0  236900.0  236700.0  ...   308000   \n",
       "2         Harris         3  210400.0  212200.0  212200.0  ...   321000   \n",
       "3           Cook         4  498100.0  500900.0  503100.0  ...  1289800   \n",
       "4        El Paso         5   77300.0   77300.0   77300.0  ...   119100   \n",
       "...          ...       ...       ...       ...       ...  ...      ...   \n",
       "14718   Franklin     14719   94600.0   94300.0   94000.0  ...   216800   \n",
       "14719    Grafton     14720   92700.0   92500.0   92400.0  ...   202100   \n",
       "14720    Madison     14721   57100.0   57300.0   57500.0  ...   121800   \n",
       "14721   Gunnison     14722  191100.0  192400.0  193700.0  ...   662800   \n",
       "14722      Clark     14723  176400.0  176300.0  176100.0  ...   333800   \n",
       "\n",
       "       2017-08  2017-09  2017-10  2017-11  2017-12  2018-01  2018-02  2018-03  \\\n",
       "0      1007500  1007800  1009600  1013300  1018700  1024400  1030700  1033800   \n",
       "1       310000   312500   314100   315000   316600   318100   319600   321100   \n",
       "2       320600   320200   320400   320800   321200   321200   323000   326900   \n",
       "3      1287700  1287400  1291500  1296600  1299000  1302700  1306400  1308500   \n",
       "4       119400   120000   120300   120300   120300   120300   120500   121000   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "14718   217700   218600   218500   218100   216400   213100   209800   209200   \n",
       "14719   208400   212200   215200   214300   213100   213700   218300   222700   \n",
       "14720   122800   124600   126700   128800   130600   131700   132500   133000   \n",
       "14721   671200   682400   695600   695500   694700   706400   705300   681500   \n",
       "14722   336400   339700   343800   346800   348900   350400   353000   356000   \n",
       "\n",
       "       2018-04  \n",
       "0      1030600  \n",
       "1       321800  \n",
       "2       329900  \n",
       "3      1307000  \n",
       "4       121500  \n",
       "...        ...  \n",
       "14718   209300  \n",
       "14719   225800  \n",
       "14720   133400  \n",
       "14721   664400  \n",
       "14722   357200  \n",
       "\n",
       "[14723 rows x 272 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/zillow_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>60657</th>\n",
       "      <th>75070</th>\n",
       "      <th>77494</th>\n",
       "      <th>60614</th>\n",
       "      <th>79936</th>\n",
       "      <th>77084</th>\n",
       "      <th>10467</th>\n",
       "      <th>60640</th>\n",
       "      <th>77449</th>\n",
       "      <th>94109</th>\n",
       "      <th>...</th>\n",
       "      <th>3765</th>\n",
       "      <th>84781</th>\n",
       "      <th>12429</th>\n",
       "      <th>97028</th>\n",
       "      <th>12720</th>\n",
       "      <th>1338</th>\n",
       "      <th>3293</th>\n",
       "      <th>40404</th>\n",
       "      <th>81225</th>\n",
       "      <th>89155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-04-01</th>\n",
       "      <td>334200</td>\n",
       "      <td>235700</td>\n",
       "      <td>210400</td>\n",
       "      <td>498100</td>\n",
       "      <td>77300</td>\n",
       "      <td>95000</td>\n",
       "      <td>152900</td>\n",
       "      <td>216500</td>\n",
       "      <td>95400</td>\n",
       "      <td>766000</td>\n",
       "      <td>...</td>\n",
       "      <td>80800</td>\n",
       "      <td>135900</td>\n",
       "      <td>78300</td>\n",
       "      <td>136200</td>\n",
       "      <td>62500</td>\n",
       "      <td>94600</td>\n",
       "      <td>92700</td>\n",
       "      <td>57100</td>\n",
       "      <td>191100</td>\n",
       "      <td>176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-01</th>\n",
       "      <td>335400</td>\n",
       "      <td>236900</td>\n",
       "      <td>212200</td>\n",
       "      <td>500900</td>\n",
       "      <td>77300</td>\n",
       "      <td>95200</td>\n",
       "      <td>152700</td>\n",
       "      <td>216700</td>\n",
       "      <td>95600</td>\n",
       "      <td>771100</td>\n",
       "      <td>...</td>\n",
       "      <td>80100</td>\n",
       "      <td>136300</td>\n",
       "      <td>78300</td>\n",
       "      <td>136600</td>\n",
       "      <td>62600</td>\n",
       "      <td>94300</td>\n",
       "      <td>92500</td>\n",
       "      <td>57300</td>\n",
       "      <td>192400</td>\n",
       "      <td>176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-06-01</th>\n",
       "      <td>336500</td>\n",
       "      <td>236700</td>\n",
       "      <td>212200</td>\n",
       "      <td>503100</td>\n",
       "      <td>77300</td>\n",
       "      <td>95400</td>\n",
       "      <td>152600</td>\n",
       "      <td>216900</td>\n",
       "      <td>95800</td>\n",
       "      <td>776500</td>\n",
       "      <td>...</td>\n",
       "      <td>79400</td>\n",
       "      <td>136600</td>\n",
       "      <td>78200</td>\n",
       "      <td>136800</td>\n",
       "      <td>62700</td>\n",
       "      <td>94000</td>\n",
       "      <td>92400</td>\n",
       "      <td>57500</td>\n",
       "      <td>193700</td>\n",
       "      <td>176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-07-01</th>\n",
       "      <td>337600</td>\n",
       "      <td>235400</td>\n",
       "      <td>210700</td>\n",
       "      <td>504600</td>\n",
       "      <td>77300</td>\n",
       "      <td>95700</td>\n",
       "      <td>152400</td>\n",
       "      <td>217000</td>\n",
       "      <td>96100</td>\n",
       "      <td>781900</td>\n",
       "      <td>...</td>\n",
       "      <td>78600</td>\n",
       "      <td>136900</td>\n",
       "      <td>78200</td>\n",
       "      <td>136800</td>\n",
       "      <td>62700</td>\n",
       "      <td>93700</td>\n",
       "      <td>92200</td>\n",
       "      <td>57700</td>\n",
       "      <td>195000</td>\n",
       "      <td>176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-08-01</th>\n",
       "      <td>338500</td>\n",
       "      <td>233300</td>\n",
       "      <td>208300</td>\n",
       "      <td>505500</td>\n",
       "      <td>77400</td>\n",
       "      <td>95900</td>\n",
       "      <td>152300</td>\n",
       "      <td>217100</td>\n",
       "      <td>96400</td>\n",
       "      <td>787300</td>\n",
       "      <td>...</td>\n",
       "      <td>77900</td>\n",
       "      <td>137100</td>\n",
       "      <td>78100</td>\n",
       "      <td>136700</td>\n",
       "      <td>62700</td>\n",
       "      <td>93400</td>\n",
       "      <td>92100</td>\n",
       "      <td>58000</td>\n",
       "      <td>196300</td>\n",
       "      <td>175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>1018700</td>\n",
       "      <td>316600</td>\n",
       "      <td>321200</td>\n",
       "      <td>1299000</td>\n",
       "      <td>120300</td>\n",
       "      <td>162800</td>\n",
       "      <td>414300</td>\n",
       "      <td>777900</td>\n",
       "      <td>172300</td>\n",
       "      <td>3778700</td>\n",
       "      <td>...</td>\n",
       "      <td>123400</td>\n",
       "      <td>257600</td>\n",
       "      <td>171300</td>\n",
       "      <td>341000</td>\n",
       "      <td>122800</td>\n",
       "      <td>216400</td>\n",
       "      <td>213100</td>\n",
       "      <td>130600</td>\n",
       "      <td>694700</td>\n",
       "      <td>348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>1024400</td>\n",
       "      <td>318100</td>\n",
       "      <td>321200</td>\n",
       "      <td>1302700</td>\n",
       "      <td>120300</td>\n",
       "      <td>162800</td>\n",
       "      <td>413900</td>\n",
       "      <td>778500</td>\n",
       "      <td>173300</td>\n",
       "      <td>3770800</td>\n",
       "      <td>...</td>\n",
       "      <td>124400</td>\n",
       "      <td>258000</td>\n",
       "      <td>172400</td>\n",
       "      <td>342300</td>\n",
       "      <td>123200</td>\n",
       "      <td>213100</td>\n",
       "      <td>213700</td>\n",
       "      <td>131700</td>\n",
       "      <td>706400</td>\n",
       "      <td>350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>1030700</td>\n",
       "      <td>319600</td>\n",
       "      <td>323000</td>\n",
       "      <td>1306400</td>\n",
       "      <td>120500</td>\n",
       "      <td>162900</td>\n",
       "      <td>411400</td>\n",
       "      <td>780500</td>\n",
       "      <td>174200</td>\n",
       "      <td>3763100</td>\n",
       "      <td>...</td>\n",
       "      <td>125500</td>\n",
       "      <td>260600</td>\n",
       "      <td>173600</td>\n",
       "      <td>345000</td>\n",
       "      <td>123200</td>\n",
       "      <td>209800</td>\n",
       "      <td>218300</td>\n",
       "      <td>132500</td>\n",
       "      <td>705300</td>\n",
       "      <td>353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>1033800</td>\n",
       "      <td>321100</td>\n",
       "      <td>326900</td>\n",
       "      <td>1308500</td>\n",
       "      <td>121000</td>\n",
       "      <td>163500</td>\n",
       "      <td>413200</td>\n",
       "      <td>782800</td>\n",
       "      <td>175400</td>\n",
       "      <td>3779800</td>\n",
       "      <td>...</td>\n",
       "      <td>126600</td>\n",
       "      <td>264700</td>\n",
       "      <td>175800</td>\n",
       "      <td>348000</td>\n",
       "      <td>120700</td>\n",
       "      <td>209200</td>\n",
       "      <td>222700</td>\n",
       "      <td>133000</td>\n",
       "      <td>681500</td>\n",
       "      <td>356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>1030600</td>\n",
       "      <td>321800</td>\n",
       "      <td>329900</td>\n",
       "      <td>1307000</td>\n",
       "      <td>121500</td>\n",
       "      <td>164300</td>\n",
       "      <td>417900</td>\n",
       "      <td>782800</td>\n",
       "      <td>176200</td>\n",
       "      <td>3813500</td>\n",
       "      <td>...</td>\n",
       "      <td>127500</td>\n",
       "      <td>266800</td>\n",
       "      <td>177500</td>\n",
       "      <td>349300</td>\n",
       "      <td>117700</td>\n",
       "      <td>209300</td>\n",
       "      <td>225800</td>\n",
       "      <td>133400</td>\n",
       "      <td>664400</td>\n",
       "      <td>357200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 14723 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              60657   75070   77494    60614   79936   77084   10467   60640  \\\n",
       "1996-04-01   334200  235700  210400   498100   77300   95000  152900  216500   \n",
       "1996-05-01   335400  236900  212200   500900   77300   95200  152700  216700   \n",
       "1996-06-01   336500  236700  212200   503100   77300   95400  152600  216900   \n",
       "1996-07-01   337600  235400  210700   504600   77300   95700  152400  217000   \n",
       "1996-08-01   338500  233300  208300   505500   77400   95900  152300  217100   \n",
       "...             ...     ...     ...      ...     ...     ...     ...     ...   \n",
       "2017-12-01  1018700  316600  321200  1299000  120300  162800  414300  777900   \n",
       "2018-01-01  1024400  318100  321200  1302700  120300  162800  413900  778500   \n",
       "2018-02-01  1030700  319600  323000  1306400  120500  162900  411400  780500   \n",
       "2018-03-01  1033800  321100  326900  1308500  121000  163500  413200  782800   \n",
       "2018-04-01  1030600  321800  329900  1307000  121500  164300  417900  782800   \n",
       "\n",
       "             77449    94109  ...   3765    84781   12429   97028   12720  \\\n",
       "1996-04-01   95400   766000  ...   80800  135900   78300  136200   62500   \n",
       "1996-05-01   95600   771100  ...   80100  136300   78300  136600   62600   \n",
       "1996-06-01   95800   776500  ...   79400  136600   78200  136800   62700   \n",
       "1996-07-01   96100   781900  ...   78600  136900   78200  136800   62700   \n",
       "1996-08-01   96400   787300  ...   77900  137100   78100  136700   62700   \n",
       "...            ...      ...  ...     ...     ...     ...     ...     ...   \n",
       "2017-12-01  172300  3778700  ...  123400  257600  171300  341000  122800   \n",
       "2018-01-01  173300  3770800  ...  124400  258000  172400  342300  123200   \n",
       "2018-02-01  174200  3763100  ...  125500  260600  173600  345000  123200   \n",
       "2018-03-01  175400  3779800  ...  126600  264700  175800  348000  120700   \n",
       "2018-04-01  176200  3813500  ...  127500  266800  177500  349300  117700   \n",
       "\n",
       "             1338    3293    40404   81225   89155  \n",
       "1996-04-01   94600   92700   57100  191100  176400  \n",
       "1996-05-01   94300   92500   57300  192400  176300  \n",
       "1996-06-01   94000   92400   57500  193700  176100  \n",
       "1996-07-01   93700   92200   57700  195000  176000  \n",
       "1996-08-01   93400   92100   58000  196300  175900  \n",
       "...            ...     ...     ...     ...     ...  \n",
       "2017-12-01  216400  213100  130600  694700  348900  \n",
       "2018-01-01  213100  213700  131700  706400  350400  \n",
       "2018-02-01  209800  218300  132500  705300  353000  \n",
       "2018-03-01  209200  222700  133000  681500  356000  \n",
       "2018-04-01  209300  225800  133400  664400  357200  \n",
       "\n",
       "[265 rows x 14723 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series = pd.DataFrame(index=pd.to_datetime(df.columns[7:]), data=np.ones(len(df.columns)-7))\n",
    "for i in range(df.shape[0]):\n",
    "    df_time_series[df['RegionName'][i]] = df.iloc[i,7:]\n",
    "df_time_series.drop(df_time_series.columns[0],axis=1, inplace=True)\n",
    "df_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156891"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89108"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nv = df[df['State'] == 'NV']\n",
    "nv_zipcodes = df_nv.RegionName.tolist()\n",
    "nv_zipcodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = df_time_series[nv_zipcodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ferityikar/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/frame.py:4317: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "df_time_series.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_time_series.iloc[:,0].plot(figsize=(16,9))\n",
    "df_time_series.iloc[:,7].plot()\n",
    "df_time_series.iloc[:,4].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(df_time_series)*.85)\n",
    "train_pre = df_time_series.iloc[:size]\n",
    "test = df_time_series.iloc[size:]\n",
    "size2 = int(len(train_pre)*.85)\n",
    "train = train_pre.iloc[:size2]\n",
    "validation = train_pre.iloc[size2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying a RNN model to see how it does on our fourth(random) zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3\n",
    "train_data = train.iloc[:,x:x+1].values.astype(int)\n",
    "val_data = validation.iloc[:,x:x+1].values.astype(int)\n",
    "test_data = test.iloc[:,x:x+1].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "train_data_scaled = scaler.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataset with 60 timesteps (5 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60,len(train_data_scaled)):\n",
    "    X_train.append(train_data_scaled[i-60:i])\n",
    "    y_train.append(train_data_scaled[i])\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Validation Data \n",
    "data_total_val = pd.concat((train.iloc[:,x:x+1], validation.iloc[:,x:x+1]),axis=0)\n",
    "val_input = data_total_val[len(train)-60:].values\n",
    "val_input = scaler.transform(val_input)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for i in range(60,len(val_input)):\n",
    "    X_val.append(val_input[i-60:i])\n",
    "    y_val.append(val_input[i])\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "\n",
    "# Test Data \n",
    "data_total = pd.concat((train_pre.iloc[:,x:x+1], test.iloc[:,x:x+1]),axis=0)\n",
    "test_input = data_total[len(train_pre)-60:].values\n",
    "test_input = scaler.transform(test_input)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(60,len(test_input)):\n",
    "    X_test.append(test_input[i-60:i])\n",
    "    y_test.append(test_input[i])\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 60)                14880     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 14,941\n",
      "Trainable params: 14,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(LSTM(units= 60, return_sequences = False, input_shape=((60,1))))\n",
    "rnn_model.add(Dense(units=1))\n",
    "rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 102ms/step - loss: 0.5973 - val_loss: 0.0367\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3454 - val_loss: 0.0065\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1537 - val_loss: 0.0069\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0401 - val_loss: 0.1045\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0923 - val_loss: 0.0546\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0362 - val_loss: 0.0155\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0291 - val_loss: 0.0067\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0342 - val_loss: 0.0063\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0307 - val_loss: 0.0101\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0234 - val_loss: 0.0172\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0195 - val_loss: 0.0239\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0205 - val_loss: 0.0257\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0198 - val_loss: 0.0207\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0170 - val_loss: 0.0149\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0158 - val_loss: 0.0111\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0154 - val_loss: 0.0101\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0144 - val_loss: 0.0111\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0131 - val_loss: 0.0125\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0123 - val_loss: 0.0121\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0115 - val_loss: 0.0107\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0103 - val_loss: 0.0082\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0093 - val_loss: 0.0068\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0085 - val_loss: 0.0067\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0074 - val_loss: 0.0066\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0059 - val_loss: 0.0063\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0053 - val_loss: 0.0057\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0041 - val_loss: 0.0034\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0040 - val_loss: 0.0033\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0040 - val_loss: 0.0031\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0036\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0030\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0039 - val_loss: 0.0027\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0037 - val_loss: 0.0031\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0036 - val_loss: 0.0025\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0035 - val_loss: 0.0026\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0033 - val_loss: 0.0026\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0032 - val_loss: 0.0025\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0033 - val_loss: 0.0023\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0031 - val_loss: 0.0026\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0031 - val_loss: 0.0023\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0030 - val_loss: 0.0023\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0021\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0027 - val_loss: 0.0019\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0023 - val_loss: 0.0019\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0023 - val_loss: 0.0018\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0019 - val_loss: 0.0016\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0022 - val_loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2bc3a5100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_raw_val = rnn_model.predict(X_val)\n",
    "y_hat_val = scaler.inverse_transform(y_hat_raw_val)\n",
    "y_val_unscaled = scaler.inverse_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_val_unscaled, color='red', label='Real Prices')\n",
    "plt.plot(y_hat_val, color='blue', label='Predicted Prices')\n",
    "plt.title('Unseen Data Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our model was successful on the first zipcode, when we check its success on other zipcodes we see that the results are not always good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import our dictionaries with all model scores from our dictionaries folder. To see how the model was run for all our zipcodes and how validation, test errors were calculated you can check ferit_yikar.ipynb file\n",
    "We have 4 different RNN models for each zip-code. Later we willcompare all our models to choose best model for all zip-codes seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in = open('jupyter_files/dictionaries/rnn_dict.pickle','rb')\n",
    "rnn_dict = pickle.load(print_in)\n",
    "print_in.close()\n",
    "\n",
    "print_in = open('jupyter_files/dictionaries/rnn_w_dropout_dict.pickle','rb')\n",
    "rnn_w_dropout_dict = pickle.load(print_in)\n",
    "print_in.close()\n",
    "\n",
    "print_in = open('jupyter_files/dictionaries/rnn_2_layer_dict.pickle','rb')\n",
    "rnn_2_layer_dict = pickle.load(print_in)\n",
    "print_in.close()\n",
    "\n",
    "print_in = open('jupyter_files/dictionaries/rnn_2_layer_w_dropout_dict.pickle','rb')\n",
    "rnn_2_layer_w_dropout_dict = pickle.load(print_in)\n",
    "print_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average our first RNN model has validation MAPE of 4% and test MAPE of 11%. We can use this model for some zipcodes but not all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11202410120109184,\n",
       " [0.02324273698072249,\n",
       "  0.09075772556037923,\n",
       "  0.034470547823716,\n",
       "  0.12895062585632752,\n",
       "  0.029378933855414725,\n",
       "  0.022868370211432492,\n",
       "  0.039684655703900194,\n",
       "  0.02964637792464425,\n",
       "  0.03665875360957248,\n",
       "  0.03329397750236747,\n",
       "  0.04887042762494738,\n",
       "  0.06338780087602505,\n",
       "  0.04776643009723585,\n",
       "  0.008295738576931755,\n",
       "  0.02053601332359013,\n",
       "  0.044532981580081746,\n",
       "  0.02729537624332177,\n",
       "  0.008685779312938529,\n",
       "  0.02498901412777501,\n",
       "  0.0111600487900889,\n",
       "  0.03850802042367164,\n",
       "  0.04358463164315352,\n",
       "  0.03962941224781203,\n",
       "  0.07853087203338435,\n",
       "  0.03629519807414032,\n",
       "  0.8325646496325515,\n",
       "  0.07281514361528459,\n",
       "  0.7362199911679829,\n",
       "  0.06153824421121954,\n",
       "  0.010681586413580206,\n",
       "  0.02521902261524228,\n",
       "  0.042711533335936094,\n",
       "  0.03584006405155033,\n",
       "  0.03518618644495326,\n",
       "  0.028052552156390038,\n",
       "  0.037681999804238045,\n",
       "  0.12573779593128578,\n",
       "  0.10823060078184607,\n",
       "  0.009379827076333499,\n",
       "  0.04218185547614392,\n",
       "  0.035127576936861804,\n",
       "  0.04461200122813491,\n",
       "  0.0578265845910539,\n",
       "  0.03791940496358292,\n",
       "  0.024706139939923093,\n",
       "  0.060105978260714954,\n",
       "  0.058480721048837794,\n",
       "  0.024865866507304758,\n",
       "  0.6883182818579276,\n",
       "  0.036145271299921046,\n",
       "  0.02975210816859166,\n",
       "  0.07068000594279708,\n",
       "  0.4064778161979697,\n",
       "  0.05270237553979061,\n",
       "  0.05457502972095844,\n",
       "  0.05058152350984232,\n",
       "  0.01961431117962244,\n",
       "  0.03295044042064042,\n",
       "  0.0815622139404831,\n",
       "  0.6470682499529812,\n",
       "  0.025824037846329896,\n",
       "  0.038498163636300625,\n",
       "  0.017631641134536035,\n",
       "  0.09979547902540706,\n",
       "  0.572111794669424,\n",
       "  0.03664509277208583,\n",
       "  0.04167824493094232,\n",
       "  0.0260358132326577,\n",
       "  0.04526378372748169,\n",
       "  0.022891300945080014,\n",
       "  0.02989309216526772,\n",
       "  0.385406707390472,\n",
       "  0.0846368911911109,\n",
       "  0.5494566929692712,\n",
       "  0.03885765360221083,\n",
       "  0.016845209189167005,\n",
       "  0.02622971110060917,\n",
       "  0.026220290898466427,\n",
       "  0.03921069609550251,\n",
       "  0.00837922805262121,\n",
       "  0.7147388793932401,\n",
       "  0.36036002856767607,\n",
       "  0.7303766493129265,\n",
       "  0.0664464771105189,\n",
       "  0.11229610699542736,\n",
       "  0.07216848550471619,\n",
       "  0.05479118695650627,\n",
       "  0.016534748311037507,\n",
       "  0.019933074674123495,\n",
       "  0.048648592266616456,\n",
       "  0.04246690585854036,\n",
       "  0.013429929386041619,\n",
       "  0.06065187923104616,\n",
       "  0.11882982160436531,\n",
       "  0.010436991536207781,\n",
       "  0.06342068882946472,\n",
       "  0.04714468661312939,\n",
       "  0.02595654041642883,\n",
       "  0.022258930811882134,\n",
       "  0.39787210222913344,\n",
       "  0.013128382784297099,\n",
       "  0.6470038434362788,\n",
       "  0.018942563381857885])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_val_mape = []\n",
    "for z in rnn_dict.values():\n",
    "    rnn_val_mape.append(z[1])\n",
    "np.mean(rnn_val_mape), rnn_val_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Prophet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models below iterates over every zipcode in Nevada and predecits the prices in May 2018. It cross validates the model fitted with 17 years of training data, one year of validation set (horizon) and the model makes predictions per zipcode for the test data, each year between 2012 and 2018. It returns average Mean Absolute Percentage Error (MAPE) for each zipcode for both cross validation and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import our dictionaries with all model scores from our dictionaries folder. To see how the model was run for all our zipcodes and how validation, test errors were calculated you can check ferit_yikar.ipynb file\n",
    "We have 2 different Facebook Prophet models for each zip-code. Later we willcompare all our models to choose best model for all zip-codes seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in = open('jupyter_files/dictionaries/fbp_50_scale_dict.pickle','rb')\n",
    "fbp_50_scale_dict = pickle.load(print_in)\n",
    "print_in.close()\n",
    "\n",
    "print_in = open('jupyter_files/dictionaries/fbp_25_scale_dict.pickle','rb')\n",
    "fbp_25_scale_dict = pickle.load(print_in)\n",
    "print_in.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average our first Facebook Prophet model has validation MAPE of 7% and test MAPE of 39%. We can use this model for some zipcodes but not all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.38811971706470977,\n",
       " [0.5748897720011332,\n",
       "  0.6062542405439542,\n",
       "  0.4025173509587912,\n",
       "  0.30864445191825685,\n",
       "  0.37382485248843633,\n",
       "  0.44865712466382734,\n",
       "  0.49986656482342,\n",
       "  0.40817639919743015,\n",
       "  0.4379469576936214,\n",
       "  0.3411474802703965,\n",
       "  0.384676001856983,\n",
       "  0.6251535560807078,\n",
       "  0.43385193642508857,\n",
       "  0.47571150677822016,\n",
       "  0.532599358749837,\n",
       "  0.49401727579586263,\n",
       "  0.40741549742173727,\n",
       "  0.4304736373936943,\n",
       "  0.4476344615185995,\n",
       "  0.395709500902834,\n",
       "  0.4897729046509126,\n",
       "  0.3931726925130529,\n",
       "  0.45494513021997685,\n",
       "  0.5930266821363646,\n",
       "  0.38346826780557625,\n",
       "  0.2767431208024321,\n",
       "  0.5723447703879934,\n",
       "  0.4417679775974168,\n",
       "  0.4503109167012608,\n",
       "  0.5675948850760327,\n",
       "  0.35917384319286316,\n",
       "  0.4687530988664417,\n",
       "  0.39681774947108595,\n",
       "  0.3206578467566916,\n",
       "  0.3689430070445902,\n",
       "  0.4411265835987629,\n",
       "  0.2988549852234578,\n",
       "  0.5829303710954058,\n",
       "  0.29754000550491355,\n",
       "  0.4638014244149111,\n",
       "  0.4590650127714048,\n",
       "  0.425751689022627,\n",
       "  0.5873428916699904,\n",
       "  0.38643639928011214,\n",
       "  0.37573404583112346,\n",
       "  0.42964404652055316,\n",
       "  0.5161154617194543,\n",
       "  0.5094323904314831,\n",
       "  0.22188868550263507,\n",
       "  0.497295593347014,\n",
       "  0.3526776456353473,\n",
       "  0.4261509300763127,\n",
       "  0.18605038006532307,\n",
       "  0.40087210954370234,\n",
       "  0.28727630430323514,\n",
       "  0.5147095964273418,\n",
       "  0.3906516592920465,\n",
       "  0.42678521800000946,\n",
       "  0.4354876555844305,\n",
       "  0.18866819120027137,\n",
       "  0.311113199383926,\n",
       "  0.3007687151312329,\n",
       "  0.4350064526948441,\n",
       "  0.22841167915728436,\n",
       "  0.16488227735459848,\n",
       "  0.49809778782389597,\n",
       "  0.5430545789476011,\n",
       "  0.29887475536122216,\n",
       "  0.32781197191257505,\n",
       "  0.34527413269615076,\n",
       "  0.3240566308338125,\n",
       "  0.1972316236046547,\n",
       "  0.1707370074508147,\n",
       "  0.29487591640079563,\n",
       "  0.46278443875245107,\n",
       "  0.4216961541208077,\n",
       "  0.40651916802224575,\n",
       "  0.4481145912083443,\n",
       "  0.41714886728495837,\n",
       "  0.39587401999331795,\n",
       "  0.1263294007067059,\n",
       "  0.41758828984270857,\n",
       "  0.17126642848040285,\n",
       "  0.37363904163418543,\n",
       "  0.08003626909053015,\n",
       "  0.3086498717706906,\n",
       "  0.37090111001807813,\n",
       "  0.3331985863141805,\n",
       "  0.39442661683195035,\n",
       "  0.33429946752134376,\n",
       "  0.4777090238367918,\n",
       "  0.30429844409472473,\n",
       "  0.29944808905928555,\n",
       "  0.32744045807117583,\n",
       "  0.30989699384728703,\n",
       "  0.20975633351717735,\n",
       "  0.4843891030373114,\n",
       "  0.24086021270371025,\n",
       "  0.3126856662354129,\n",
       "  0.15732546336109335,\n",
       "  0.5613130532458728,\n",
       "  0.37427112735902346,\n",
       "  0.3453177421825445])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_val_mape = []\n",
    "for z in fbp_50_scale_dict.values():\n",
    "    fb_val_mape.append(z[1])\n",
    "np.mean(fb_val_mape), fb_val_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMA - SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in = open('jupyter_files/SARIMAX_summary.pickl','rb')\n",
    "sarimax_dict = pickle.load(print_in)\n",
    "print_in.close()\n",
    "print_in = open('jupyter_files/SARIMA_summary.pickl','rb')\n",
    "sarima_dict = pickle.load(print_in)\n",
    "print_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14147840001613393"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sarima_dict.values())[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [rnn_dict, rnn_2_layer_dict, rnn_2_layer_w_dropout_dict, rnn_w_dropout_dict, fbp_50_scale_dict]#, sarima_dict]\n",
    "best_model_dict = {}\n",
    "for zipcode in rnn_dict.keys():\n",
    "    best_model = [1]\n",
    "    for model in models:\n",
    "        if model[zipcode][0]<best_model[0]:\n",
    "            best_model = model[zipcode]\n",
    "    best_model_dict[zipcode] = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{89108: [0.03918008339347867,\n",
       "  0.02324273698072249,\n",
       "  array([[206739.39]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89121: [0.04249053080533641,\n",
       "  0.09075772556037923,\n",
       "  array([[204989.14]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89117: [0.028123232541239714,\n",
       "  0.045895206093298516,\n",
       "  array([[332322.88]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89052: [0.07413099525999427,\n",
       "  0.30864445191825685,\n",
       "  229682.2443676644,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89123: [0.02305602132279191,\n",
       "  0.011121031869867733,\n",
       "  array([[305719.94]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89031: [0.02240274911956474,\n",
       "  0.022868370211432492,\n",
       "  array([[240816.86]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89110: [0.03143365884024171,\n",
       "  0.07795946815163626,\n",
       "  array([[192151.]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89074: [0.02461964466857517,\n",
       "  0.016955612960913333,\n",
       "  array([[312295.03]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89103: [0.020230009987194573,\n",
       "  0.03665875360957248,\n",
       "  array([[250961.8]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89148: [0.03206962109933308,\n",
       "  0.027555278884971675,\n",
       "  array([[302781.97]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89147: [0.02492848783461322,\n",
       "  0.04887042762494738,\n",
       "  array([[265378.44]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89119: [0.04708653721453273,\n",
       "  0.06338780087602505,\n",
       "  array([[236049.47]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89129: [0.023377066907098898,\n",
       "  0.04776643009723585,\n",
       "  array([[276296.78]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89122: [0.024444143664827393,\n",
       "  0.008295738576931755,\n",
       "  array([[211914.44]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89115: [0.020478888982663934,\n",
       "  0.03813845307477733,\n",
       "  array([[173611.97]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89502: [0.019394399597616983,\n",
       "  0.044532981580081746,\n",
       "  array([[280435.62]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89014: [0.02051603703483403,\n",
       "  0.04637591348038984,\n",
       "  array([[280800.]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89131: [0.02234317176379308,\n",
       "  0.0352344442531203,\n",
       "  array([[331004.06]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89509: [0.028534813351679976,\n",
       "  0.029498854400012174,\n",
       "  array([[432338.9]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89436: [0.02772421229075083,\n",
       "  0.0111600487900889,\n",
       "  array([[378837.12]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89015: [0.0285866787275961,\n",
       "  0.03850802042367164,\n",
       "  array([[251806.58]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89128: [0.027227066318064624,\n",
       "  0.04358463164315352,\n",
       "  array([[271911.2]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89523: [0.024815995826525057,\n",
       "  0.022116312702984548,\n",
       "  array([[391011.22]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89104: [0.02787402702206847,\n",
       "  0.0471366220025526,\n",
       "  array([[208524.]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89012: [0.025782151700148987,\n",
       "  0.0382602477437557,\n",
       "  array([[335998.1]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89030: [0.0, 0.2767431208024321, 62100.0, 'Facebook Prophet 0.5 Scale'],\n",
       " 89431: [0.03982397137602468,\n",
       "  0.03815724733471657,\n",
       "  array([[243782.95]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89032: [0.02435210625527166,\n",
       "  0.014453127312831045,\n",
       "  array([[227407.]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89506: [0.032410059608340605,\n",
       "  0.07719998737821851,\n",
       "  array([[270413.03]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89102: [0.030151874467689202,\n",
       "  0.010681586413580206,\n",
       "  array([[218885.69]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89139: [0.03416216647779677,\n",
       "  0.03369473401855004,\n",
       "  array([[287437.78]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89149: [0.03020102233697849,\n",
       "  0.042711533335936094,\n",
       "  array([[301354.53]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89178: [0.030180608369524756,\n",
       "  0.03584006405155033,\n",
       "  array([[292375.47]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89113: [0.02825407942549682,\n",
       "  0.03518618644495326,\n",
       "  array([[304314.97]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89521: [0.03428616594461293,\n",
       "  0.02326395652104087,\n",
       "  array([[402596.84]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89183: [0.017644404319569283,\n",
       "  0.037681999804238045,\n",
       "  array([[270984.16]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89135: [0.025592319667568624,\n",
       "  0.026726473701510843,\n",
       "  array([[412659.8]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89107: [0.04806053831933459,\n",
       "  0.007211643273714699,\n",
       "  array([[197391.03]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89511: [0.015128621016044138,\n",
       "  0.009379827076333499,\n",
       "  array([[650157.6]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89002: [0.023340184704304105,\n",
       "  0.04218185547614392,\n",
       "  array([[300737.56]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89130: [0.024898875109147105,\n",
       "  0.035127576936861804,\n",
       "  array([[270581.6]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89134: [0.028247919650316575,\n",
       "  0.035763989615938095,\n",
       "  array([[313139.97]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89503: [0.04393609920874447,\n",
       "  0.017877839527142926,\n",
       "  array([[316642.3]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89081: [0.035024718934313165,\n",
       "  0.042674718935269765,\n",
       "  array([[252159.83]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89141: [0.022761032010995132,\n",
       "  0.024706139939923093,\n",
       "  array([[316948.5]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89011: [0.028554922138012973,\n",
       "  0.05612697934015769,\n",
       "  array([[262186.8]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89142: [0.02871841435337775,\n",
       "  0.05803488985532168,\n",
       "  array([[205930.2]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89434: [0.0386242288083223,\n",
       "  0.024865866507304758,\n",
       "  array([[296423.4]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89512: [0.0, 0.22188868550263507, 115600.0, 'Facebook Prophet 0.5 Scale'],\n",
       " 89145: [0.02131109046032544,\n",
       "  0.036145271299921046,\n",
       "  array([[240277.45]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89084: [0.024468790217601864,\n",
       "  0.02975210816859166,\n",
       "  array([[296102.3]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89701: [0.026525679964151852,\n",
       "  0.07068000594279708,\n",
       "  array([[265169.]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89801: [0.003197895622070568,\n",
       "  0.18605038006532307,\n",
       "  165674.9113653253,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89120: [0.035821297882089,\n",
       "  0.05270237553979061,\n",
       "  array([[262426.88]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89044: [0.01614564672361334,\n",
       "  0.03151480104371876,\n",
       "  array([[349552.47]], dtype=float32),\n",
       "  'RNN 2 Layer Model'],\n",
       " 89156: [0.032590240958653205,\n",
       "  0.05058152350984232,\n",
       "  array([[200560.62]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89048: [0.04699048327043555,\n",
       "  0.3906516592920465,\n",
       "  74636.93072923394,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89118: [0.023137440949657265,\n",
       "  0.03295044042064042,\n",
       "  array([[264572.38]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89706: [0.03282951964714845,\n",
       "  0.08425611364301155,\n",
       "  array([[261385.22]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89408: [0.0, 0.18866819120027137, 130900.0, 'Facebook Prophet 0.5 Scale'],\n",
       " 89166: [0.017171340465638812,\n",
       "  0.05284322112895361,\n",
       "  array([[278315.5]], dtype=float32),\n",
       "  'RNN 2 Layer Model'],\n",
       " 89144: [0.022278700444537582,\n",
       "  0.038498163636300625,\n",
       "  array([[339739.22]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89146: [0.027806433178535796,\n",
       "  0.017631641134536035,\n",
       "  array([[316637.25]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89027: [0.020740086093070157,\n",
       "  0.09979547902540706,\n",
       "  array([[236619.5]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89403: [0.0, 0.16488227735459848, 158700.0, 'Facebook Prophet 0.5 Scale'],\n",
       " 89433: [0.025982609278387397,\n",
       "  0.03664509277208583,\n",
       "  array([[269685.12]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89005: [0.02579207246055772,\n",
       "  0.04167824493094232,\n",
       "  array([[310641.6]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89138: [0.024203707878021945,\n",
       "  0.0260358132326577,\n",
       "  array([[429147.1]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89460: [0.02155220634351883,\n",
       "  0.04526378372748169,\n",
       "  array([[316541.72]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89423: [0.025744513303702633,\n",
       "  0.02797251913627973,\n",
       "  array([[390167.53]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89410: [0.02266121148259634,\n",
       "  0.02989309216526772,\n",
       "  array([[364308.94]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89815: [0.0040705597099636535,\n",
       "  0.1972316236046547,\n",
       "  157628.88032379237,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89029: [0.01887977325164621,\n",
       "  0.07952504371731113,\n",
       "  array([[179293.98]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89109: [0.001085619317763456,\n",
       "  0.29487591640079563,\n",
       "  147029.74307195845,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89508: [0.02404940841422752,\n",
       "  0.03885765360221083,\n",
       "  array([[295647.6]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89703: [0.026605819705325644,\n",
       "  0.016845209189167005,\n",
       "  array([[395978.06]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89441: [0.04616578523780177,\n",
       "  0.02622971110060917,\n",
       "  array([[404043.8]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89060: [0.051374730839320246,\n",
       "  0.4481145912083443,\n",
       "  43268.07774765215,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89143: [0.029578799592597304,\n",
       "  0.03921069609550251,\n",
       "  array([[278578.9]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89519: [0.02622597364713289,\n",
       "  0.00837922805262121,\n",
       "  array([[530255.]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89447: [0.0, 0.1263294007067059, 105800.0, 'Facebook Prophet 0.5 Scale'],\n",
       " 89179: [0.0605525696581128,\n",
       "  0.08430970827822995,\n",
       "  array([[274424.34]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89429: [0.0, 0.17126642848040285, 100000.0, 'Facebook Prophet 0.5 Scale'],\n",
       " 89061: [0.0462120730553399,\n",
       "  0.37363904163418543,\n",
       "  91747.13796971427,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89451: [0.01146325738703841,\n",
       "  0.11453240842348449,\n",
       "  array([[887312.9]], dtype=float32),\n",
       "  'RNN 2 Layer Model'],\n",
       " 89501: [0.053523688064274745,\n",
       "  0.05064160048424097,\n",
       "  array([[336516.84]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89705: [0.01937345629182956,\n",
       "  0.06401691101955152,\n",
       "  array([[311722.75]], dtype=float32),\n",
       "  'RNN 2 Layer Model'],\n",
       " 89510: [0.04755960050638048,\n",
       "  0.016534748311037507,\n",
       "  array([[419795.16]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89086: [0.01673016945521541,\n",
       "  0.03195043797599434,\n",
       "  array([[276370.8]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89448: [0.020017315596736633,\n",
       "  0.048648592266616456,\n",
       "  array([[711522.44]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89704: [0.04805456593353047,\n",
       "  0.060358284469924486,\n",
       "  array([[384078.25]], dtype=float32),\n",
       "  'RNN 2 Layer Model'],\n",
       " 89449: [0.01973184372284484,\n",
       "  0.013429929386041619,\n",
       "  array([[375250.25]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89040: [0.023108450067453077,\n",
       "  0.09858743608405558,\n",
       "  array([[203617.3]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89444: [0.026226758084559564,\n",
       "  0.06654471331692842,\n",
       "  array([[265535.44]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89085: [0.026943162246036954,\n",
       "  0.02956933611526067,\n",
       "  array([[324097.06]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89034: [0.01614472350821041,\n",
       "  0.06342068882946472,\n",
       "  array([[320045.7]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89021: [0.0261632453865171,\n",
       "  0.01956434431718388,\n",
       "  array([[308233.66]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89439: [0.024921618023112452,\n",
       "  0.029563771898699343,\n",
       "  array([[434206.47]], dtype=float32),\n",
       "  'RNN w/ Dropout Model'],\n",
       " 89411: [0.02399191267321002,\n",
       "  0.022258930811882134,\n",
       "  array([[643628.1]], dtype=float32),\n",
       "  'RNN Model'],\n",
       " 89124: [0.05740765026870919,\n",
       "  0.051293241221755936,\n",
       "  array([[320317.47]], dtype=float32),\n",
       "  'RNN 2 Layer w/ Dropout Model'],\n",
       " 89440: [0.06752511405742875,\n",
       "  0.5613130532458728,\n",
       "  36897.156711760465,\n",
       "  'Facebook Prophet 0.5 Scale'],\n",
       " 89413: [0.027003445982966806,\n",
       "  0.16854946154128886,\n",
       "  array([[2011938.9]], dtype=float32),\n",
       "  'RNN 2 Layer Model'],\n",
       " 89155: [0.020357993171009073,\n",
       "  0.018942563381857885,\n",
       "  array([[356689.66]], dtype=float32),\n",
       "  'RNN Model']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07400322811805274"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_val_mape = []\n",
    "for z in best_model_dict.values():\n",
    "    bm_val_mape.append(z[1])\n",
    "bm_val_mape_mean = np.mean(bm_val_mape)\n",
    "bm_val_mape_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Facebook Prophet 0.5 Scale',\n",
       "  'RNN 2 Layer Model',\n",
       "  'RNN 2 Layer w/ Dropout Model',\n",
       "  'RNN Model',\n",
       "  'RNN w/ Dropout Model'},\n",
       " [47, 1, 6, 14, 35])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_types = []\n",
    "for item in best_model_dict.values():\n",
    "    model_types.append([item[3]])\n",
    "list = []\n",
    "for item in model_types:\n",
    "    list.append(item[0])\n",
    "labels = set(list)\n",
    "sizes = []\n",
    "#list.count(labels[0])\n",
    "for i in labels:\n",
    "    sizes.append(list.count(i))\n",
    "labels, sizes\n",
    "#labels2 = x = ['FB Prophet 50% Scaler', 'SARIMA','RNN w/ Dropout', 'SARIMAX','RNN 2 Layers','RNN 2 Layers w/ Dropout','RNN']\n",
    "#labels, sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1584x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explode = (0.1, 0, 0.1, 0, 0.1)\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(22,10))\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.xticks(fontsize= 50)\n",
    "plt.show()\n",
    "fig1.savefig('pie.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_mape_list = []\n",
    "for item in best_model_dict.values():\n",
    "    best_model_mape_list.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_1_mape = np.mean(rnn_mape)\n",
    "rnn_2_mape = np.mean(rnn_mape2)\n",
    "rnn_3_mape = np.mean(rnn_mape3)\n",
    "rnn_4_mape = np.mean(rnn_mape4)\n",
    "fbp_1_mape = np.mean(fbp_mape)\n",
    "fbp_2_mape = np.mean(fbp_mape2)\n",
    "sarima_mape = np.mean(sarima_mape_list)\n",
    "sarimax_mape = np.mean(sarimax_mape_list)\n",
    "best_model_mape = np.mean(best_model_mape_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-223-de4837f53625>:7: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(['0%','2.5%','5%','7.5%','10%','12.5%','15%','17.5%','20%'])\n",
      "<ipython-input-223-de4837f53625>:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(labels = x, rotation=40)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1872x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(26,10))\n",
    "\n",
    "\n",
    "y = [rnn_1_mape,rnn_2_mape,rnn_3_mape,rnn_4_mape, fbp_1_mape, fbp_2_mape, sarima_mape, sarimax_mape, best_model_mape]\n",
    "x = ['RNN','RNN 2 Layers','RNN w/ Dropout','RNN 2 Layer w/ Dropout', 'FB Prophet 50% Scaler', 'FB Prophet 25% Scaler', 'SARIMA', 'SARIMAX', 'Fusion Model']\n",
    "ax.set_title('Model MAPE Improvement Chart')\n",
    "ax.set_yticklabels(['0%','2.5%','5%','7.5%','10%','12.5%','15%','17.5%','20%'])\n",
    "ax.set_xticklabels(labels = x, rotation=40)\n",
    "ax.set_ylabel('MAPE')\n",
    "ax.set_xlabel('Models')\n",
    "sns.barplot(x=x, y=y, palette=\"coolwarm\")\n",
    "sns.set_style('ticks')\n",
    "sns.set_context(\"talk\");\n",
    "fig.savefig('model_improvement.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding our Best Models predictions to our DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_columns = []\n",
    "for column in df_time_series.columns:\n",
    "    zipcode_columns.append(zipcode_converter[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series.columns = zipcode_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>89108</th>\n",
       "      <th>89121</th>\n",
       "      <th>89117</th>\n",
       "      <th>89052</th>\n",
       "      <th>89123</th>\n",
       "      <th>89031</th>\n",
       "      <th>89110</th>\n",
       "      <th>89074</th>\n",
       "      <th>89103</th>\n",
       "      <th>89148</th>\n",
       "      <th>...</th>\n",
       "      <th>89444</th>\n",
       "      <th>89085</th>\n",
       "      <th>89034</th>\n",
       "      <th>89021</th>\n",
       "      <th>89439</th>\n",
       "      <th>89411</th>\n",
       "      <th>89124</th>\n",
       "      <th>89440</th>\n",
       "      <th>89413</th>\n",
       "      <th>89155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-04-01</th>\n",
       "      <td>102500.0</td>\n",
       "      <td>106800.0</td>\n",
       "      <td>165100.0</td>\n",
       "      <td>185700.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>122800.0</td>\n",
       "      <td>95800.0</td>\n",
       "      <td>148000.0</td>\n",
       "      <td>118900.0</td>\n",
       "      <td>157300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>116800.0</td>\n",
       "      <td>170900.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>153200.0</td>\n",
       "      <td>184200.0</td>\n",
       "      <td>299200.0</td>\n",
       "      <td>166100.0</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562400.0</td>\n",
       "      <td>176400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-01</th>\n",
       "      <td>102500.0</td>\n",
       "      <td>107000.0</td>\n",
       "      <td>164500.0</td>\n",
       "      <td>186300.0</td>\n",
       "      <td>143500.0</td>\n",
       "      <td>122800.0</td>\n",
       "      <td>95800.0</td>\n",
       "      <td>147800.0</td>\n",
       "      <td>119000.0</td>\n",
       "      <td>156000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>117000.0</td>\n",
       "      <td>170800.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>153700.0</td>\n",
       "      <td>185000.0</td>\n",
       "      <td>299600.0</td>\n",
       "      <td>166600.0</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562800.0</td>\n",
       "      <td>176300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-06-01</th>\n",
       "      <td>102500.0</td>\n",
       "      <td>107200.0</td>\n",
       "      <td>164000.0</td>\n",
       "      <td>186900.0</td>\n",
       "      <td>143100.0</td>\n",
       "      <td>122700.0</td>\n",
       "      <td>95800.0</td>\n",
       "      <td>147600.0</td>\n",
       "      <td>119000.0</td>\n",
       "      <td>154700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>117200.0</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195900.0</td>\n",
       "      <td>154100.0</td>\n",
       "      <td>185800.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>167300.0</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562700.0</td>\n",
       "      <td>176100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-07-01</th>\n",
       "      <td>102600.0</td>\n",
       "      <td>107400.0</td>\n",
       "      <td>163500.0</td>\n",
       "      <td>187400.0</td>\n",
       "      <td>142700.0</td>\n",
       "      <td>122700.0</td>\n",
       "      <td>95900.0</td>\n",
       "      <td>147300.0</td>\n",
       "      <td>119100.0</td>\n",
       "      <td>153500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>117400.0</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195700.0</td>\n",
       "      <td>154400.0</td>\n",
       "      <td>186400.0</td>\n",
       "      <td>300200.0</td>\n",
       "      <td>167900.0</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562400.0</td>\n",
       "      <td>176000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-08-01</th>\n",
       "      <td>102700.0</td>\n",
       "      <td>107600.0</td>\n",
       "      <td>163200.0</td>\n",
       "      <td>187700.0</td>\n",
       "      <td>142400.0</td>\n",
       "      <td>122700.0</td>\n",
       "      <td>96100.0</td>\n",
       "      <td>147100.0</td>\n",
       "      <td>119200.0</td>\n",
       "      <td>152600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>117600.0</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195400.0</td>\n",
       "      <td>154700.0</td>\n",
       "      <td>186900.0</td>\n",
       "      <td>300500.0</td>\n",
       "      <td>168600.0</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562300.0</td>\n",
       "      <td>175900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>197300.0</td>\n",
       "      <td>198700.0</td>\n",
       "      <td>327100.0</td>\n",
       "      <td>403800.0</td>\n",
       "      <td>290400.0</td>\n",
       "      <td>231600.0</td>\n",
       "      <td>186600.0</td>\n",
       "      <td>300100.0</td>\n",
       "      <td>240400.0</td>\n",
       "      <td>291400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>266200.0</td>\n",
       "      <td>313100.0</td>\n",
       "      <td>311700.0</td>\n",
       "      <td>298300.0</td>\n",
       "      <td>444500.0</td>\n",
       "      <td>639300.0</td>\n",
       "      <td>316800.0</td>\n",
       "      <td>199800.0</td>\n",
       "      <td>2098400.0</td>\n",
       "      <td>348900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>200700.0</td>\n",
       "      <td>201500.0</td>\n",
       "      <td>330700.0</td>\n",
       "      <td>407300.0</td>\n",
       "      <td>294300.0</td>\n",
       "      <td>234600.0</td>\n",
       "      <td>189200.0</td>\n",
       "      <td>303500.0</td>\n",
       "      <td>243700.0</td>\n",
       "      <td>294100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>316500.0</td>\n",
       "      <td>315500.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>449500.0</td>\n",
       "      <td>642500.0</td>\n",
       "      <td>317600.0</td>\n",
       "      <td>201600.0</td>\n",
       "      <td>2121300.0</td>\n",
       "      <td>350400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>203500.0</td>\n",
       "      <td>204000.0</td>\n",
       "      <td>334600.0</td>\n",
       "      <td>410400.0</td>\n",
       "      <td>297400.0</td>\n",
       "      <td>237200.0</td>\n",
       "      <td>191700.0</td>\n",
       "      <td>306700.0</td>\n",
       "      <td>246300.0</td>\n",
       "      <td>296900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275600.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>302500.0</td>\n",
       "      <td>450100.0</td>\n",
       "      <td>653800.0</td>\n",
       "      <td>323400.0</td>\n",
       "      <td>207000.0</td>\n",
       "      <td>2153600.0</td>\n",
       "      <td>353000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>206600.0</td>\n",
       "      <td>206700.0</td>\n",
       "      <td>338800.0</td>\n",
       "      <td>413700.0</td>\n",
       "      <td>300200.0</td>\n",
       "      <td>239800.0</td>\n",
       "      <td>194500.0</td>\n",
       "      <td>309800.0</td>\n",
       "      <td>249500.0</td>\n",
       "      <td>299400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>282100.0</td>\n",
       "      <td>322400.0</td>\n",
       "      <td>323600.0</td>\n",
       "      <td>305700.0</td>\n",
       "      <td>451100.0</td>\n",
       "      <td>666000.0</td>\n",
       "      <td>334700.0</td>\n",
       "      <td>216500.0</td>\n",
       "      <td>2167100.0</td>\n",
       "      <td>356000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>209300.0</td>\n",
       "      <td>208600.0</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>416100.0</td>\n",
       "      <td>302400.0</td>\n",
       "      <td>241900.0</td>\n",
       "      <td>196600.0</td>\n",
       "      <td>312200.0</td>\n",
       "      <td>252000.0</td>\n",
       "      <td>300800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>286000.0</td>\n",
       "      <td>324700.0</td>\n",
       "      <td>326600.0</td>\n",
       "      <td>307800.0</td>\n",
       "      <td>455300.0</td>\n",
       "      <td>672600.0</td>\n",
       "      <td>344300.0</td>\n",
       "      <td>222800.0</td>\n",
       "      <td>2161900.0</td>\n",
       "      <td>357200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               89108     89121     89117     89052     89123     89031  \\\n",
       "1996-04-01  102500.0  106800.0  165100.0  185700.0  144000.0  122800.0   \n",
       "1996-05-01  102500.0  107000.0  164500.0  186300.0  143500.0  122800.0   \n",
       "1996-06-01  102500.0  107200.0  164000.0  186900.0  143100.0  122700.0   \n",
       "1996-07-01  102600.0  107400.0  163500.0  187400.0  142700.0  122700.0   \n",
       "1996-08-01  102700.0  107600.0  163200.0  187700.0  142400.0  122700.0   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-12-01  197300.0  198700.0  327100.0  403800.0  290400.0  231600.0   \n",
       "2018-01-01  200700.0  201500.0  330700.0  407300.0  294300.0  234600.0   \n",
       "2018-02-01  203500.0  204000.0  334600.0  410400.0  297400.0  237200.0   \n",
       "2018-03-01  206600.0  206700.0  338800.0  413700.0  300200.0  239800.0   \n",
       "2018-04-01  209300.0  208600.0  342000.0  416100.0  302400.0  241900.0   \n",
       "\n",
       "               89110     89074     89103     89148  ...     89444     89085  \\\n",
       "1996-04-01   95800.0  148000.0  118900.0  157300.0  ...  116800.0  170900.0   \n",
       "1996-05-01   95800.0  147800.0  119000.0  156000.0  ...  117000.0  170800.0   \n",
       "1996-06-01   95800.0  147600.0  119000.0  154700.0  ...  117200.0  170700.0   \n",
       "1996-07-01   95900.0  147300.0  119100.0  153500.0  ...  117400.0  170700.0   \n",
       "1996-08-01   96100.0  147100.0  119200.0  152600.0  ...  117600.0  170700.0   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2017-12-01  186600.0  300100.0  240400.0  291400.0  ...  266200.0  313100.0   \n",
       "2018-01-01  189200.0  303500.0  243700.0  294100.0  ...  270000.0  316500.0   \n",
       "2018-02-01  191700.0  306700.0  246300.0  296900.0  ...  275600.0  319500.0   \n",
       "2018-03-01  194500.0  309800.0  249500.0  299400.0  ...  282100.0  322400.0   \n",
       "2018-04-01  196600.0  312200.0  252000.0  300800.0  ...  286000.0  324700.0   \n",
       "\n",
       "               89034     89021     89439     89411     89124     89440  \\\n",
       "1996-04-01  196000.0  153200.0  184200.0  299200.0  166100.0  293200.0   \n",
       "1996-05-01  196000.0  153700.0  185000.0  299600.0  166600.0  293200.0   \n",
       "1996-06-01  195900.0  154100.0  185800.0  299900.0  167300.0  293200.0   \n",
       "1996-07-01  195700.0  154400.0  186400.0  300200.0  167900.0  293200.0   \n",
       "1996-08-01  195400.0  154700.0  186900.0  300500.0  168600.0  293200.0   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2017-12-01  311700.0  298300.0  444500.0  639300.0  316800.0  199800.0   \n",
       "2018-01-01  315500.0  299900.0  449500.0  642500.0  317600.0  201600.0   \n",
       "2018-02-01  319500.0  302500.0  450100.0  653800.0  323400.0  207000.0   \n",
       "2018-03-01  323600.0  305700.0  451100.0  666000.0  334700.0  216500.0   \n",
       "2018-04-01  326600.0  307800.0  455300.0  672600.0  344300.0  222800.0   \n",
       "\n",
       "                89413     89155  \n",
       "1996-04-01   562400.0  176400.0  \n",
       "1996-05-01   562800.0  176300.0  \n",
       "1996-06-01   562700.0  176100.0  \n",
       "1996-07-01   562400.0  176000.0  \n",
       "1996-08-01   562300.0  175900.0  \n",
       "...               ...       ...  \n",
       "2017-12-01  2098400.0  348900.0  \n",
       "2018-01-01  2121300.0  350400.0  \n",
       "2018-02-01  2153600.0  353000.0  \n",
       "2018-03-01  2167100.0  356000.0  \n",
       "2018-04-01  2161900.0  357200.0  \n",
       "\n",
       "[265 rows x 103 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_05_18 = []\n",
    "for zipcode in df_time_series.columns:\n",
    "    if zipcode in best_model_dict.keys():\n",
    "        predictions_05_18.append(best_model_dict[zipcode][1])\n",
    "    else:\n",
    "        predictions_05_18.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions_05_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ferityikar/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df_time_series.loc['2018-05-01_pred'] = predictions_05_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>89108</th>\n",
       "      <th>89121</th>\n",
       "      <th>89117</th>\n",
       "      <th>89052</th>\n",
       "      <th>89123</th>\n",
       "      <th>89031</th>\n",
       "      <th>89110</th>\n",
       "      <th>89074</th>\n",
       "      <th>89103</th>\n",
       "      <th>89148</th>\n",
       "      <th>...</th>\n",
       "      <th>89444</th>\n",
       "      <th>89085</th>\n",
       "      <th>89034</th>\n",
       "      <th>89021</th>\n",
       "      <th>89439</th>\n",
       "      <th>89411</th>\n",
       "      <th>89124</th>\n",
       "      <th>89440</th>\n",
       "      <th>89413</th>\n",
       "      <th>89155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-04-01 00:00:00</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>106800.000000</td>\n",
       "      <td>165100.00000</td>\n",
       "      <td>185700.000</td>\n",
       "      <td>144000.00000</td>\n",
       "      <td>122800.000000</td>\n",
       "      <td>95800.000000</td>\n",
       "      <td>148000.00</td>\n",
       "      <td>118900.000000</td>\n",
       "      <td>157300.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>116800.0000</td>\n",
       "      <td>170900.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>153200.0</td>\n",
       "      <td>184200.00000</td>\n",
       "      <td>299200.0</td>\n",
       "      <td>166100.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562400.000</td>\n",
       "      <td>176400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-01 00:00:00</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>107000.000000</td>\n",
       "      <td>164500.00000</td>\n",
       "      <td>186300.000</td>\n",
       "      <td>143500.00000</td>\n",
       "      <td>122800.000000</td>\n",
       "      <td>95800.000000</td>\n",
       "      <td>147800.00</td>\n",
       "      <td>119000.000000</td>\n",
       "      <td>156000.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117000.0000</td>\n",
       "      <td>170800.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>153700.0</td>\n",
       "      <td>185000.00000</td>\n",
       "      <td>299600.0</td>\n",
       "      <td>166600.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562800.000</td>\n",
       "      <td>176300.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-06-01 00:00:00</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>107200.000000</td>\n",
       "      <td>164000.00000</td>\n",
       "      <td>186900.000</td>\n",
       "      <td>143100.00000</td>\n",
       "      <td>122700.000000</td>\n",
       "      <td>95800.000000</td>\n",
       "      <td>147600.00</td>\n",
       "      <td>119000.000000</td>\n",
       "      <td>154700.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117200.0000</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195900.0</td>\n",
       "      <td>154100.0</td>\n",
       "      <td>185800.00000</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>167300.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562700.000</td>\n",
       "      <td>176100.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-07-01 00:00:00</th>\n",
       "      <td>102600.000000</td>\n",
       "      <td>107400.000000</td>\n",
       "      <td>163500.00000</td>\n",
       "      <td>187400.000</td>\n",
       "      <td>142700.00000</td>\n",
       "      <td>122700.000000</td>\n",
       "      <td>95900.000000</td>\n",
       "      <td>147300.00</td>\n",
       "      <td>119100.000000</td>\n",
       "      <td>153500.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117400.0000</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195700.0</td>\n",
       "      <td>154400.0</td>\n",
       "      <td>186400.00000</td>\n",
       "      <td>300200.0</td>\n",
       "      <td>167900.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562400.000</td>\n",
       "      <td>176000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-08-01 00:00:00</th>\n",
       "      <td>102700.000000</td>\n",
       "      <td>107600.000000</td>\n",
       "      <td>163200.00000</td>\n",
       "      <td>187700.000</td>\n",
       "      <td>142400.00000</td>\n",
       "      <td>122700.000000</td>\n",
       "      <td>96100.000000</td>\n",
       "      <td>147100.00</td>\n",
       "      <td>119200.000000</td>\n",
       "      <td>152600.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117600.0000</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195400.0</td>\n",
       "      <td>154700.0</td>\n",
       "      <td>186900.00000</td>\n",
       "      <td>300500.0</td>\n",
       "      <td>168600.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562300.000</td>\n",
       "      <td>175900.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>200700.000000</td>\n",
       "      <td>201500.000000</td>\n",
       "      <td>330700.00000</td>\n",
       "      <td>407300.000</td>\n",
       "      <td>294300.00000</td>\n",
       "      <td>234600.000000</td>\n",
       "      <td>189200.000000</td>\n",
       "      <td>303500.00</td>\n",
       "      <td>243700.000000</td>\n",
       "      <td>294100.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>270000.0000</td>\n",
       "      <td>316500.0</td>\n",
       "      <td>315500.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>449500.00000</td>\n",
       "      <td>642500.0</td>\n",
       "      <td>317600.0000</td>\n",
       "      <td>201600.0</td>\n",
       "      <td>2121300.000</td>\n",
       "      <td>350400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01 00:00:00</th>\n",
       "      <td>203500.000000</td>\n",
       "      <td>204000.000000</td>\n",
       "      <td>334600.00000</td>\n",
       "      <td>410400.000</td>\n",
       "      <td>297400.00000</td>\n",
       "      <td>237200.000000</td>\n",
       "      <td>191700.000000</td>\n",
       "      <td>306700.00</td>\n",
       "      <td>246300.000000</td>\n",
       "      <td>296900.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>275600.0000</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>302500.0</td>\n",
       "      <td>450100.00000</td>\n",
       "      <td>653800.0</td>\n",
       "      <td>323400.0000</td>\n",
       "      <td>207000.0</td>\n",
       "      <td>2153600.000</td>\n",
       "      <td>353000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 00:00:00</th>\n",
       "      <td>206600.000000</td>\n",
       "      <td>206700.000000</td>\n",
       "      <td>338800.00000</td>\n",
       "      <td>413700.000</td>\n",
       "      <td>300200.00000</td>\n",
       "      <td>239800.000000</td>\n",
       "      <td>194500.000000</td>\n",
       "      <td>309800.00</td>\n",
       "      <td>249500.000000</td>\n",
       "      <td>299400.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>282100.0000</td>\n",
       "      <td>322400.0</td>\n",
       "      <td>323600.0</td>\n",
       "      <td>305700.0</td>\n",
       "      <td>451100.00000</td>\n",
       "      <td>666000.0</td>\n",
       "      <td>334700.0000</td>\n",
       "      <td>216500.0</td>\n",
       "      <td>2167100.000</td>\n",
       "      <td>356000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:00:00</th>\n",
       "      <td>209300.000000</td>\n",
       "      <td>208600.000000</td>\n",
       "      <td>342000.00000</td>\n",
       "      <td>416100.000</td>\n",
       "      <td>302400.00000</td>\n",
       "      <td>241900.000000</td>\n",
       "      <td>196600.000000</td>\n",
       "      <td>312200.00</td>\n",
       "      <td>252000.000000</td>\n",
       "      <td>300800.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>286000.0000</td>\n",
       "      <td>324700.0</td>\n",
       "      <td>326600.0</td>\n",
       "      <td>307800.0</td>\n",
       "      <td>455300.00000</td>\n",
       "      <td>672600.0</td>\n",
       "      <td>344300.0000</td>\n",
       "      <td>222800.0</td>\n",
       "      <td>2161900.000</td>\n",
       "      <td>357200.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01_pred</th>\n",
       "      <td>212876.996122</td>\n",
       "      <td>212357.255857</td>\n",
       "      <td>343835.59375</td>\n",
       "      <td>421466.875</td>\n",
       "      <td>303811.84375</td>\n",
       "      <td>246300.944755</td>\n",
       "      <td>200482.013725</td>\n",
       "      <td>312865.75</td>\n",
       "      <td>251018.223795</td>\n",
       "      <td>305326.5625</td>\n",
       "      <td>...</td>\n",
       "      <td>279638.8125</td>\n",
       "      <td>330100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>453697.84375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333740.6875</td>\n",
       "      <td>190700.0</td>\n",
       "      <td>2082682.875</td>\n",
       "      <td>361679.90625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             89108          89121         89117       89052  \\\n",
       "1996-04-01 00:00:00  102500.000000  106800.000000  165100.00000  185700.000   \n",
       "1996-05-01 00:00:00  102500.000000  107000.000000  164500.00000  186300.000   \n",
       "1996-06-01 00:00:00  102500.000000  107200.000000  164000.00000  186900.000   \n",
       "1996-07-01 00:00:00  102600.000000  107400.000000  163500.00000  187400.000   \n",
       "1996-08-01 00:00:00  102700.000000  107600.000000  163200.00000  187700.000   \n",
       "...                            ...            ...           ...         ...   \n",
       "2018-01-01 00:00:00  200700.000000  201500.000000  330700.00000  407300.000   \n",
       "2018-02-01 00:00:00  203500.000000  204000.000000  334600.00000  410400.000   \n",
       "2018-03-01 00:00:00  206600.000000  206700.000000  338800.00000  413700.000   \n",
       "2018-04-01 00:00:00  209300.000000  208600.000000  342000.00000  416100.000   \n",
       "2018-05-01_pred      212876.996122  212357.255857  343835.59375  421466.875   \n",
       "\n",
       "                            89123          89031          89110      89074  \\\n",
       "1996-04-01 00:00:00  144000.00000  122800.000000   95800.000000  148000.00   \n",
       "1996-05-01 00:00:00  143500.00000  122800.000000   95800.000000  147800.00   \n",
       "1996-06-01 00:00:00  143100.00000  122700.000000   95800.000000  147600.00   \n",
       "1996-07-01 00:00:00  142700.00000  122700.000000   95900.000000  147300.00   \n",
       "1996-08-01 00:00:00  142400.00000  122700.000000   96100.000000  147100.00   \n",
       "...                           ...            ...            ...        ...   \n",
       "2018-01-01 00:00:00  294300.00000  234600.000000  189200.000000  303500.00   \n",
       "2018-02-01 00:00:00  297400.00000  237200.000000  191700.000000  306700.00   \n",
       "2018-03-01 00:00:00  300200.00000  239800.000000  194500.000000  309800.00   \n",
       "2018-04-01 00:00:00  302400.00000  241900.000000  196600.000000  312200.00   \n",
       "2018-05-01_pred      303811.84375  246300.944755  200482.013725  312865.75   \n",
       "\n",
       "                             89103        89148  ...        89444     89085  \\\n",
       "1996-04-01 00:00:00  118900.000000  157300.0000  ...  116800.0000  170900.0   \n",
       "1996-05-01 00:00:00  119000.000000  156000.0000  ...  117000.0000  170800.0   \n",
       "1996-06-01 00:00:00  119000.000000  154700.0000  ...  117200.0000  170700.0   \n",
       "1996-07-01 00:00:00  119100.000000  153500.0000  ...  117400.0000  170700.0   \n",
       "1996-08-01 00:00:00  119200.000000  152600.0000  ...  117600.0000  170700.0   \n",
       "...                            ...          ...  ...          ...       ...   \n",
       "2018-01-01 00:00:00  243700.000000  294100.0000  ...  270000.0000  316500.0   \n",
       "2018-02-01 00:00:00  246300.000000  296900.0000  ...  275600.0000  319500.0   \n",
       "2018-03-01 00:00:00  249500.000000  299400.0000  ...  282100.0000  322400.0   \n",
       "2018-04-01 00:00:00  252000.000000  300800.0000  ...  286000.0000  324700.0   \n",
       "2018-05-01_pred      251018.223795  305326.5625  ...  279638.8125  330100.0   \n",
       "\n",
       "                        89034     89021         89439     89411        89124  \\\n",
       "1996-04-01 00:00:00  196000.0  153200.0  184200.00000  299200.0  166100.0000   \n",
       "1996-05-01 00:00:00  196000.0  153700.0  185000.00000  299600.0  166600.0000   \n",
       "1996-06-01 00:00:00  195900.0  154100.0  185800.00000  299900.0  167300.0000   \n",
       "1996-07-01 00:00:00  195700.0  154400.0  186400.00000  300200.0  167900.0000   \n",
       "1996-08-01 00:00:00  195400.0  154700.0  186900.00000  300500.0  168600.0000   \n",
       "...                       ...       ...           ...       ...          ...   \n",
       "2018-01-01 00:00:00  315500.0  299900.0  449500.00000  642500.0  317600.0000   \n",
       "2018-02-01 00:00:00  319500.0  302500.0  450100.00000  653800.0  323400.0000   \n",
       "2018-03-01 00:00:00  323600.0  305700.0  451100.00000  666000.0  334700.0000   \n",
       "2018-04-01 00:00:00  326600.0  307800.0  455300.00000  672600.0  344300.0000   \n",
       "2018-05-01_pred           0.0       0.0  453697.84375       0.0  333740.6875   \n",
       "\n",
       "                        89440        89413         89155  \n",
       "1996-04-01 00:00:00  293200.0   562400.000  176400.00000  \n",
       "1996-05-01 00:00:00  293200.0   562800.000  176300.00000  \n",
       "1996-06-01 00:00:00  293200.0   562700.000  176100.00000  \n",
       "1996-07-01 00:00:00  293200.0   562400.000  176000.00000  \n",
       "1996-08-01 00:00:00  293200.0   562300.000  175900.00000  \n",
       "...                       ...          ...           ...  \n",
       "2018-01-01 00:00:00  201600.0  2121300.000  350400.00000  \n",
       "2018-02-01 00:00:00  207000.0  2153600.000  353000.00000  \n",
       "2018-03-01 00:00:00  216500.0  2167100.000  356000.00000  \n",
       "2018-04-01 00:00:00  222800.0  2161900.000  357200.00000  \n",
       "2018-05-01_pred      190700.0  2082682.875  361679.90625  \n",
       "\n",
       "[266 rows x 103 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_return = {}\n",
    "for i in df_time_series.columns:\n",
    "    investment_return[i] = (df_time_series[i][-1]-df_time_series[i][-2])/df_time_series[i][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{89448: -1.0,\n",
       " 89449: -1.0,\n",
       " 89034: -1.0,\n",
       " 89021: -1.0,\n",
       " 89411: -1.0,\n",
       " 89440: -0.1440754039497307,\n",
       " 89109: -0.06368849984907939,\n",
       " 89403: -0.05772005772005772,\n",
       " 89436: -0.05225015001536002,\n",
       " 89512: -0.047372954349698536,\n",
       " 89413: -0.03664236319903788,\n",
       " 89429: -0.03379828326180258,\n",
       " 89124: -0.030668929712460064,\n",
       " 89511: -0.02986241620023129,\n",
       " 89156: -0.028897632102470983,\n",
       " 89451: -0.02755555701662108,\n",
       " 89122: -0.02687998117247378,\n",
       " 89410: -0.024777401804670914,\n",
       " 89433: -0.023406785042469288,\n",
       " 89444: -0.022241914335664335,\n",
       " 89423: -0.01899039052890529,\n",
       " 89703: -0.016678927604038413,\n",
       " 89701: -0.016057372505543236,\n",
       " 89519: -0.013868014974433893,\n",
       " 89705: -0.013570170632435817,\n",
       " 89801: -0.011791737329410124,\n",
       " 89521: -0.010857668067226892,\n",
       " 89144: -0.009602167568337129,\n",
       " 89434: -0.008952254641909815,\n",
       " 89509: -0.008843687817820032,\n",
       " 89815: -0.00859215226680593,\n",
       " 89706: -0.0077280844155844155,\n",
       " 89503: -0.00548033526756931,\n",
       " 89523: -0.0052024848254931715,\n",
       " 89103: -0.0038959373226174474,\n",
       " 89704: -0.0037051052198448153,\n",
       " 89439: -0.0035189023720623765,\n",
       " 89447: -0.0021149037569591664,\n",
       " 89134: 0.0002814465408805031,\n",
       " 89135: 0.0007228047766159696,\n",
       " 89501: 0.0007674237940218999,\n",
       " 89118: 0.0014013991514543684,\n",
       " 89074: 0.0021324471492632927,\n",
       " 89460: 0.002898670333367592,\n",
       " 89138: 0.0033907070303583657,\n",
       " 89506: 0.0038006416430090436,\n",
       " 89040: 0.003864246500965251,\n",
       " 89431: 0.004110708275741608,\n",
       " 89123: 0.004668795469576719,\n",
       " 89128: 0.004818333059491853,\n",
       " 89084: 0.004982521186440678,\n",
       " 89117: 0.005367233187134503,\n",
       " 89002: 0.006339834662678631,\n",
       " 89508: 0.0068769475917980815,\n",
       " 89115: 0.00689631270757408,\n",
       " 89029: 0.007226240424241816,\n",
       " 89441: 0.007586334997622444,\n",
       " 89044: 0.007659311985231469,\n",
       " 89131: 0.007676996112440191,\n",
       " 89102: 0.008061170549308945,\n",
       " 89146: 0.008200316355284487,\n",
       " 89178: 0.00960934819897084,\n",
       " 89502: 0.010230144183125343,\n",
       " 89014: 0.011042309722060698,\n",
       " 89142: 0.011910567737782554,\n",
       " 89113: 0.011957274590163934,\n",
       " 89129: 0.01209570583059348,\n",
       " 89048: 0.012444001991040319,\n",
       " 89155: 0.012541730823068309,\n",
       " 89030: 0.012873693870063938,\n",
       " 89052: 0.012898041336217256,\n",
       " 89107: 0.013849103563232377,\n",
       " 89120: 0.014241467975789993,\n",
       " 89141: 0.014408379556259905,\n",
       " 89179: 0.014536210317460318,\n",
       " 89027: 0.014765261012282932,\n",
       " 89148: 0.015048412566489361,\n",
       " 89145: 0.015232606010703994,\n",
       " 89012: 0.015464465431738622,\n",
       " 89149: 0.015549208724349615,\n",
       " 89130: 0.016206261510128914,\n",
       " 89085: 0.016630736064059133,\n",
       " 89108: 0.017090282472717315,\n",
       " 89015: 0.01740494764210884,\n",
       " 89121: 0.018011773042736167,\n",
       " 89031: 0.01819323999528761,\n",
       " 89183: 0.01949963208241354,\n",
       " 89110: 0.019745746310630814,\n",
       " 89104: 0.01981829457598941,\n",
       " 89086: 0.020193354851261818,\n",
       " 89408: 0.020610057708161583,\n",
       " 89147: 0.021103901235491745,\n",
       " 89119: 0.021339230134029687,\n",
       " 89032: 0.021442448761473946,\n",
       " 89061: 0.02262443438914027,\n",
       " 89005: 0.022760683932007697,\n",
       " 89011: 0.022894300075872533,\n",
       " 89081: 0.02315332765638677,\n",
       " 89143: 0.023676292949176808,\n",
       " 89166: 0.02389814733707078,\n",
       " 89510: 0.025691410950661853,\n",
       " 89139: 0.027802706405703665,\n",
       " 89060: 0.05800064931281908}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investment_return = dict(sorted(investment_return.items(), key=lambda item: item[1]))\n",
    "investment_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for i in investment_return.keys():\n",
    "    list.append(i)\n",
    "best_5_investments = list[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89143, 89166, 89510, 89139, 89060]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_5_investments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>89143</th>\n",
       "      <th>89166</th>\n",
       "      <th>89510</th>\n",
       "      <th>89139</th>\n",
       "      <th>89060</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-06-01 00:00:00</th>\n",
       "      <td>244600.00000</td>\n",
       "      <td>255300.00000</td>\n",
       "      <td>400200.00000</td>\n",
       "      <td>254200.000000</td>\n",
       "      <td>119600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01 00:00:00</th>\n",
       "      <td>247300.00000</td>\n",
       "      <td>258600.00000</td>\n",
       "      <td>408000.00000</td>\n",
       "      <td>256600.000000</td>\n",
       "      <td>121800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01 00:00:00</th>\n",
       "      <td>250700.00000</td>\n",
       "      <td>262000.00000</td>\n",
       "      <td>412500.00000</td>\n",
       "      <td>258900.000000</td>\n",
       "      <td>125100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01 00:00:00</th>\n",
       "      <td>255000.00000</td>\n",
       "      <td>265800.00000</td>\n",
       "      <td>415700.00000</td>\n",
       "      <td>261900.000000</td>\n",
       "      <td>127600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01 00:00:00</th>\n",
       "      <td>259500.00000</td>\n",
       "      <td>270100.00000</td>\n",
       "      <td>415100.00000</td>\n",
       "      <td>265800.000000</td>\n",
       "      <td>130000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 00:00:00</th>\n",
       "      <td>262500.00000</td>\n",
       "      <td>273500.00000</td>\n",
       "      <td>416500.00000</td>\n",
       "      <td>269300.000000</td>\n",
       "      <td>134600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01 00:00:00</th>\n",
       "      <td>265500.00000</td>\n",
       "      <td>276500.00000</td>\n",
       "      <td>419500.00000</td>\n",
       "      <td>273100.000000</td>\n",
       "      <td>140100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>268500.00000</td>\n",
       "      <td>279200.00000</td>\n",
       "      <td>421800.00000</td>\n",
       "      <td>276700.000000</td>\n",
       "      <td>143500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01 00:00:00</th>\n",
       "      <td>271700.00000</td>\n",
       "      <td>281700.00000</td>\n",
       "      <td>420600.00000</td>\n",
       "      <td>280100.000000</td>\n",
       "      <td>146700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 00:00:00</th>\n",
       "      <td>275800.00000</td>\n",
       "      <td>283900.00000</td>\n",
       "      <td>418300.00000</td>\n",
       "      <td>283100.000000</td>\n",
       "      <td>149700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:00:00</th>\n",
       "      <td>279400.00000</td>\n",
       "      <td>285400.00000</td>\n",
       "      <td>415500.00000</td>\n",
       "      <td>284900.000000</td>\n",
       "      <td>151400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01_pred</th>\n",
       "      <td>286015.15625</td>\n",
       "      <td>292220.53125</td>\n",
       "      <td>426174.78125</td>\n",
       "      <td>292820.991055</td>\n",
       "      <td>160181.298306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            89143         89166         89510          89139  \\\n",
       "2017-06-01 00:00:00  244600.00000  255300.00000  400200.00000  254200.000000   \n",
       "2017-07-01 00:00:00  247300.00000  258600.00000  408000.00000  256600.000000   \n",
       "2017-08-01 00:00:00  250700.00000  262000.00000  412500.00000  258900.000000   \n",
       "2017-09-01 00:00:00  255000.00000  265800.00000  415700.00000  261900.000000   \n",
       "2017-10-01 00:00:00  259500.00000  270100.00000  415100.00000  265800.000000   \n",
       "2017-11-01 00:00:00  262500.00000  273500.00000  416500.00000  269300.000000   \n",
       "2017-12-01 00:00:00  265500.00000  276500.00000  419500.00000  273100.000000   \n",
       "2018-01-01 00:00:00  268500.00000  279200.00000  421800.00000  276700.000000   \n",
       "2018-02-01 00:00:00  271700.00000  281700.00000  420600.00000  280100.000000   \n",
       "2018-03-01 00:00:00  275800.00000  283900.00000  418300.00000  283100.000000   \n",
       "2018-04-01 00:00:00  279400.00000  285400.00000  415500.00000  284900.000000   \n",
       "2018-05-01_pred      286015.15625  292220.53125  426174.78125  292820.991055   \n",
       "\n",
       "                             89060  \n",
       "2017-06-01 00:00:00  119600.000000  \n",
       "2017-07-01 00:00:00  121800.000000  \n",
       "2017-08-01 00:00:00  125100.000000  \n",
       "2017-09-01 00:00:00  127600.000000  \n",
       "2017-10-01 00:00:00  130000.000000  \n",
       "2017-11-01 00:00:00  134600.000000  \n",
       "2017-12-01 00:00:00  140100.000000  \n",
       "2018-01-01 00:00:00  143500.000000  \n",
       "2018-02-01 00:00:00  146700.000000  \n",
       "2018-03-01 00:00:00  149700.000000  \n",
       "2018-04-01 00:00:00  151400.000000  \n",
       "2018-05-01_pred      160181.298306  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investment_chart_data = df_time_series[best_5_investments][-12:]\n",
    "investment_chart_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1584x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "months = [\"June '17\", \"July '17\",\"Aug '17\",\"Sep '17\",\"Oct '17\",\"Nov '17\",\"Dec '18\",\"Jan '18\",\"Feb '18\",\"Mar '18\",\"Apr '18\", \"May '18\"]\n",
    "fig, ax = plt.subplots(figsize=(22,7))\n",
    "fig = plt.figure()\n",
    "\n",
    "ax.plot(months, investment_chart_data[89143])\n",
    "ax.plot(months, investment_chart_data[89166])\n",
    "ax.plot(months, investment_chart_data[89510])\n",
    "ax.plot(months, investment_chart_data[89139])\n",
    "ax.plot(months, investment_chart_data[89060])\n",
    "\n",
    "\n",
    "\n",
    "ax.set_title('Top 5 Profitable Zipcodes')\n",
    "ax.legend(['89143','89166','89510','89139','89060'], loc=('upper left'));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>89108</th>\n",
       "      <th>89121</th>\n",
       "      <th>89117</th>\n",
       "      <th>89052</th>\n",
       "      <th>89123</th>\n",
       "      <th>89031</th>\n",
       "      <th>89110</th>\n",
       "      <th>89074</th>\n",
       "      <th>89103</th>\n",
       "      <th>89148</th>\n",
       "      <th>...</th>\n",
       "      <th>89444</th>\n",
       "      <th>89085</th>\n",
       "      <th>89034</th>\n",
       "      <th>89021</th>\n",
       "      <th>89439</th>\n",
       "      <th>89411</th>\n",
       "      <th>89124</th>\n",
       "      <th>89440</th>\n",
       "      <th>89413</th>\n",
       "      <th>89155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1996-04-01 00:00:00</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>106800.000000</td>\n",
       "      <td>165100.00000</td>\n",
       "      <td>185700.000</td>\n",
       "      <td>144000.00000</td>\n",
       "      <td>122800.000000</td>\n",
       "      <td>95800.000000</td>\n",
       "      <td>148000.00</td>\n",
       "      <td>118900.000000</td>\n",
       "      <td>157300.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>116800.0000</td>\n",
       "      <td>170900.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>153200.0</td>\n",
       "      <td>184200.00000</td>\n",
       "      <td>299200.0</td>\n",
       "      <td>166100.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562400.000</td>\n",
       "      <td>176400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-05-01 00:00:00</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>107000.000000</td>\n",
       "      <td>164500.00000</td>\n",
       "      <td>186300.000</td>\n",
       "      <td>143500.00000</td>\n",
       "      <td>122800.000000</td>\n",
       "      <td>95800.000000</td>\n",
       "      <td>147800.00</td>\n",
       "      <td>119000.000000</td>\n",
       "      <td>156000.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117000.0000</td>\n",
       "      <td>170800.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>153700.0</td>\n",
       "      <td>185000.00000</td>\n",
       "      <td>299600.0</td>\n",
       "      <td>166600.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562800.000</td>\n",
       "      <td>176300.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-06-01 00:00:00</th>\n",
       "      <td>102500.000000</td>\n",
       "      <td>107200.000000</td>\n",
       "      <td>164000.00000</td>\n",
       "      <td>186900.000</td>\n",
       "      <td>143100.00000</td>\n",
       "      <td>122700.000000</td>\n",
       "      <td>95800.000000</td>\n",
       "      <td>147600.00</td>\n",
       "      <td>119000.000000</td>\n",
       "      <td>154700.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117200.0000</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195900.0</td>\n",
       "      <td>154100.0</td>\n",
       "      <td>185800.00000</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>167300.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562700.000</td>\n",
       "      <td>176100.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-07-01 00:00:00</th>\n",
       "      <td>102600.000000</td>\n",
       "      <td>107400.000000</td>\n",
       "      <td>163500.00000</td>\n",
       "      <td>187400.000</td>\n",
       "      <td>142700.00000</td>\n",
       "      <td>122700.000000</td>\n",
       "      <td>95900.000000</td>\n",
       "      <td>147300.00</td>\n",
       "      <td>119100.000000</td>\n",
       "      <td>153500.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117400.0000</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195700.0</td>\n",
       "      <td>154400.0</td>\n",
       "      <td>186400.00000</td>\n",
       "      <td>300200.0</td>\n",
       "      <td>167900.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562400.000</td>\n",
       "      <td>176000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996-08-01 00:00:00</th>\n",
       "      <td>102700.000000</td>\n",
       "      <td>107600.000000</td>\n",
       "      <td>163200.00000</td>\n",
       "      <td>187700.000</td>\n",
       "      <td>142400.00000</td>\n",
       "      <td>122700.000000</td>\n",
       "      <td>96100.000000</td>\n",
       "      <td>147100.00</td>\n",
       "      <td>119200.000000</td>\n",
       "      <td>152600.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>117600.0000</td>\n",
       "      <td>170700.0</td>\n",
       "      <td>195400.0</td>\n",
       "      <td>154700.0</td>\n",
       "      <td>186900.00000</td>\n",
       "      <td>300500.0</td>\n",
       "      <td>168600.0000</td>\n",
       "      <td>293200.0</td>\n",
       "      <td>562300.000</td>\n",
       "      <td>175900.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:00:00</th>\n",
       "      <td>200700.000000</td>\n",
       "      <td>201500.000000</td>\n",
       "      <td>330700.00000</td>\n",
       "      <td>407300.000</td>\n",
       "      <td>294300.00000</td>\n",
       "      <td>234600.000000</td>\n",
       "      <td>189200.000000</td>\n",
       "      <td>303500.00</td>\n",
       "      <td>243700.000000</td>\n",
       "      <td>294100.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>270000.0000</td>\n",
       "      <td>316500.0</td>\n",
       "      <td>315500.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>449500.00000</td>\n",
       "      <td>642500.0</td>\n",
       "      <td>317600.0000</td>\n",
       "      <td>201600.0</td>\n",
       "      <td>2121300.000</td>\n",
       "      <td>350400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01 00:00:00</th>\n",
       "      <td>203500.000000</td>\n",
       "      <td>204000.000000</td>\n",
       "      <td>334600.00000</td>\n",
       "      <td>410400.000</td>\n",
       "      <td>297400.00000</td>\n",
       "      <td>237200.000000</td>\n",
       "      <td>191700.000000</td>\n",
       "      <td>306700.00</td>\n",
       "      <td>246300.000000</td>\n",
       "      <td>296900.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>275600.0000</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>302500.0</td>\n",
       "      <td>450100.00000</td>\n",
       "      <td>653800.0</td>\n",
       "      <td>323400.0000</td>\n",
       "      <td>207000.0</td>\n",
       "      <td>2153600.000</td>\n",
       "      <td>353000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01 00:00:00</th>\n",
       "      <td>206600.000000</td>\n",
       "      <td>206700.000000</td>\n",
       "      <td>338800.00000</td>\n",
       "      <td>413700.000</td>\n",
       "      <td>300200.00000</td>\n",
       "      <td>239800.000000</td>\n",
       "      <td>194500.000000</td>\n",
       "      <td>309800.00</td>\n",
       "      <td>249500.000000</td>\n",
       "      <td>299400.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>282100.0000</td>\n",
       "      <td>322400.0</td>\n",
       "      <td>323600.0</td>\n",
       "      <td>305700.0</td>\n",
       "      <td>451100.00000</td>\n",
       "      <td>666000.0</td>\n",
       "      <td>334700.0000</td>\n",
       "      <td>216500.0</td>\n",
       "      <td>2167100.000</td>\n",
       "      <td>356000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01 00:00:00</th>\n",
       "      <td>209300.000000</td>\n",
       "      <td>208600.000000</td>\n",
       "      <td>342000.00000</td>\n",
       "      <td>416100.000</td>\n",
       "      <td>302400.00000</td>\n",
       "      <td>241900.000000</td>\n",
       "      <td>196600.000000</td>\n",
       "      <td>312200.00</td>\n",
       "      <td>252000.000000</td>\n",
       "      <td>300800.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>286000.0000</td>\n",
       "      <td>324700.0</td>\n",
       "      <td>326600.0</td>\n",
       "      <td>307800.0</td>\n",
       "      <td>455300.00000</td>\n",
       "      <td>672600.0</td>\n",
       "      <td>344300.0000</td>\n",
       "      <td>222800.0</td>\n",
       "      <td>2161900.000</td>\n",
       "      <td>357200.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01_pred</th>\n",
       "      <td>212876.996122</td>\n",
       "      <td>212357.255857</td>\n",
       "      <td>343835.59375</td>\n",
       "      <td>421466.875</td>\n",
       "      <td>303811.84375</td>\n",
       "      <td>246300.944755</td>\n",
       "      <td>200482.013725</td>\n",
       "      <td>312865.75</td>\n",
       "      <td>251018.223795</td>\n",
       "      <td>305326.5625</td>\n",
       "      <td>...</td>\n",
       "      <td>279638.8125</td>\n",
       "      <td>330100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>453697.84375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>333740.6875</td>\n",
       "      <td>190700.0</td>\n",
       "      <td>2082682.875</td>\n",
       "      <td>361679.90625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             89108          89121         89117       89052  \\\n",
       "1996-04-01 00:00:00  102500.000000  106800.000000  165100.00000  185700.000   \n",
       "1996-05-01 00:00:00  102500.000000  107000.000000  164500.00000  186300.000   \n",
       "1996-06-01 00:00:00  102500.000000  107200.000000  164000.00000  186900.000   \n",
       "1996-07-01 00:00:00  102600.000000  107400.000000  163500.00000  187400.000   \n",
       "1996-08-01 00:00:00  102700.000000  107600.000000  163200.00000  187700.000   \n",
       "...                            ...            ...           ...         ...   \n",
       "2018-01-01 00:00:00  200700.000000  201500.000000  330700.00000  407300.000   \n",
       "2018-02-01 00:00:00  203500.000000  204000.000000  334600.00000  410400.000   \n",
       "2018-03-01 00:00:00  206600.000000  206700.000000  338800.00000  413700.000   \n",
       "2018-04-01 00:00:00  209300.000000  208600.000000  342000.00000  416100.000   \n",
       "2018-05-01_pred      212876.996122  212357.255857  343835.59375  421466.875   \n",
       "\n",
       "                            89123          89031          89110      89074  \\\n",
       "1996-04-01 00:00:00  144000.00000  122800.000000   95800.000000  148000.00   \n",
       "1996-05-01 00:00:00  143500.00000  122800.000000   95800.000000  147800.00   \n",
       "1996-06-01 00:00:00  143100.00000  122700.000000   95800.000000  147600.00   \n",
       "1996-07-01 00:00:00  142700.00000  122700.000000   95900.000000  147300.00   \n",
       "1996-08-01 00:00:00  142400.00000  122700.000000   96100.000000  147100.00   \n",
       "...                           ...            ...            ...        ...   \n",
       "2018-01-01 00:00:00  294300.00000  234600.000000  189200.000000  303500.00   \n",
       "2018-02-01 00:00:00  297400.00000  237200.000000  191700.000000  306700.00   \n",
       "2018-03-01 00:00:00  300200.00000  239800.000000  194500.000000  309800.00   \n",
       "2018-04-01 00:00:00  302400.00000  241900.000000  196600.000000  312200.00   \n",
       "2018-05-01_pred      303811.84375  246300.944755  200482.013725  312865.75   \n",
       "\n",
       "                             89103        89148  ...        89444     89085  \\\n",
       "1996-04-01 00:00:00  118900.000000  157300.0000  ...  116800.0000  170900.0   \n",
       "1996-05-01 00:00:00  119000.000000  156000.0000  ...  117000.0000  170800.0   \n",
       "1996-06-01 00:00:00  119000.000000  154700.0000  ...  117200.0000  170700.0   \n",
       "1996-07-01 00:00:00  119100.000000  153500.0000  ...  117400.0000  170700.0   \n",
       "1996-08-01 00:00:00  119200.000000  152600.0000  ...  117600.0000  170700.0   \n",
       "...                            ...          ...  ...          ...       ...   \n",
       "2018-01-01 00:00:00  243700.000000  294100.0000  ...  270000.0000  316500.0   \n",
       "2018-02-01 00:00:00  246300.000000  296900.0000  ...  275600.0000  319500.0   \n",
       "2018-03-01 00:00:00  249500.000000  299400.0000  ...  282100.0000  322400.0   \n",
       "2018-04-01 00:00:00  252000.000000  300800.0000  ...  286000.0000  324700.0   \n",
       "2018-05-01_pred      251018.223795  305326.5625  ...  279638.8125  330100.0   \n",
       "\n",
       "                        89034     89021         89439     89411        89124  \\\n",
       "1996-04-01 00:00:00  196000.0  153200.0  184200.00000  299200.0  166100.0000   \n",
       "1996-05-01 00:00:00  196000.0  153700.0  185000.00000  299600.0  166600.0000   \n",
       "1996-06-01 00:00:00  195900.0  154100.0  185800.00000  299900.0  167300.0000   \n",
       "1996-07-01 00:00:00  195700.0  154400.0  186400.00000  300200.0  167900.0000   \n",
       "1996-08-01 00:00:00  195400.0  154700.0  186900.00000  300500.0  168600.0000   \n",
       "...                       ...       ...           ...       ...          ...   \n",
       "2018-01-01 00:00:00  315500.0  299900.0  449500.00000  642500.0  317600.0000   \n",
       "2018-02-01 00:00:00  319500.0  302500.0  450100.00000  653800.0  323400.0000   \n",
       "2018-03-01 00:00:00  323600.0  305700.0  451100.00000  666000.0  334700.0000   \n",
       "2018-04-01 00:00:00  326600.0  307800.0  455300.00000  672600.0  344300.0000   \n",
       "2018-05-01_pred           0.0       0.0  453697.84375       0.0  333740.6875   \n",
       "\n",
       "                        89440        89413         89155  \n",
       "1996-04-01 00:00:00  293200.0   562400.000  176400.00000  \n",
       "1996-05-01 00:00:00  293200.0   562800.000  176300.00000  \n",
       "1996-06-01 00:00:00  293200.0   562700.000  176100.00000  \n",
       "1996-07-01 00:00:00  293200.0   562400.000  176000.00000  \n",
       "1996-08-01 00:00:00  293200.0   562300.000  175900.00000  \n",
       "...                       ...          ...           ...  \n",
       "2018-01-01 00:00:00  201600.0  2121300.000  350400.00000  \n",
       "2018-02-01 00:00:00  207000.0  2153600.000  353000.00000  \n",
       "2018-03-01 00:00:00  216500.0  2167100.000  356000.00000  \n",
       "2018-04-01 00:00:00  222800.0  2161900.000  357200.00000  \n",
       "2018-05-01_pred      190700.0  2082682.875  361679.90625  \n",
       "\n",
       "[266 rows x 103 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 48085.5508 - val_loss: 70.4427\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 132760.1406 - val_loss: 70.2606\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 83623.4375 - val_loss: 77.6148\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16281.0430 - val_loss: 79.2507\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4100.2905 - val_loss: 77.4637\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 70966.1016 - val_loss: 77.3141\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 54380.8008 - val_loss: 80.6926\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17259.9961 - val_loss: 81.6933\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7471.9990 - val_loss: 80.0082\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 55874.8086 - val_loss: 79.7206\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33641.9766 - val_loss: 81.3715\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27480.5762 - val_loss: 83.6959\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17681.2930 - val_loss: 82.4140\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21191.7324 - val_loss: 82.5766\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13569.1592 - val_loss: 83.4720\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8530.3281 - val_loss: 84.3346\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13691.3613 - val_loss: 83.6648\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21160.4219 - val_loss: 83.4899\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4673.2422 - val_loss: 84.8462\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30500.9238 - val_loss: 85.5635\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26431.3613 - val_loss: 84.5232\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2644.3977 - val_loss: 85.0466\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7191.6299 - val_loss: 84.2816\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14878.9160 - val_loss: 84.9828\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3943.1587 - val_loss: 84.8064\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16847.7852 - val_loss: 84.9109\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2616.3237 - val_loss: 86.2734\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 35914.9219 - val_loss: 86.7591\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27253.4531 - val_loss: 86.1380\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5131.1235 - val_loss: 84.1875\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 62452.5938 - val_loss: 83.5330\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 58700.8750 - val_loss: 84.8473\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23360.2363 - val_loss: 86.5859\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3693.9766 - val_loss: 89.2188\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 85035.7109 - val_loss: 90.5022\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 72314.6719 - val_loss: 90.2431\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33161.1562 - val_loss: 89.1218\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9052.6055 - val_loss: 87.0567\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 62542.1250 - val_loss: 86.4001\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 69204.7500 - val_loss: 87.2910\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 51822.3555 - val_loss: 88.3313\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19332.3750 - val_loss: 89.4425\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11744.0225 - val_loss: 90.5121\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14191.7168 - val_loss: 90.1349\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3656.2295 - val_loss: 90.3574\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7948.5322 - val_loss: 90.2888\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 601.0316 - val_loss: 89.3877\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29169.0723 - val_loss: 89.4420\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24290.5371 - val_loss: 90.0256\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1489.8262 - val_loss: 90.8719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 109346.1250 - val_loss: 110.0506\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 147550.9531 - val_loss: 105.7346\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 66764.4062 - val_loss: 106.5142\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 58314.0117 - val_loss: 110.3191\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 208475.2969 - val_loss: 107.2347\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 74132.5938 - val_loss: 101.7326\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 223629.0000 - val_loss: 100.0141\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 141416.0469 - val_loss: 103.1070\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 92367.4141 - val_loss: 106.2805\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 143432.3906 - val_loss: 104.2293\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45207.4492 - val_loss: 101.9201\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 122365.9844 - val_loss: 102.9886\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 53778.3711 - val_loss: 106.2193\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 196343.1094 - val_loss: 106.8525\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 120794.7500 - val_loss: 104.3534\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39095.1758 - val_loss: 103.3664\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 23567.3750 - val_loss: 104.0257\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24152.8008 - val_loss: 103.1609\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11612.9590 - val_loss: 102.8549\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7647.1333 - val_loss: 102.6429\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7336.5732 - val_loss: 102.7357\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21894.1504 - val_loss: 102.2689\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 47039.7930 - val_loss: 102.0769\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9010.8369 - val_loss: 102.6413\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18529.8652 - val_loss: 103.1552\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34614.7500 - val_loss: 104.1159\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 58685.3477 - val_loss: 103.1019\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11822.0391 - val_loss: 102.6107\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18632.1543 - val_loss: 103.0826\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24711.2402 - val_loss: 102.3591\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35754.2500 - val_loss: 101.7047\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31786.0781 - val_loss: 102.5519\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19561.6055 - val_loss: 101.7361\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24974.7695 - val_loss: 102.3831\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 38869.0234 - val_loss: 102.4058\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19743.2578 - val_loss: 101.4325\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8262.8154 - val_loss: 101.8102\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22733.8867 - val_loss: 101.3125\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 28166.1191 - val_loss: 101.3769\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 31717.3164 - val_loss: 102.6816\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 73777.2734 - val_loss: 101.8668\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21870.4961 - val_loss: 100.8936\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 77222.9375 - val_loss: 101.6067\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31175.2793 - val_loss: 102.8830\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 29081.9629 - val_loss: 102.2317\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11030.8926 - val_loss: 102.8061\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30958.6641 - val_loss: 102.4581\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15530.6846 - val_loss: 102.4660\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16757.1543 - val_loss: 102.3256\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19381.2422 - val_loss: 102.5926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 1 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 57.7800 - val_loss: 38.7170\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47.7402 - val_loss: 29.1224\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 37.9042 - val_loss: 37.6954\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32.6638 - val_loss: 33.0170\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27.6202 - val_loss: 19.6893\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.5426 - val_loss: 12.7351\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20.5773 - val_loss: 17.0647\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.2276 - val_loss: 7.6647\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.2309 - val_loss: 7.9649\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.5134 - val_loss: 3.1358\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.7345 - val_loss: 4.1382\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.1945 - val_loss: 3.2758\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.3860 - val_loss: 3.2250\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.2269 - val_loss: 3.3523\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.6959 - val_loss: 4.3220\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.5364 - val_loss: 4.2952\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.3206 - val_loss: 3.6108\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.1737 - val_loss: 4.1865\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.9154 - val_loss: 2.8963\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.9555 - val_loss: 2.7851\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.1922 - val_loss: 3.3643\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.7951 - val_loss: 3.5323\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5698 - val_loss: 3.1342\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.4127 - val_loss: 3.7309\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.0911 - val_loss: 3.9261\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.5672 - val_loss: 3.2349\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.6393 - val_loss: 3.1375\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2719 - val_loss: 3.6234\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.6874 - val_loss: 5.6890\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.9913 - val_loss: 1.3568\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3143 - val_loss: 3.2321\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5163 - val_loss: 3.2041\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6665 - val_loss: 3.2308\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7309 - val_loss: 3.4179\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.3764 - val_loss: 2.8847\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3306 - val_loss: 3.9104\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.4901 - val_loss: 1.7017\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4642 - val_loss: 5.4694\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4671 - val_loss: 1.8831\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9132 - val_loss: 3.3267\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9844 - val_loss: 2.7028\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9894 - val_loss: 2.4549\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8054 - val_loss: 5.0506\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2226 - val_loss: 1.5944\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6545 - val_loss: 3.6213\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5004 - val_loss: 2.6501\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4742 - val_loss: 2.5832\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4375 - val_loss: 4.1670\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.5028 - val_loss: 1.4440\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3592 - val_loss: 3.5887\n",
      "Iteration number 2 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 121.5051 - val_loss: 98.1408\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 69.8706 - val_loss: 52.8936\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33.6626 - val_loss: 12.6598\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31.6024 - val_loss: 24.3897\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22.6426 - val_loss: 35.0597\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23.0239 - val_loss: 31.8119\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19.5397 - val_loss: 19.4544\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19.0029 - val_loss: 11.9570\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16.2919 - val_loss: 15.2965\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.5507 - val_loss: 13.8847\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.7442 - val_loss: 9.7035\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.1657 - val_loss: 5.1673\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.9794 - val_loss: 4.5045\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.7045 - val_loss: 6.2826\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.4863 - val_loss: 4.3956\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.7229 - val_loss: 5.3275\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5248 - val_loss: 4.8954\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5808 - val_loss: 6.2427\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.4102 - val_loss: 3.2227\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.2143 - val_loss: 6.4173\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9911 - val_loss: 2.5714\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9056 - val_loss: 5.3130\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.5369 - val_loss: 2.9524\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.2636 - val_loss: 3.8850\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0913 - val_loss: 4.8019\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9966 - val_loss: 3.5707\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8496 - val_loss: 2.8522\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.6344 - val_loss: 3.4213\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.5272 - val_loss: 2.3893\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.5236 - val_loss: 4.1486\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5960 - val_loss: 2.4846\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0835 - val_loss: 4.1698\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.2015 - val_loss: 2.3473\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3006 - val_loss: 3.4400\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0146 - val_loss: 2.6533\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0618 - val_loss: 3.7348\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8336 - val_loss: 2.6351\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0808 - val_loss: 3.8175\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.8894 - val_loss: 2.4410\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0216 - val_loss: 2.7733\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.4634 - val_loss: 2.4836\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.1897 - val_loss: 2.4220\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.2449 - val_loss: 2.9221\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.0298 - val_loss: 2.6046\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2752 - val_loss: 3.3456\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4389 - val_loss: 2.5416\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6579 - val_loss: 2.7945\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0188 - val_loss: 2.1996\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.5487 - val_loss: 2.1385\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4989 - val_loss: 2.1491\n",
      "Iteration number 3 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 77.7590 - val_loss: 62.2216\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 52.8757 - val_loss: 32.5403\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 42.3793 - val_loss: 32.4866\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 28.4981 - val_loss: 25.9237\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24.3893 - val_loss: 10.9301\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21.3096 - val_loss: 12.3044\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18.8514 - val_loss: 5.8748\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.7729 - val_loss: 8.5945\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.1014 - val_loss: 5.2465\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 14.6711 - val_loss: 6.9480\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.4464 - val_loss: 6.2344\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.8733 - val_loss: 6.2258\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.0257 - val_loss: 5.4233\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.5675 - val_loss: 5.6769\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.0420 - val_loss: 4.7492\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.7571 - val_loss: 6.6264\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.5149 - val_loss: 4.7006\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.3851 - val_loss: 5.2099\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10.0838 - val_loss: 6.0328\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.7229 - val_loss: 4.7526\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2819 - val_loss: 5.6426\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.8099 - val_loss: 4.7364\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5478 - val_loss: 5.5266\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7376 - val_loss: 4.2110\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.1953 - val_loss: 4.5155\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.0600 - val_loss: 6.6158\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3006 - val_loss: 2.9273\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2837 - val_loss: 7.6977\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.7427 - val_loss: 2.9116\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.9320 - val_loss: 5.4538\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.2993 - val_loss: 3.6150\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.6132 - val_loss: 4.3487\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.8866 - val_loss: 3.5119\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.6111 - val_loss: 4.7978\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7177 - val_loss: 2.7650\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6389 - val_loss: 6.5308\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.0376 - val_loss: 2.7883\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4135 - val_loss: 4.9035\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9966 - val_loss: 2.6206\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1756 - val_loss: 5.0159\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3034 - val_loss: 3.3395\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.1236 - val_loss: 3.5082\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.2660 - val_loss: 4.1668\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.9947 - val_loss: 3.8021\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1060 - val_loss: 2.7649\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8246 - val_loss: 3.4127\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.3429 - val_loss: 3.0853\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.1831 - val_loss: 4.1254\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4432 - val_loss: 2.3130\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.9195 - val_loss: 3.8350\n",
      "Iteration number 4 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 21822.1758 - val_loss: 66.6627\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 278095.0000 - val_loss: 61.0409\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 243388.0469 - val_loss: 73.0938\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19088.5039 - val_loss: 80.1178\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 98212.2812 - val_loss: 84.6682\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 85591.3672 - val_loss: 83.2650\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 29839.6113 - val_loss: 80.4944\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17158.7500 - val_loss: 80.3800\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 32677.5566 - val_loss: 80.9825\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1766.8317 - val_loss: 84.6360\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 68457.9844 - val_loss: 86.2526\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 73539.0625 - val_loss: 86.0142\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 52387.8711 - val_loss: 82.7192\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 37307.6680 - val_loss: 81.3724\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36557.4258 - val_loss: 83.0757\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2394.5566 - val_loss: 85.7901\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 50162.1758 - val_loss: 86.2957\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48637.4141 - val_loss: 84.7273\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10677.4463 - val_loss: 82.4272\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 58390.6992 - val_loss: 81.3686\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 36485.7812 - val_loss: 83.0803\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8574.5234 - val_loss: 85.0163\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13072.9834 - val_loss: 84.6329\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 9898.4336 - val_loss: 84.6160\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 272.7167 - val_loss: 83.8581\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30285.3262 - val_loss: 83.6725\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16258.2109 - val_loss: 86.4272\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 44953.3242 - val_loss: 87.3630\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 44230.2773 - val_loss: 86.2764\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15095.3945 - val_loss: 84.2313\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 30413.2441 - val_loss: 84.0591\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28699.4238 - val_loss: 85.4346\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4390.5928 - val_loss: 87.8748\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 60047.9141 - val_loss: 88.7170\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 52304.8633 - val_loss: 87.9274\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25767.5781 - val_loss: 85.9350\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28364.9629 - val_loss: 84.9669\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26296.1504 - val_loss: 85.5686\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3798.3250 - val_loss: 86.6740\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1733.9213 - val_loss: 85.8244\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32979.0469 - val_loss: 85.3226\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 27823.3789 - val_loss: 87.6184\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28147.6875 - val_loss: 88.1971\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 25543.9941 - val_loss: 86.9976\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15363.8799 - val_loss: 86.6796\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5257.8311 - val_loss: 88.6821\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29687.6309 - val_loss: 88.9374\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34145.5352 - val_loss: 88.5834\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19429.6934 - val_loss: 86.8757\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20260.4941 - val_loss: 86.6655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 5 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 49257.1445 - val_loss: 75.1683\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 174727.6406 - val_loss: 84.2641\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 85958.7344 - val_loss: 80.4554\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 55797.0273 - val_loss: 72.5885\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 130791.7500 - val_loss: 71.1461\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 131899.7500 - val_loss: 72.8724\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 88555.0078 - val_loss: 78.7432\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15554.0703 - val_loss: 81.7803\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13656.5234 - val_loss: 80.5230\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15254.9111 - val_loss: 81.3196\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1163.7451 - val_loss: 82.4682\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12201.6885 - val_loss: 83.1782\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26822.4883 - val_loss: 82.6898\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6725.0737 - val_loss: 82.2745\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1490.1140 - val_loss: 81.1490\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27325.3555 - val_loss: 81.8193\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5645.5376 - val_loss: 82.2886\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3249.4192 - val_loss: 82.5550\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1530.6379 - val_loss: 81.4564\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21088.8379 - val_loss: 82.1136\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5914.4551 - val_loss: 83.3900\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40788.2617 - val_loss: 84.5884\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23223.3965 - val_loss: 82.2919\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 38253.2383 - val_loss: 81.2315\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33000.8828 - val_loss: 82.8365\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4512.6631 - val_loss: 83.2635\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9473.4541 - val_loss: 83.4883\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15611.6582 - val_loss: 84.2471\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11768.6895 - val_loss: 82.7854\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22944.1113 - val_loss: 83.0757\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7327.7222 - val_loss: 84.0421\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4092.2727 - val_loss: 84.6444\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17391.3359 - val_loss: 83.9798\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4412.9126 - val_loss: 84.3902\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13334.8135 - val_loss: 84.1739\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2182.9956 - val_loss: 81.6623\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 55193.2422 - val_loss: 81.6113\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 49904.8281 - val_loss: 82.9430\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8279.4443 - val_loss: 84.7449\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37103.9375 - val_loss: 86.6305\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 41189.8516 - val_loss: 85.9113\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14032.0547 - val_loss: 84.4070\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12615.0176 - val_loss: 84.4464\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14344.5957 - val_loss: 85.5959\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12077.3799 - val_loss: 85.4175\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 682.0203 - val_loss: 84.0292\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 42138.1836 - val_loss: 83.6379\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32827.3164 - val_loss: 85.0254\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10237.7568 - val_loss: 85.8917\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2236.0781 - val_loss: 85.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 6 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 70.4585 - val_loss: 52.4626\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 59.8631 - val_loss: 31.5861\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 49.1708 - val_loss: 41.0355\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 40.6009 - val_loss: 43.9696\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35.5320 - val_loss: 34.5629\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29.5127 - val_loss: 18.7960\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27.6072 - val_loss: 16.7483\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.6715 - val_loss: 20.5479\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 21.0080 - val_loss: 10.8983\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 19.0879 - val_loss: 9.2620\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 17.7670 - val_loss: 4.8063\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15.5454 - val_loss: 3.8681\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.3197 - val_loss: 3.5730\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.6006 - val_loss: 4.4237\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.3417 - val_loss: 3.2173\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.3563 - val_loss: 5.5764\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.1957 - val_loss: 2.6943\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.7386 - val_loss: 5.2321\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.9650 - val_loss: 2.5592\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.7345 - val_loss: 3.6024\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.3038 - val_loss: 6.9486\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.6672 - val_loss: 2.7378\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.2256 - val_loss: 5.4476\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.9362 - val_loss: 2.4694\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.1893 - val_loss: 4.4058\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.7607 - val_loss: 4.6274\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.9782 - val_loss: 3.8548\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.9902 - val_loss: 5.3803\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.9153 - val_loss: 3.5704\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.5732 - val_loss: 4.7788\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5802 - val_loss: 4.3089\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.4157 - val_loss: 2.9695\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.9720 - val_loss: 6.7014\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8376 - val_loss: 2.4598\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.6624 - val_loss: 6.6105\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8076 - val_loss: 5.0773\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9316 - val_loss: 3.9931\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3465 - val_loss: 3.9850\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9208 - val_loss: 4.8455\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.1502 - val_loss: 3.8232\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5770 - val_loss: 3.5053\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4641 - val_loss: 4.4907\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.5535 - val_loss: 4.8769\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.7550 - val_loss: 2.6664\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.7338 - val_loss: 4.9406\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.0641 - val_loss: 4.7119\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4256 - val_loss: 2.7555\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2633 - val_loss: 4.7368\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4631 - val_loss: 3.9255\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.1849 - val_loss: 3.4388\n",
      "Iteration number 7 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 43704.4375 - val_loss: 99.1301\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 220012.8594 - val_loss: 99.6690\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 133430.6562 - val_loss: 102.6879\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 84371.6484 - val_loss: 110.8555\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 46256.4219 - val_loss: 112.6356\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 64769.5273 - val_loss: 111.5170\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37436.0469 - val_loss: 107.0152\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32868.8789 - val_loss: 106.1037\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31703.7637 - val_loss: 107.0353\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1259.1360 - val_loss: 107.2674\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9532.0947 - val_loss: 108.5089\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14963.8486 - val_loss: 107.2787\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8444.9297 - val_loss: 108.0279\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20027.4336 - val_loss: 107.6932\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 737.0588 - val_loss: 108.1886\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 27881.3945 - val_loss: 108.1619\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16709.0996 - val_loss: 104.6144\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 60539.0195 - val_loss: 103.4673\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 54766.1133 - val_loss: 105.8723\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10115.4873 - val_loss: 109.2828\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 73259.6875 - val_loss: 110.2494\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 65283.1836 - val_loss: 108.2023\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20526.3184 - val_loss: 105.4549\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30624.6289 - val_loss: 103.6722\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 30094.3887 - val_loss: 104.9450\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4468.0742 - val_loss: 104.8408\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12634.7305 - val_loss: 104.7301\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8520.0107 - val_loss: 105.2020\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1680.6644 - val_loss: 104.4063\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 38242.4961 - val_loss: 102.8238\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 31916.7266 - val_loss: 104.6564\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16577.8574 - val_loss: 105.2571\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 12480.1367 - val_loss: 103.3047\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 25847.0176 - val_loss: 103.3332\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15101.6064 - val_loss: 104.2155\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4745.9741 - val_loss: 104.2926\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1090.4459 - val_loss: 104.5574\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 31230.4570 - val_loss: 105.6407\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18603.3262 - val_loss: 104.1961\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10664.7783 - val_loss: 103.4821\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8962.1270 - val_loss: 104.5885\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12737.6465 - val_loss: 104.1285\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4153.5698 - val_loss: 102.7772\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31693.1719 - val_loss: 102.3420\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24643.5020 - val_loss: 103.7663\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11454.1133 - val_loss: 103.9680\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6854.0239 - val_loss: 102.4371\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21573.7949 - val_loss: 102.7130\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13294.9170 - val_loss: 103.6464\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16605.9375 - val_loss: 104.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 8 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 87.3541 - val_loss: 72.9971\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 66.2597 - val_loss: 51.4525\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 50.9399 - val_loss: 54.0616\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 44.4303 - val_loss: 43.8415\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35.9074 - val_loss: 23.3005\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 32.1542 - val_loss: 22.6162\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28.0417 - val_loss: 21.8710\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25.1126 - val_loss: 7.8102\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24.0170 - val_loss: 11.4138\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22.6644 - val_loss: 7.3767\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21.0039 - val_loss: 5.7574\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20.0011 - val_loss: 6.5669\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.6573 - val_loss: 3.9642\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.0647 - val_loss: 5.4969\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.9454 - val_loss: 4.1710\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.7678 - val_loss: 5.0573\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.2534 - val_loss: 4.9751\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.5106 - val_loss: 4.4392\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.2483 - val_loss: 4.5322\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.2924 - val_loss: 5.7460\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.3490 - val_loss: 5.5891\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.6679 - val_loss: 4.1939\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.5113 - val_loss: 3.9765\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.5851 - val_loss: 3.5263\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.6658 - val_loss: 5.0527\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.8604 - val_loss: 4.3459\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.6830 - val_loss: 3.6877\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.3302 - val_loss: 3.0715\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.2126 - val_loss: 3.9627\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.9290 - val_loss: 3.3355\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.9314 - val_loss: 3.5394\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4699 - val_loss: 2.9690\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.2564 - val_loss: 4.6300\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.4297 - val_loss: 2.7556\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7864 - val_loss: 2.9279\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.8689 - val_loss: 2.5131\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.5521 - val_loss: 2.4654\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7299 - val_loss: 2.7864\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.9957 - val_loss: 3.3180\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.3431 - val_loss: 2.3160\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.1101 - val_loss: 2.5390\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.2042 - val_loss: 2.4922\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8314 - val_loss: 2.6316\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6041 - val_loss: 3.4282\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7687 - val_loss: 3.6547\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.9132 - val_loss: 2.6088\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2923 - val_loss: 3.8149\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7974 - val_loss: 2.5594\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.4599 - val_loss: 2.4905\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7840 - val_loss: 2.4944\n",
      "Iteration number 9 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 194146.2031 - val_loss: 91.2233\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37392.7891 - val_loss: 105.5105\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 78061.4297 - val_loss: 109.6370\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 99360.8750 - val_loss: 107.0429\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 39949.4023 - val_loss: 102.7615\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 48901.6094 - val_loss: 100.6551\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37036.6914 - val_loss: 102.3920\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8329.1133 - val_loss: 103.0695\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3161.9983 - val_loss: 104.2536\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 34915.9414 - val_loss: 104.0986\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20275.1309 - val_loss: 100.5845\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 54855.2734 - val_loss: 100.1002\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 42309.0391 - val_loss: 102.6091\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 764.4224 - val_loss: 102.1079\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8228.2998 - val_loss: 103.7323\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 42674.8477 - val_loss: 104.3194\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24975.6543 - val_loss: 100.4705\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 44221.5312 - val_loss: 99.6667\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46403.6484 - val_loss: 100.4861\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15732.7930 - val_loss: 103.3634\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24073.8359 - val_loss: 103.7193\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 34591.6484 - val_loss: 103.3048\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23806.3262 - val_loss: 99.3253\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 77842.7734 - val_loss: 97.8311\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 74366.2266 - val_loss: 98.7125\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 44996.5742 - val_loss: 101.1795\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16431.8379 - val_loss: 102.4123\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13777.9795 - val_loss: 101.5985\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 979.8381 - val_loss: 101.7888\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5672.4331 - val_loss: 101.4346\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 31771.2207 - val_loss: 100.2361\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27669.8848 - val_loss: 102.3042\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 27697.6152 - val_loss: 102.6141\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21277.8848 - val_loss: 101.2202\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20485.4512 - val_loss: 100.7854\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10477.2188 - val_loss: 102.0625\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12798.6953 - val_loss: 101.9669\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10226.9053 - val_loss: 101.5012\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20448.4336 - val_loss: 100.5384\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 18521.5430 - val_loss: 101.6593\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10130.0703 - val_loss: 101.4895\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 647.6997 - val_loss: 100.1820\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26081.6992 - val_loss: 100.4125\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19970.2617 - val_loss: 101.5379\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20045.1641 - val_loss: 102.0066\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9888.8154 - val_loss: 100.1273\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 49885.9023 - val_loss: 99.2055\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39684.8438 - val_loss: 100.5360\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5745.2241 - val_loss: 101.3284\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4448.2510 - val_loss: 101.4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 10 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 89954.0078 - val_loss: 91.2521\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 99238.4531 - val_loss: 99.0914\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 201925.3125 - val_loss: 100.8140\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 85286.3672 - val_loss: 96.2172\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 64518.0117 - val_loss: 89.8943\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 223559.5469 - val_loss: 90.6342\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 140403.8281 - val_loss: 94.7843\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 57481.1719 - val_loss: 98.8798\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 100805.9766 - val_loss: 97.8058\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37408.0117 - val_loss: 94.1452\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 67701.1641 - val_loss: 94.0391\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 67356.7969 - val_loss: 95.2350\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 47401.3438 - val_loss: 98.6151\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 82666.4375 - val_loss: 98.1280\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 34458.4883 - val_loss: 95.9142\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 56328.3281 - val_loss: 95.6354\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 51091.1289 - val_loss: 97.0188\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37198.7695 - val_loss: 99.0919\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 84553.9531 - val_loss: 97.8873\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 37153.9805 - val_loss: 95.1163\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 100535.7891 - val_loss: 94.6236\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 68811.6875 - val_loss: 96.0318\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 18022.1211 - val_loss: 96.8512\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25667.2559 - val_loss: 95.7576\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 41196.1797 - val_loss: 97.2602\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 48640.0898 - val_loss: 97.7349\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23794.7031 - val_loss: 96.3353\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 52417.3633 - val_loss: 96.9998\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15565.4932 - val_loss: 99.2835\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 90627.4219 - val_loss: 99.4782\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 72094.8750 - val_loss: 98.0480\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 38005.1328 - val_loss: 96.4539\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 70629.4297 - val_loss: 97.0574\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21435.8301 - val_loss: 99.2424\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 78648.5703 - val_loss: 99.7715\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 73722.3984 - val_loss: 98.6897\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15869.5391 - val_loss: 96.4520\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 89753.6250 - val_loss: 96.1995\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 83566.0859 - val_loss: 96.8861\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15400.4375 - val_loss: 99.2814\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 77460.9766 - val_loss: 100.2577\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 77339.0234 - val_loss: 98.9139\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 20880.8457 - val_loss: 97.0213\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 40489.1680 - val_loss: 97.4451\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1371.6830 - val_loss: 97.0177\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21183.4121 - val_loss: 97.3123\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15556.4092 - val_loss: 97.5631\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1886.7164 - val_loss: 97.6034\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 30989.3086 - val_loss: 97.9399\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7025.7095 - val_loss: 97.6318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 11 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 69382.6797 - val_loss: 115.0790\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 606383.5625 - val_loss: 114.7181\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 439804.6875 - val_loss: 102.6088\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 111621.2891 - val_loss: 90.8820\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 267555.8750 - val_loss: 88.5235\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 230514.0000 - val_loss: 91.4206\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 82716.3828 - val_loss: 96.3592\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14539.1484 - val_loss: 95.2976\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 49433.7656 - val_loss: 96.6296\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4999.0210 - val_loss: 96.0011\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 40207.2656 - val_loss: 96.8227\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5155.4731 - val_loss: 97.6214\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14337.0967 - val_loss: 96.3619\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 50128.8164 - val_loss: 96.9181\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6374.0264 - val_loss: 97.2677\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 23100.2949 - val_loss: 98.2296\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 49986.2578 - val_loss: 98.4613\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3094.9084 - val_loss: 98.1470\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13498.7266 - val_loss: 97.5598\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11093.8408 - val_loss: 98.5925\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25429.2793 - val_loss: 96.6683\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 61704.2812 - val_loss: 97.5327\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10719.7734 - val_loss: 98.8768\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18933.4844 - val_loss: 97.5069\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 65030.8438 - val_loss: 97.6344\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 29782.9961 - val_loss: 99.3203\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 36990.1133 - val_loss: 97.9376\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23704.3984 - val_loss: 98.0210\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10543.5967 - val_loss: 101.2127\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 119640.4375 - val_loss: 101.0668\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 95706.9531 - val_loss: 99.5676\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 54881.2500 - val_loss: 96.9135\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 76294.5078 - val_loss: 98.1825\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12300.8418 - val_loss: 98.9801\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23939.7070 - val_loss: 99.0696\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12855.0967 - val_loss: 100.0510\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14493.3154 - val_loss: 98.7366\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 75727.9844 - val_loss: 98.2645\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 51535.2617 - val_loss: 101.5468\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 110216.2344 - val_loss: 102.6536\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 76458.8281 - val_loss: 101.0045\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19351.8184 - val_loss: 99.8118\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5967.8481 - val_loss: 100.3505\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13746.6309 - val_loss: 99.7553\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13342.8574 - val_loss: 100.1054\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7535.4312 - val_loss: 97.9607\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 80911.8125 - val_loss: 98.0222\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 36020.7930 - val_loss: 99.7872\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 26950.5996 - val_loss: 100.5058\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25204.7930 - val_loss: 98.9528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 12 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 92802.8828 - val_loss: 74.1958\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 79310.2578 - val_loss: 70.2526\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 58440.7109 - val_loss: 80.2672\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 231945.5312 - val_loss: 84.1281\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 171110.0156 - val_loss: 78.5308\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35661.2188 - val_loss: 77.9873\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19114.3809 - val_loss: 79.6024\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8762.4482 - val_loss: 80.5365\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 16172.6436 - val_loss: 81.2652\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16745.9844 - val_loss: 82.7680\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20170.5469 - val_loss: 81.9175\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15817.8916 - val_loss: 83.4123\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 30248.7734 - val_loss: 80.8918\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 52323.8164 - val_loss: 81.3577\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7677.7954 - val_loss: 82.6473\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35935.1875 - val_loss: 85.0846\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 40099.7070 - val_loss: 83.6986\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 66056.7734 - val_loss: 82.9283\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35682.3555 - val_loss: 85.2492\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5878.4404 - val_loss: 84.9858\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21627.7324 - val_loss: 86.1649\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17887.1211 - val_loss: 85.1048\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13143.5889 - val_loss: 86.1922\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 39629.9531 - val_loss: 83.6556\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 45731.8086 - val_loss: 83.8715\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 30116.0820 - val_loss: 86.1606\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 35117.4102 - val_loss: 84.9647\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 29380.2500 - val_loss: 85.8719\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 15584.5439 - val_loss: 84.8599\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 70919.7422 - val_loss: 84.7225\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 46809.1875 - val_loss: 89.8191\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 197815.2031 - val_loss: 91.9163\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 170675.6719 - val_loss: 90.6500\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 67999.2734 - val_loss: 88.0335\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 62281.5977 - val_loss: 86.8159\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 71681.0312 - val_loss: 87.8457\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10962.0225 - val_loss: 89.9913\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 52373.9297 - val_loss: 90.8108\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 53263.1133 - val_loss: 90.0997\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 30943.3555 - val_loss: 89.0669\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 39850.9688 - val_loss: 89.9086\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18836.3496 - val_loss: 92.5290\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 90397.4766 - val_loss: 92.4925\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 84265.0000 - val_loss: 91.0737\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33904.8867 - val_loss: 89.5997\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 40763.3984 - val_loss: 91.6551\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 24331.0762 - val_loss: 91.7257\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20184.0234 - val_loss: 91.1431\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 29601.2344 - val_loss: 92.3118\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22858.5391 - val_loss: 92.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 13 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 57503.3672 - val_loss: 112.4765\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 140512.6875 - val_loss: 118.7352\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 75079.7891 - val_loss: 111.9488\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 48358.2812 - val_loss: 107.3479\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18020.6621 - val_loss: 109.9244\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 61944.6914 - val_loss: 113.4867\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 57475.7969 - val_loss: 110.0753\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8094.8267 - val_loss: 110.0736\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6239.7852 - val_loss: 109.4091\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 28057.2988 - val_loss: 108.8209\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 15113.0918 - val_loss: 111.5793\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 45503.3359 - val_loss: 112.0193\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 38122.7188 - val_loss: 109.2179\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7562.8066 - val_loss: 109.7677\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8200.1348 - val_loss: 109.4004\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2836.8308 - val_loss: 109.1595\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7920.9434 - val_loss: 110.0022\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32287.6582 - val_loss: 110.9653\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24986.0645 - val_loss: 107.3160\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36978.7891 - val_loss: 107.2400\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37840.6953 - val_loss: 108.8817\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10026.5186 - val_loss: 109.5256\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20847.9590 - val_loss: 108.5125\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3520.1304 - val_loss: 111.0477\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39781.0234 - val_loss: 111.5789\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40927.2852 - val_loss: 110.7424\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14593.7744 - val_loss: 108.1394\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36874.8633 - val_loss: 107.1504\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35710.8398 - val_loss: 107.9029\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21354.9004 - val_loss: 110.8227\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 54767.1758 - val_loss: 111.7659\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 47657.0234 - val_loss: 110.6987\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25837.1875 - val_loss: 108.0095\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 25404.7031 - val_loss: 107.4397\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27756.7109 - val_loss: 108.0257\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 809.8970 - val_loss: 109.9070\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29026.0918 - val_loss: 110.4357\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31221.4414 - val_loss: 108.9121\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5256.5688 - val_loss: 106.7544\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 40512.9805 - val_loss: 106.5961\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34218.7188 - val_loss: 107.5695\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 317.1206 - val_loss: 109.2530\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20405.2637 - val_loss: 109.5175\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29354.2637 - val_loss: 109.0191\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8433.9502 - val_loss: 107.2303\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37155.2812 - val_loss: 106.4088\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33347.9414 - val_loss: 107.4062\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1341.8766 - val_loss: 109.0574\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 46505.7734 - val_loss: 110.2940\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 38791.7852 - val_loss: 108.3601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 14 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 181269.4062 - val_loss: 93.4778\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 38251.5156 - val_loss: 106.1953\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 198152.3906 - val_loss: 108.3581\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 167681.2812 - val_loss: 105.2836\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 92015.1016 - val_loss: 101.0465\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 33380.8438 - val_loss: 95.8060\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 101150.6406 - val_loss: 94.3402\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 98898.3438 - val_loss: 95.9631\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47243.5391 - val_loss: 98.2394\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14596.7314 - val_loss: 99.1021\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12108.1680 - val_loss: 97.7024\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 42535.4102 - val_loss: 97.5412\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20937.9844 - val_loss: 98.6468\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 15691.8057 - val_loss: 99.7843\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22384.0801 - val_loss: 99.4257\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2316.1912 - val_loss: 97.4796\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 60094.0977 - val_loss: 97.0265\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 55923.1445 - val_loss: 98.1014\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33130.2383 - val_loss: 100.4285\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37927.3047 - val_loss: 101.0056\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34967.0078 - val_loss: 99.7806\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14906.5029 - val_loss: 99.4324\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7394.0576 - val_loss: 99.8743\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4613.1665 - val_loss: 100.0491\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17461.1562 - val_loss: 100.2434\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6938.7983 - val_loss: 98.8122\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 26859.6914 - val_loss: 98.9335\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23641.7656 - val_loss: 99.5317\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5925.1367 - val_loss: 99.9793\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3449.3630 - val_loss: 100.3796\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14296.5908 - val_loss: 100.0711\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2281.6531 - val_loss: 98.6124\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 45335.5742 - val_loss: 98.5025\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36699.1250 - val_loss: 99.7607\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11065.6904 - val_loss: 100.3337\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3301.9729 - val_loss: 100.3016\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4274.8398 - val_loss: 99.5992\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21524.6641 - val_loss: 99.6693\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12916.7373 - val_loss: 101.3061\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28429.3555 - val_loss: 101.2613\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26093.1289 - val_loss: 100.4036\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8382.4355 - val_loss: 100.1997\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6404.1729 - val_loss: 100.2953\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11638.8975 - val_loss: 100.0162\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6087.7373 - val_loss: 100.8247\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 26468.0176 - val_loss: 101.1883\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16400.5996 - val_loss: 100.4519\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4093.1636 - val_loss: 100.3102\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2131.9023 - val_loss: 101.3180\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 34186.7266 - val_loss: 101.4847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 15 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 165662.0312 - val_loss: 89.5539\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 22987.8359 - val_loss: 82.0533\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 218758.7031 - val_loss: 81.3942\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 160197.0781 - val_loss: 86.5392\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20985.9648 - val_loss: 90.4471\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 110703.3828 - val_loss: 89.6232\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24847.2344 - val_loss: 85.9972\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 143979.3906 - val_loss: 84.5754\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 103860.0156 - val_loss: 88.2952\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 34326.7109 - val_loss: 91.6093\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 107706.2422 - val_loss: 91.0607\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 59671.9336 - val_loss: 88.3576\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 79010.8281 - val_loss: 87.4041\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 57116.7773 - val_loss: 89.7572\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 28860.3359 - val_loss: 89.9716\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10972.4541 - val_loss: 89.7152\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6050.2070 - val_loss: 90.0402\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6600.6060 - val_loss: 89.7196\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7075.7979 - val_loss: 91.3796\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 54415.6094 - val_loss: 90.9605\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12434.9541 - val_loss: 90.2189\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19800.0918 - val_loss: 90.9441\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10186.9355 - val_loss: 91.0482\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14701.0215 - val_loss: 90.1751\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 20939.1094 - val_loss: 90.9361\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32271.2012 - val_loss: 91.2210\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12622.9814 - val_loss: 90.3498\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14683.0605 - val_loss: 90.7522\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10520.0010 - val_loss: 89.1666\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 79710.2031 - val_loss: 89.1097\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 63142.1523 - val_loss: 91.4904\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 60691.2773 - val_loss: 92.2406\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19749.7891 - val_loss: 91.2792\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8792.4971 - val_loss: 91.7651\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11254.5576 - val_loss: 90.6971\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23642.8672 - val_loss: 91.5519\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36785.9492 - val_loss: 91.3722\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10445.9102 - val_loss: 90.7818\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15492.1230 - val_loss: 91.6000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 49757.4805 - val_loss: 92.1979\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21861.9883 - val_loss: 91.4414\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18453.2441 - val_loss: 92.6958\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24300.5508 - val_loss: 92.4122\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4360.3535 - val_loss: 92.3767\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25850.8906 - val_loss: 92.7766\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21672.9609 - val_loss: 93.4065\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3130.8105 - val_loss: 92.9912\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12533.4727 - val_loss: 94.1524\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 37842.4492 - val_loss: 94.0706\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2017.2957 - val_loss: 93.2539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 16 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 56.6403 - val_loss: 38.5349\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 47.0222 - val_loss: 29.0525\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 36.1224 - val_loss: 39.8124\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 31.3014 - val_loss: 37.2615\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26.4646 - val_loss: 21.7475\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24.3540 - val_loss: 16.6117\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 19.9016 - val_loss: 17.6374\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 17.4000 - val_loss: 8.5512\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 14.3573 - val_loss: 3.8554\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14.4430 - val_loss: 2.1764\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 13.4738 - val_loss: 6.8646\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.5602 - val_loss: 2.2738\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11.8602 - val_loss: 4.1685\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 10.9464 - val_loss: 1.7787\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.9484 - val_loss: 2.0863\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2016 - val_loss: 2.3784\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 10.0979 - val_loss: 2.4877\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.5704 - val_loss: 2.0369\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.3276 - val_loss: 2.2946\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.8293 - val_loss: 4.3706\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.0359 - val_loss: 2.2407\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.5671 - val_loss: 3.7208\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.4346 - val_loss: 2.2124\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.0771 - val_loss: 2.3772\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.3163 - val_loss: 3.7769\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.0393 - val_loss: 2.4280\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2948 - val_loss: 2.4721\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.8190 - val_loss: 2.2260\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3791 - val_loss: 4.2531\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2290 - val_loss: 2.3133\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.3656 - val_loss: 3.5666\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.2757 - val_loss: 2.0080\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0940 - val_loss: 2.8312\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.6766 - val_loss: 2.4075\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7081 - val_loss: 2.3537\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9647 - val_loss: 2.9894\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9905 - val_loss: 2.3907\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9183 - val_loss: 2.1651\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.9344 - val_loss: 2.2981\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2832 - val_loss: 3.2092\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.1289 - val_loss: 1.7615\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.1213 - val_loss: 3.5796\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.9175 - val_loss: 1.8194\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6987 - val_loss: 2.4773\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5538 - val_loss: 2.6456\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.4264 - val_loss: 3.1707\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.5033 - val_loss: 2.3314\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5923 - val_loss: 2.3442\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0366 - val_loss: 2.3033\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.0193 - val_loss: 1.7270\n",
      "Iteration number 17 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 82.9693 - val_loss: 67.7449\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 37.8758 - val_loss: 28.5071\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31.3375 - val_loss: 14.9678\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 25.4427 - val_loss: 29.4740\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20.6078 - val_loss: 33.2744\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18.6038 - val_loss: 23.8983\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16.2140 - val_loss: 14.3847\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14.6174 - val_loss: 19.2649\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 13.6607 - val_loss: 15.8071\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.9025 - val_loss: 8.7374\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.9471 - val_loss: 10.9226\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.8633 - val_loss: 3.2552\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.7188 - val_loss: 6.6629\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.8860 - val_loss: 2.0467\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.0069 - val_loss: 3.6591\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.7234 - val_loss: 2.6772\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.6325 - val_loss: 2.7850\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1619 - val_loss: 2.9718\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.7056 - val_loss: 2.1532\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.1294 - val_loss: 4.1967\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.1159 - val_loss: 2.1347\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6398 - val_loss: 3.3226\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0681 - val_loss: 2.0680\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7365 - val_loss: 2.8831\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6002 - val_loss: 2.4211\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0604 - val_loss: 2.4258\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1470 - val_loss: 2.2988\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8722 - val_loss: 2.7458\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2527 - val_loss: 2.4808\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1295 - val_loss: 2.2608\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.9340 - val_loss: 2.6272\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.8846 - val_loss: 2.7510\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5321 - val_loss: 2.8783\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.6769 - val_loss: 2.6133\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4484 - val_loss: 3.4073\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.7520 - val_loss: 2.6732\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9297 - val_loss: 2.5074\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5704 - val_loss: 2.6941\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4040 - val_loss: 2.3325\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.9185 - val_loss: 2.6529\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1933 - val_loss: 3.0217\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.0493 - val_loss: 3.0031\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.2192 - val_loss: 2.1616\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4867 - val_loss: 2.4190\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4442 - val_loss: 2.3656\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4609 - val_loss: 2.2206\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.3691 - val_loss: 2.6509\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.5911 - val_loss: 2.9378\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.5055 - val_loss: 2.5908\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8459 - val_loss: 2.5808\n",
      "Iteration number 18 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 80945.3906 - val_loss: 101.9984\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 173535.4531 - val_loss: 104.8891\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 140482.6719 - val_loss: 98.0541\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21123.2344 - val_loss: 91.5310\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 150528.4531 - val_loss: 88.8923\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 123232.6328 - val_loss: 92.1886\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 73168.3672 - val_loss: 96.5290\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19275.8594 - val_loss: 97.2834\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 30485.6875 - val_loss: 96.7745\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4416.9946 - val_loss: 96.9043\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15617.7910 - val_loss: 96.8475\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 12403.3398 - val_loss: 96.7258\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11298.9805 - val_loss: 96.9636\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26036.1504 - val_loss: 96.2422\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9207.9678 - val_loss: 97.9465\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25606.8203 - val_loss: 98.1694\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 29974.0371 - val_loss: 97.3326\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3612.6045 - val_loss: 95.4945\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 76649.2578 - val_loss: 94.8108\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 65443.0859 - val_loss: 96.7023\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 7257.9390 - val_loss: 98.6983\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 57851.3555 - val_loss: 99.7375\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 56846.9023 - val_loss: 99.3027\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36678.6562 - val_loss: 97.6912\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33720.3242 - val_loss: 96.9171\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29686.8066 - val_loss: 98.4038\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 23959.5488 - val_loss: 98.6678\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18994.0918 - val_loss: 97.6361\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13240.1230 - val_loss: 97.9116\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3518.9177 - val_loss: 98.6448\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 30817.5156 - val_loss: 98.9754\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22063.9648 - val_loss: 98.1584\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19856.7168 - val_loss: 97.6658\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1232.0435 - val_loss: 98.4970\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36310.3594 - val_loss: 99.4354\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40581.1758 - val_loss: 99.2964\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14368.2686 - val_loss: 98.0654\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 30356.4570 - val_loss: 97.2955\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32200.4961 - val_loss: 97.6242\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9472.6025 - val_loss: 98.5715\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 27442.0898 - val_loss: 99.1873\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 25867.0332 - val_loss: 98.6902\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5370.4956 - val_loss: 98.3867\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 3560.1768 - val_loss: 98.3335\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2896.8704 - val_loss: 98.8859\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 21885.7637 - val_loss: 98.8931\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6840.7715 - val_loss: 98.2088\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19692.6660 - val_loss: 97.8976\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18404.4453 - val_loss: 98.6400\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8894.8359 - val_loss: 98.5118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 19 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 132811.0469 - val_loss: 104.2077\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 12482.8828 - val_loss: 116.7451\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 143918.1250 - val_loss: 116.2581\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 136116.2031 - val_loss: 113.3111\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 90635.6094 - val_loss: 107.4156\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 34679.5156 - val_loss: 106.0415\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 22227.7559 - val_loss: 107.3698\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1941.7555 - val_loss: 107.0395\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3081.8630 - val_loss: 108.6687\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 30979.2012 - val_loss: 107.7766\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14869.6104 - val_loss: 105.1557\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 61133.1914 - val_loss: 104.2278\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 31660.6484 - val_loss: 106.5773\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8869.8535 - val_loss: 107.3134\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13700.0420 - val_loss: 105.5421\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 28692.9082 - val_loss: 105.6943\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14910.1592 - val_loss: 108.1849\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 50168.1406 - val_loss: 108.4163\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34953.8242 - val_loss: 107.3354\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 22297.9590 - val_loss: 103.2650\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 96161.6016 - val_loss: 101.3935\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 83235.7266 - val_loss: 102.5829\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 55676.1719 - val_loss: 105.1734\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3588.2703 - val_loss: 105.7246\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9916.7656 - val_loss: 105.2208\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11905.5947 - val_loss: 105.0609\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 899.2670 - val_loss: 104.8108\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17577.9844 - val_loss: 104.8017\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6454.1367 - val_loss: 106.2462\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 39720.0117 - val_loss: 106.7012\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 34892.9414 - val_loss: 105.0170\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14593.6533 - val_loss: 104.5718\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4873.1724 - val_loss: 105.7140\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17840.3379 - val_loss: 105.5898\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15228.1123 - val_loss: 105.1345\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20502.9668 - val_loss: 104.0545\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17173.5605 - val_loss: 105.6466\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20641.8809 - val_loss: 105.4518\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17357.2656 - val_loss: 104.2334\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 13647.0098 - val_loss: 104.4060\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1553.8468 - val_loss: 105.3803\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 37334.1367 - val_loss: 105.9586\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21726.3867 - val_loss: 104.6967\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4843.1680 - val_loss: 102.3730\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 56060.7500 - val_loss: 101.8358\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 60890.3750 - val_loss: 102.1178\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 46434.4766 - val_loss: 103.5554\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14064.5098 - val_loss: 105.4402\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 33137.8516 - val_loss: 105.5942\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 39166.3281 - val_loss: 104.9097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 20 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 153036.5938 - val_loss: 91.0811\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 51344.8867 - val_loss: 94.5889\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11474.7021 - val_loss: 88.9247\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 82364.7734 - val_loss: 87.4663\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 67666.9844 - val_loss: 91.1752\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13824.7666 - val_loss: 96.8132\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 99244.3750 - val_loss: 98.1420\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 82364.0781 - val_loss: 97.0751\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27235.7461 - val_loss: 93.0124\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 43126.6914 - val_loss: 91.0889\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 52095.1719 - val_loss: 91.8878\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27680.6914 - val_loss: 95.1971\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 25640.0547 - val_loss: 95.3093\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27548.8945 - val_loss: 93.4797\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 11939.5547 - val_loss: 93.9073\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8529.1758 - val_loss: 93.6507\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21257.2461 - val_loss: 93.1275\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9183.7842 - val_loss: 94.0099\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2601.6189 - val_loss: 94.2404\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8991.8965 - val_loss: 93.2996\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15834.4111 - val_loss: 93.8337\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5786.2026 - val_loss: 93.6783\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13946.8389 - val_loss: 93.7883\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4582.2695 - val_loss: 93.6678\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18321.9551 - val_loss: 93.4856\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 501.6021 - val_loss: 94.9518\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 27122.6504 - val_loss: 95.3608\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 25960.1309 - val_loss: 94.6539\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5346.0771 - val_loss: 94.1289\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 422.6141 - val_loss: 93.1363\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20072.6680 - val_loss: 94.0515\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 2995.4751 - val_loss: 93.6216\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24235.1660 - val_loss: 93.3483\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4822.7593 - val_loss: 95.6259\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 52947.9102 - val_loss: 96.8026\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 51594.9062 - val_loss: 95.3953\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 20711.9512 - val_loss: 93.2889\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 41898.3945 - val_loss: 92.6731\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 36621.5547 - val_loss: 94.0834\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4758.2710 - val_loss: 96.1020\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 32284.0918 - val_loss: 96.3121\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 35852.3438 - val_loss: 95.4982\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16415.5059 - val_loss: 93.7548\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 28512.2930 - val_loss: 93.6651\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23748.3945 - val_loss: 94.1376\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2148.8240 - val_loss: 96.1643\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 28889.4609 - val_loss: 96.6882\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 43679.6875 - val_loss: 96.5427\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31596.6211 - val_loss: 95.1401\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8989.8730 - val_loss: 94.8020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 21 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 75.9538 - val_loss: 60.6610\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 40.7564 - val_loss: 24.6694\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 39.0600 - val_loss: 30.1170\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 29.8222 - val_loss: 37.0989\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 27.7230 - val_loss: 26.6163\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22.9384 - val_loss: 14.0634\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 21.1100 - val_loss: 21.6008\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 18.9889 - val_loss: 12.6067\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 16.4311 - val_loss: 9.6227\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14.9490 - val_loss: 6.7772\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13.7686 - val_loss: 2.6990\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 12.6687 - val_loss: 2.8650\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.2264 - val_loss: 3.9233\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 12.0804 - val_loss: 2.9657\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 11.5235 - val_loss: 2.6963\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.7011 - val_loss: 5.1280\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.6088 - val_loss: 3.0294\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.8011 - val_loss: 5.2022\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.7435 - val_loss: 2.5318\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.1217 - val_loss: 3.7389\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.7477 - val_loss: 3.1432\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.4798 - val_loss: 2.5673\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.2318 - val_loss: 3.3795\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9180 - val_loss: 2.3858\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4996 - val_loss: 3.0501\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.4637 - val_loss: 2.2873\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2991 - val_loss: 3.8701\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.4176 - val_loss: 2.3273\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.0281 - val_loss: 3.0360\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.7423 - val_loss: 2.3380\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.8912 - val_loss: 4.8183\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.0586 - val_loss: 2.2085\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.5351 - val_loss: 4.5080\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.7744 - val_loss: 3.0554\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.5258 - val_loss: 3.9048\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.7518 - val_loss: 2.2900\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5565 - val_loss: 2.5077\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3140 - val_loss: 2.5011\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.1454 - val_loss: 2.4475\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.9997 - val_loss: 2.6700\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.9923 - val_loss: 2.2124\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2377 - val_loss: 2.9641\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2035 - val_loss: 2.5913\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.1334 - val_loss: 2.2243\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3441 - val_loss: 2.6879\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.9933 - val_loss: 2.1881\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0846 - val_loss: 2.1943\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.7495 - val_loss: 2.2020\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.0662 - val_loss: 2.0579\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4184 - val_loss: 2.8046\n",
      "Iteration number 22 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 97703.3984 - val_loss: 90.5917\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 69955.1797 - val_loss: 91.3402\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21140.0059 - val_loss: 87.9083\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 70272.4688 - val_loss: 85.6569\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 47313.2227 - val_loss: 88.8724\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 562.6105 - val_loss: 89.6084\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8805.2471 - val_loss: 90.5145\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7208.7993 - val_loss: 91.1042\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 16695.1309 - val_loss: 89.3401\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 41527.7344 - val_loss: 88.2318\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 963.4958 - val_loss: 89.9162\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 22845.2637 - val_loss: 89.8558\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10794.1875 - val_loss: 93.5922\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 53410.9922 - val_loss: 93.5703\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 41877.8789 - val_loss: 90.6227\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 33042.1367 - val_loss: 89.7141\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 10477.8447 - val_loss: 93.2828\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 61056.6367 - val_loss: 95.1065\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 54088.7578 - val_loss: 94.2108\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 41316.5312 - val_loss: 89.3217\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 52419.6406 - val_loss: 88.5771\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 57517.5000 - val_loss: 89.9088\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19497.6797 - val_loss: 91.8789\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6209.1792 - val_loss: 96.5124\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 96286.2734 - val_loss: 98.0424\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 98651.6250 - val_loss: 97.6083\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 73982.9688 - val_loss: 95.6794\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 42787.2812 - val_loss: 92.6479\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 39201.4805 - val_loss: 92.0541\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33792.8164 - val_loss: 93.5600\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11177.8623 - val_loss: 94.0801\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5484.7769 - val_loss: 93.7719\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1428.1842 - val_loss: 92.9170\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24386.5391 - val_loss: 92.8093\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7314.2148 - val_loss: 93.8518\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26563.5059 - val_loss: 95.1511\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 27656.9609 - val_loss: 94.2224\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3884.5281 - val_loss: 93.8752\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6085.3193 - val_loss: 93.8830\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 349.9977 - val_loss: 92.7577\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21629.6465 - val_loss: 93.0600\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15999.8027 - val_loss: 94.3629\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13698.9453 - val_loss: 94.2165\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6369.7007 - val_loss: 92.8673\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 32834.5117 - val_loss: 92.5890\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24090.9844 - val_loss: 93.7588\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4907.4702 - val_loss: 95.8762\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 44394.1328 - val_loss: 96.2619\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 45107.2266 - val_loss: 95.6647\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17918.8066 - val_loss: 94.6573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 23 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 80.4739 - val_loss: 57.8881\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 33.6353 - val_loss: 11.9251\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 36.7496 - val_loss: 22.0174\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 25.3321 - val_loss: 34.3443\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 24.0986 - val_loss: 27.9146\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 19.1588 - val_loss: 14.7260\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 17.9839 - val_loss: 11.8057\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14.6866 - val_loss: 14.7944\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.8731 - val_loss: 5.3641\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5403 - val_loss: 4.8095\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.2412 - val_loss: 3.0450\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.4839 - val_loss: 6.5515\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.6204 - val_loss: 3.9796\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.1988 - val_loss: 6.5109\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.2192 - val_loss: 5.9679\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.6833 - val_loss: 4.3373\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3515 - val_loss: 6.3587\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.5797 - val_loss: 2.1597\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.6649 - val_loss: 6.7958\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6589 - val_loss: 2.4987\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.2238 - val_loss: 4.8147\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.5199 - val_loss: 2.9518\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.3293 - val_loss: 4.3269\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.1513 - val_loss: 2.5175\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2316 - val_loss: 5.3111\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.0192 - val_loss: 2.9308\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.8134 - val_loss: 3.5135\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6118 - val_loss: 3.1251\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4887 - val_loss: 2.2262\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.3452 - val_loss: 5.7725\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.7685 - val_loss: 2.1298\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.5721 - val_loss: 4.8364\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.1460 - val_loss: 2.2188\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.4811 - val_loss: 5.1095\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.9227 - val_loss: 1.9450\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4253 - val_loss: 4.1645\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.6448 - val_loss: 1.8674\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.2691 - val_loss: 4.0077\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.3070 - val_loss: 1.9824\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0650 - val_loss: 5.1746\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6745 - val_loss: 1.8749\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9762 - val_loss: 3.2440\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5695 - val_loss: 2.4354\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.5530 - val_loss: 2.2184\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4932 - val_loss: 2.0068\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4919 - val_loss: 4.1385\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.5007 - val_loss: 2.0069\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4550 - val_loss: 2.5580\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.1995 - val_loss: 2.0589\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.4513 - val_loss: 2.7858\n",
      "Iteration number 24 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 2636116.2500 - val_loss: 95.4007\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4853628.0000 - val_loss: 93.7893\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3844320.7500 - val_loss: 92.6838\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2058236.6250 - val_loss: 91.6880\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1898743.3750 - val_loss: 90.8270\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1327176.3750 - val_loss: 90.1421\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1175817.2500 - val_loss: 89.3573\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1512773.3750 - val_loss: 88.5174\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 901658.4375 - val_loss: 87.6250\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 603991.2500 - val_loss: 86.9198\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 176071.0312 - val_loss: 86.1455\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 198488.6562 - val_loss: 85.2728\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 173589.6875 - val_loss: 84.4111\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 199097.9219 - val_loss: 83.5481\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 200652.3906 - val_loss: 82.6773\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 165238.2031 - val_loss: 81.9148\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 158930.8906 - val_loss: 81.2121\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 212487.5781 - val_loss: 80.4408\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 161701.8125 - val_loss: 79.6025\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 210251.0469 - val_loss: 78.8511\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 196460.5312 - val_loss: 77.9091\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 168045.2500 - val_loss: 76.9904\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 171737.7344 - val_loss: 76.2393\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 191277.0000 - val_loss: 75.5300\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 191711.0312 - val_loss: 74.6395\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 174409.3438 - val_loss: 73.9019\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 170968.3438 - val_loss: 73.0158\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 185510.2500 - val_loss: 72.2170\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 175447.1250 - val_loss: 71.4107\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 181165.6250 - val_loss: 70.7857\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 171836.2969 - val_loss: 70.0617\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 183571.7500 - val_loss: 69.3277\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 184945.1406 - val_loss: 68.5633\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 164777.9219 - val_loss: 67.7053\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 155166.4531 - val_loss: 66.9896\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 190561.7969 - val_loss: 66.2642\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 182635.7812 - val_loss: 65.5894\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 164220.9375 - val_loss: 64.8633\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 141276.4844 - val_loss: 64.1770\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 190606.0000 - val_loss: 63.4645\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 142864.0156 - val_loss: 62.8265\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 187450.5781 - val_loss: 62.2553\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 182137.3906 - val_loss: 61.7899\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 391377.4062 - val_loss: 61.1379\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 703898.5000 - val_loss: 60.9345\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 986813.2500 - val_loss: 60.8561\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 382976.6250 - val_loss: 60.7890\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 234275.9688 - val_loss: 60.6519\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 307744.1250 - val_loss: 60.4758\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 70031.6719 - val_loss: 60.1220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 25 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 122910.2266 - val_loss: 106.6814\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 127596.5625 - val_loss: 105.4160\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 62446.4336 - val_loss: 100.5896\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7725.1836 - val_loss: 100.5872\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 860.6899 - val_loss: 103.1105\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 83630.2500 - val_loss: 103.8015\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 47875.1328 - val_loss: 101.9382\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 26568.8398 - val_loss: 96.3695\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 98973.7266 - val_loss: 95.5603\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 122627.0781 - val_loss: 96.5474\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 88380.5234 - val_loss: 99.4495\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 18501.1797 - val_loss: 102.9756\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 76352.1172 - val_loss: 103.9834\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 73703.3047 - val_loss: 102.4749\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21435.9492 - val_loss: 100.7890\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14475.0117 - val_loss: 100.2441\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17063.4863 - val_loss: 101.2210\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 26398.4668 - val_loss: 101.4495\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10317.4053 - val_loss: 100.1424\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 43708.4375 - val_loss: 99.5432\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37304.5508 - val_loss: 100.8390\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 14612.9033 - val_loss: 101.2603\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5401.4229 - val_loss: 99.9545\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37523.8828 - val_loss: 99.9638\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 28810.0566 - val_loss: 100.8935\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4872.0376 - val_loss: 102.9106\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 75832.5234 - val_loss: 103.5892\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 68244.8750 - val_loss: 102.8975\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 38255.5312 - val_loss: 101.2621\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14813.2568 - val_loss: 100.6284\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16251.1436 - val_loss: 101.0381\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12121.0332 - val_loss: 101.3529\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 1990.8654 - val_loss: 100.4069\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34418.8320 - val_loss: 100.1533\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28463.8262 - val_loss: 101.0987\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13206.1992 - val_loss: 101.3796\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5141.0410 - val_loss: 100.2991\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 36535.4922 - val_loss: 100.1154\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25956.6660 - val_loss: 100.6011\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16241.2109 - val_loss: 102.6349\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 69940.3125 - val_loss: 103.4240\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 69522.4375 - val_loss: 102.8604\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 36126.2188 - val_loss: 101.9940\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15228.2998 - val_loss: 100.0634\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 67910.2266 - val_loss: 99.0711\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 61023.1172 - val_loss: 99.5137\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 44988.1641 - val_loss: 101.0871\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3569.3035 - val_loss: 102.7711\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 63577.1133 - val_loss: 103.3013\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 67400.6484 - val_loss: 103.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 26 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 107363.6328 - val_loss: 100.0468\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 75528.2500 - val_loss: 102.7378\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 25727.3027 - val_loss: 100.0184\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 58383.7695 - val_loss: 96.3767\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 55118.4570 - val_loss: 98.9474\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7265.3379 - val_loss: 100.0708\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6989.7461 - val_loss: 101.5237\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 18502.2773 - val_loss: 100.0611\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 10129.7227 - val_loss: 101.5715\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 20554.9551 - val_loss: 100.6932\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 4599.6597 - val_loss: 102.0027\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 23227.5723 - val_loss: 101.0759\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 1841.7446 - val_loss: 97.5608\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 64649.8828 - val_loss: 97.8437\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 52760.9141 - val_loss: 100.6006\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 13522.6826 - val_loss: 104.8428\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 72047.1875 - val_loss: 105.9622\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 66419.8906 - val_loss: 104.3529\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 10638.6465 - val_loss: 102.5615\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 42129.4062 - val_loss: 100.5951\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 45938.6953 - val_loss: 101.5193\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 12278.3359 - val_loss: 103.2264\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 17486.5059 - val_loss: 103.7146\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 18877.8125 - val_loss: 102.2372\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 14890.4541 - val_loss: 102.7203\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 2613.5786 - val_loss: 102.2574\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 25440.2871 - val_loss: 101.9796\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 18163.2910 - val_loss: 104.8247\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 43467.7773 - val_loss: 105.2958\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 41938.3906 - val_loss: 104.4279\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 11712.4941 - val_loss: 102.4018\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 46741.3984 - val_loss: 101.4666\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 39508.6055 - val_loss: 103.1196\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 5743.7773 - val_loss: 103.5602\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 3749.8125 - val_loss: 103.8779\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 15998.8506 - val_loss: 103.9100\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 8651.8232 - val_loss: 102.2501\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 44022.8711 - val_loss: 101.7902\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 36127.7305 - val_loss: 102.8527\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 10518.6846 - val_loss: 104.6735\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 48953.3398 - val_loss: 105.5494\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 45653.8242 - val_loss: 104.0257\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 7055.4160 - val_loss: 103.4655\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 7396.3667 - val_loss: 103.7166\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 2247.3738 - val_loss: 102.3815\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 28795.5039 - val_loss: 102.5860\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 23152.5273 - val_loss: 103.1330\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 3817.5625 - val_loss: 104.5272\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 37153.1523 - val_loss: 105.2300\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 35253.7773 - val_loss: 104.5988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 27 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 196378.4219 - val_loss: 92.3783\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 84537.9375 - val_loss: 97.5313\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 63485.2695 - val_loss: 88.9814\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 111916.2891 - val_loss: 87.3583\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 99307.3594 - val_loss: 90.7091\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 33813.1953 - val_loss: 95.3416\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 73675.5547 - val_loss: 97.2720\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 66865.6016 - val_loss: 95.0586\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 657.3619 - val_loss: 94.4063\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19031.0391 - val_loss: 94.2279\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17845.2207 - val_loss: 93.6416\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 15914.8818 - val_loss: 94.4732\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 307.0338 - val_loss: 92.9196\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 33248.5977 - val_loss: 92.6269\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 27722.1191 - val_loss: 94.1076\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15719.8848 - val_loss: 94.3579\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3291.5652 - val_loss: 92.1994\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 38340.6836 - val_loss: 92.5796\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 27757.2793 - val_loss: 93.7024\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 15105.7842 - val_loss: 94.5232\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1045.6958 - val_loss: 94.5910\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 42843.5312 - val_loss: 95.9311\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 36749.0195 - val_loss: 93.0591\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 51365.0547 - val_loss: 91.9551\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 25354.8066 - val_loss: 93.6567\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 25733.2012 - val_loss: 95.5717\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29296.2832 - val_loss: 94.5082\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7900.2021 - val_loss: 94.3403\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 946.1033 - val_loss: 93.5532\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 21029.8594 - val_loss: 94.0797\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3091.5520 - val_loss: 93.9248\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8856.3877 - val_loss: 94.8075\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 19521.1309 - val_loss: 94.8602\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 15563.4668 - val_loss: 93.9016\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 10367.6494 - val_loss: 96.2335\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 46802.3477 - val_loss: 96.4353\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 42259.6094 - val_loss: 95.1946\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 556.3193 - val_loss: 93.6367\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 51253.9609 - val_loss: 92.4475\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 35413.8789 - val_loss: 93.5233\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 23864.6621 - val_loss: 96.6537\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 45904.0273 - val_loss: 97.2991\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 63447.2812 - val_loss: 97.1562\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 49222.1133 - val_loss: 95.5723\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 3524.4536 - val_loss: 94.9789\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 598.0298 - val_loss: 94.3254\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 20319.3457 - val_loss: 94.6566\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5742.9575 - val_loss: 95.6900\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 30294.6914 - val_loss: 96.1848\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 23509.7480 - val_loss: 95.3075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 28 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 73398.9375 - val_loss: 117.6777\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 146707.6250 - val_loss: 113.6285\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 91152.2188 - val_loss: 105.7094\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 35239.8906 - val_loss: 105.8260\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 14744.4736 - val_loss: 111.0109\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 63356.9922 - val_loss: 109.8432\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 46074.7617 - val_loss: 105.4575\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 19472.8750 - val_loss: 106.4617\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5163.8818 - val_loss: 105.9675\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 47343.9023 - val_loss: 103.7852\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 36639.7109 - val_loss: 105.7857\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 27274.5137 - val_loss: 107.6734\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4295.0078 - val_loss: 103.6346\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 77102.5391 - val_loss: 101.5934\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 69266.9297 - val_loss: 103.8053\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 35908.1719 - val_loss: 108.6165\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 72275.0547 - val_loss: 110.2166\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 67734.4375 - val_loss: 107.2589\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7140.7681 - val_loss: 104.0432\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 52759.2656 - val_loss: 102.8664\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 50211.9180 - val_loss: 104.7380\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17686.8418 - val_loss: 108.3225\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 44091.0859 - val_loss: 108.1338\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 41906.6758 - val_loss: 107.3646\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 30405.7656 - val_loss: 102.9991\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 54754.6797 - val_loss: 102.0125\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 58535.6562 - val_loss: 103.2540\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16316.7090 - val_loss: 105.2300\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 20159.9453 - val_loss: 105.6796\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18308.4805 - val_loss: 104.0369\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12838.1953 - val_loss: 104.5359\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 2645.2388 - val_loss: 106.4778\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 50946.2383 - val_loss: 106.8374\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 31798.2109 - val_loss: 105.6212\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1337.4749 - val_loss: 104.3646\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5851.7764 - val_loss: 105.4086\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 20514.7051 - val_loss: 105.0084\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2573.5410 - val_loss: 103.8470\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 21424.0566 - val_loss: 103.6010\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 18867.8594 - val_loss: 105.1418\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 12278.4844 - val_loss: 104.5921\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2783.4395 - val_loss: 104.9355\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8655.4053 - val_loss: 104.4641\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4356.4380 - val_loss: 104.8518\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 20714.3242 - val_loss: 105.1812\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11297.0234 - val_loss: 103.3843\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 38589.6367 - val_loss: 102.7497\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 30120.7832 - val_loss: 104.0651\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9959.9307 - val_loss: 104.6629\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 5478.3301 - val_loss: 102.8833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 29 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 25973.3496 - val_loss: 105.7352\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 231964.0312 - val_loss: 104.3488\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 169784.6250 - val_loss: 97.8519\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 38734.7188 - val_loss: 90.9839\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 100655.0625 - val_loss: 89.3774\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 82152.9844 - val_loss: 92.0611\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 8911.4619 - val_loss: 95.5398\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 67644.9609 - val_loss: 97.6525\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 66125.1797 - val_loss: 95.5910\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14647.7373 - val_loss: 92.8173\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 65687.9062 - val_loss: 91.8415\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 60698.4258 - val_loss: 94.2792\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13909.7930 - val_loss: 97.4707\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 66956.1875 - val_loss: 98.2083\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 62425.0117 - val_loss: 96.3972\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6704.3140 - val_loss: 94.5658\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 27266.2246 - val_loss: 94.0226\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 30841.9844 - val_loss: 94.7477\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1686.8558 - val_loss: 95.3202\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 5091.3750 - val_loss: 96.3915\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 24899.2148 - val_loss: 96.2963\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 1113.5056 - val_loss: 94.6536\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 38391.6094 - val_loss: 93.9079\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 36827.1680 - val_loss: 95.6072\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 14206.7842 - val_loss: 96.1215\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 14995.2939 - val_loss: 95.3995\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9055.4736 - val_loss: 97.9284\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 43173.4023 - val_loss: 98.0023\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 43304.8203 - val_loss: 97.1365\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 18306.9727 - val_loss: 95.0293\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 43607.6250 - val_loss: 94.2254\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 40706.7188 - val_loss: 95.8769\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5960.1865 - val_loss: 96.2441\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3217.3464 - val_loss: 96.8435\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 21565.0703 - val_loss: 96.9976\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 2814.4973 - val_loss: 96.4133\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2826.4932 - val_loss: 95.6992\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 21285.9941 - val_loss: 95.9100\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3184.0667 - val_loss: 97.0317\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 13273.7402 - val_loss: 97.1338\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15406.5596 - val_loss: 96.3958\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1802.6851 - val_loss: 96.7665\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6089.5591 - val_loss: 95.7818\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 24454.3359 - val_loss: 95.7692\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10108.3037 - val_loss: 97.2204\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 21433.7480 - val_loss: 97.4940\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19977.6211 - val_loss: 96.3091\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14496.2686 - val_loss: 96.3883\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6023.7432 - val_loss: 97.9835\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 45987.6797 - val_loss: 98.5635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 30 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 34930.6211 - val_loss: 84.6999\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 340086.2500 - val_loss: 79.1136\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 252168.3125 - val_loss: 88.6945\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 126351.1562 - val_loss: 97.7109\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11111.7139 - val_loss: 99.9667\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 27546.0234 - val_loss: 98.5993\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9395.9346 - val_loss: 99.1554\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7589.9624 - val_loss: 97.2627\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 42934.8008 - val_loss: 96.9396\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21026.2754 - val_loss: 101.3425\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 47019.1602 - val_loss: 101.7861\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 57937.8867 - val_loss: 100.4210\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25248.6035 - val_loss: 96.8738\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40837.6562 - val_loss: 96.3050\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 38432.3359 - val_loss: 97.3422\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4270.3999 - val_loss: 100.5748\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 80623.1953 - val_loss: 102.5213\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 58244.9531 - val_loss: 100.6798\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16411.5059 - val_loss: 96.2248\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 76336.1797 - val_loss: 93.8803\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 80398.6406 - val_loss: 95.1282\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 45509.6758 - val_loss: 97.3301\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11275.8760 - val_loss: 98.4040\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3011.8569 - val_loss: 97.5297\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8681.9316 - val_loss: 97.0028\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 18504.3691 - val_loss: 98.3822\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18451.7480 - val_loss: 98.4154\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5771.9473 - val_loss: 98.0644\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2125.1753 - val_loss: 97.8747\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3384.9563 - val_loss: 98.3018\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 31360.5371 - val_loss: 99.4612\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 29010.3789 - val_loss: 97.5837\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 24695.7871 - val_loss: 97.0010\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17888.2852 - val_loss: 99.8993\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 46225.6523 - val_loss: 100.2507\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 44682.0234 - val_loss: 98.8742\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18265.6582 - val_loss: 96.2926\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32143.1641 - val_loss: 96.2882\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 31765.2168 - val_loss: 96.7642\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5995.3345 - val_loss: 99.4814\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 42065.9297 - val_loss: 100.3434\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 52581.1328 - val_loss: 99.9524\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 41114.3086 - val_loss: 97.5771\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 19770.8672 - val_loss: 97.0804\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 14825.8154 - val_loss: 98.1205\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 15401.5234 - val_loss: 98.3083\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6021.4072 - val_loss: 97.1098\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26550.4473 - val_loss: 96.8201\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18792.6973 - val_loss: 97.9395\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2038.9739 - val_loss: 97.7512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 31 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 76.6168 - val_loss: 57.0204\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 49.1868 - val_loss: 32.5022\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 42.7768 - val_loss: 38.5178\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 33.7367 - val_loss: 35.2200\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 27.7363 - val_loss: 22.5146\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 24.3326 - val_loss: 15.1913\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 21.3988 - val_loss: 15.5885\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 17.3572 - val_loss: 4.6548\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 17.0866 - val_loss: 7.5897\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 16.0613 - val_loss: 3.7297\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 14.6577 - val_loss: 3.9644\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 13.5900 - val_loss: 5.9709\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 13.7653 - val_loss: 4.4164\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 11.9338 - val_loss: 3.4671\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 11.7166 - val_loss: 4.1819\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.4539 - val_loss: 3.7730\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 11.5574 - val_loss: 4.1037\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 11.1911 - val_loss: 4.3082\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.0074 - val_loss: 3.1025\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10.6251 - val_loss: 4.2070\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 10.8368 - val_loss: 3.0731\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 10.9241 - val_loss: 4.1180\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10.7485 - val_loss: 2.8524\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 10.2236 - val_loss: 3.6857\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 10.2427 - val_loss: 2.7589\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 9.8084 - val_loss: 2.5270\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 9.6054 - val_loss: 3.3580\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 9.9453 - val_loss: 2.4220\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 9.4031 - val_loss: 4.1646\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.7603 - val_loss: 3.1531\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 9.3636 - val_loss: 3.5978\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 8.9390 - val_loss: 2.8113\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 9.0146 - val_loss: 3.4249\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 8.4097 - val_loss: 2.4919\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 8.0516 - val_loss: 2.7335\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 7.8205 - val_loss: 1.7402\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.4115 - val_loss: 2.7650\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.9397 - val_loss: 3.0964\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 7.5883 - val_loss: 1.9794\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 7.2926 - val_loss: 2.2030\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 7.2489 - val_loss: 1.8769\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 6.8124 - val_loss: 2.9184\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 6.8347 - val_loss: 2.3636\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 6.4615 - val_loss: 2.3959\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.2634 - val_loss: 1.6898\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.3233 - val_loss: 2.5650\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.0554 - val_loss: 2.3495\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 5.8661 - val_loss: 2.9872\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 7.1185 - val_loss: 1.5651\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.8713 - val_loss: 3.2777\n",
      "Iteration number 32 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 81.4237 - val_loss: 68.4334\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 50.5618 - val_loss: 38.6314\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 43.4994 - val_loss: 40.0330\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 35.2657 - val_loss: 41.0430\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 31.0584 - val_loss: 28.8019\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 25.6536 - val_loss: 18.3314\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 21.7105 - val_loss: 18.1929\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 18.0834 - val_loss: 9.6156\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 16.3307 - val_loss: 9.1515\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 16.1411 - val_loss: 5.5007\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 14.6479 - val_loss: 5.9282\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 13.6296 - val_loss: 5.2528\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 13.0906 - val_loss: 5.9466\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 12.5698 - val_loss: 6.1855\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 12.2014 - val_loss: 4.4614\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 11.6139 - val_loss: 4.9446\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.9319 - val_loss: 5.4537\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 11.6655 - val_loss: 4.7489\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 10.6411 - val_loss: 4.0158\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 10.2552 - val_loss: 5.2576\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 9.8862 - val_loss: 4.0702\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.4500 - val_loss: 4.6461\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.8610 - val_loss: 2.6946\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.6801 - val_loss: 5.0602\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8.4922 - val_loss: 4.0644\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.9758 - val_loss: 3.5107\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.6520 - val_loss: 4.2287\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 7.4953 - val_loss: 3.3958\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 7.1566 - val_loss: 3.9355\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.6682 - val_loss: 4.5369\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 7.6660 - val_loss: 3.7068\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 6.8241 - val_loss: 2.3198\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.3727 - val_loss: 5.6627\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 7.0325 - val_loss: 2.2623\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.2992 - val_loss: 2.9501\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.8963 - val_loss: 3.0324\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.6882 - val_loss: 3.0284\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 6.6221 - val_loss: 2.9294\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.6011 - val_loss: 2.0395\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.2182 - val_loss: 3.2364\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.3613 - val_loss: 2.0285\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.1858 - val_loss: 2.0472\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.4307 - val_loss: 3.6309\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.6644 - val_loss: 2.0181\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0840 - val_loss: 2.4500\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.0341 - val_loss: 2.1042\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 5.7871 - val_loss: 2.0670\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.8110 - val_loss: 2.8845\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.9106 - val_loss: 1.9970\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 5.8173 - val_loss: 3.0543\n",
      "Iteration number 33 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 91.5987 - val_loss: 76.3170\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 46.1025 - val_loss: 36.4421\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 35.6294 - val_loss: 18.9644\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 30.7655 - val_loss: 32.9740\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 26.1620 - val_loss: 36.8378\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 24.4185 - val_loss: 26.5283\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19.8847 - val_loss: 15.1945\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 18.3388 - val_loss: 15.9888\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.8606 - val_loss: 14.1683\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.9379 - val_loss: 7.8624\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.3767 - val_loss: 2.9888\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.4451 - val_loss: 3.3030\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.8799 - val_loss: 3.3359\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.0675 - val_loss: 2.4722\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.5927 - val_loss: 2.7105\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.8920 - val_loss: 3.1424\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.1058 - val_loss: 2.4709\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.2391 - val_loss: 4.4923\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.0000 - val_loss: 3.1236\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.9257 - val_loss: 3.0538\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9217 - val_loss: 2.6159\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.5265 - val_loss: 2.4739\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.3806 - val_loss: 3.9081\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.3200 - val_loss: 2.4403\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.5825 - val_loss: 3.5587\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0035 - val_loss: 2.1870\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.1138 - val_loss: 2.1452\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.0290 - val_loss: 3.6953\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.0958 - val_loss: 2.5004\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.7510 - val_loss: 2.2155\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.4935 - val_loss: 2.0574\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.5107 - val_loss: 2.1095\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.8314 - val_loss: 1.9686\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.4394 - val_loss: 2.7668\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3242 - val_loss: 2.1572\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5395 - val_loss: 4.2198\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.8314 - val_loss: 1.7859\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.0946 - val_loss: 2.3619\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9918 - val_loss: 1.9669\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.0350 - val_loss: 2.0347\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.2666 - val_loss: 3.6357\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5.6697 - val_loss: 2.1798\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.8622 - val_loss: 3.0637\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.1465 - val_loss: 1.6850\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.8267 - val_loss: 2.4959\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4.8446 - val_loss: 2.2763\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.5845 - val_loss: 1.7491\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 4.5886 - val_loss: 3.1149\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.5861 - val_loss: 1.7160\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4.4150 - val_loss: 2.1369\n",
      "Iteration number 34 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 178968.1250 - val_loss: 99.0497\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 222254.1094 - val_loss: 104.7032\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 64782.2383 - val_loss: 109.9773\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 160038.3750 - val_loss: 109.7538\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28761.1484 - val_loss: 105.2459\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 61133.3008 - val_loss: 102.4961\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 101469.1484 - val_loss: 102.8183\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 62922.6758 - val_loss: 105.8454\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 97671.7969 - val_loss: 106.8731\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 87914.4375 - val_loss: 103.5748\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 75495.3047 - val_loss: 102.4830\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 34832.8594 - val_loss: 104.1493\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 37081.9297 - val_loss: 104.8867\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 45023.6406 - val_loss: 104.0251\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11856.6885 - val_loss: 103.9416\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 15008.8818 - val_loss: 105.3963\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 48748.6094 - val_loss: 104.9926\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 26238.0137 - val_loss: 103.3743\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 49558.4727 - val_loss: 103.3294\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5117.8208 - val_loss: 103.3775\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7470.8789 - val_loss: 103.8174\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 56567.1758 - val_loss: 104.0369\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 25623.6973 - val_loss: 101.9053\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 16603.3594 - val_loss: 102.4069\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 36213.7734 - val_loss: 102.1516\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2247.4009 - val_loss: 101.2126\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40703.5586 - val_loss: 101.5656\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 33711.2578 - val_loss: 102.9544\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 44217.8633 - val_loss: 101.3850\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12177.4844 - val_loss: 101.5485\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18932.5527 - val_loss: 100.7630\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 18861.0371 - val_loss: 101.2274\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19506.6816 - val_loss: 101.0258\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 24919.7090 - val_loss: 100.9010\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 22361.4414 - val_loss: 102.4318\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 43851.3516 - val_loss: 102.2968\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1196.0781 - val_loss: 102.6762\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 22300.9531 - val_loss: 102.7137\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6617.2231 - val_loss: 101.4467\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 25975.8340 - val_loss: 101.7292\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21021.9434 - val_loss: 101.6493\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15061.5098 - val_loss: 101.1553\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10406.0391 - val_loss: 100.7875\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8494.5908 - val_loss: 101.0258\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13194.4219 - val_loss: 100.5781\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7256.2905 - val_loss: 101.0971\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 24822.3047 - val_loss: 100.7698\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17842.2109 - val_loss: 100.6830\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17720.2578 - val_loss: 102.3726\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 46937.9180 - val_loss: 102.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 35 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 74.9444 - val_loss: 53.6921\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31.5734 - val_loss: 10.4871\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 30.9982 - val_loss: 20.0084\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21.9976 - val_loss: 34.2115\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 21.4110 - val_loss: 27.9614\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16.9837 - val_loss: 16.3684\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.6667 - val_loss: 14.4351\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.0501 - val_loss: 16.3184\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.2252 - val_loss: 7.0235\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.9505 - val_loss: 9.2453\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.4741 - val_loss: 5.3459\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.9097 - val_loss: 5.7170\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.1227 - val_loss: 4.8284\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.9600 - val_loss: 4.5993\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.7666 - val_loss: 4.7869\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.6757 - val_loss: 4.8528\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.4114 - val_loss: 4.0864\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2893 - val_loss: 4.1699\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2648 - val_loss: 4.2604\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.9998 - val_loss: 4.4672\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2380 - val_loss: 3.6722\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.1108 - val_loss: 4.0018\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.3901 - val_loss: 4.1960\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.3072 - val_loss: 3.2959\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9212 - val_loss: 3.2202\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9865 - val_loss: 3.6646\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4089 - val_loss: 3.5330\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.1400 - val_loss: 3.6998\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.0330 - val_loss: 2.4489\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0211 - val_loss: 3.6888\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.9741 - val_loss: 2.6324\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.7382 - val_loss: 3.3255\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6576 - val_loss: 2.5176\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6221 - val_loss: 3.2144\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4119 - val_loss: 2.0628\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5121 - val_loss: 4.7613\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.5309 - val_loss: 1.8276\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3621 - val_loss: 2.0701\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.2154 - val_loss: 2.7635\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.1542 - val_loss: 2.9928\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.2161 - val_loss: 2.2652\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.0996 - val_loss: 3.0997\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.8992 - val_loss: 2.0231\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.1920 - val_loss: 2.1205\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4067 - val_loss: 3.3383\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.4753 - val_loss: 3.4257\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 5.2698 - val_loss: 4.2030\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.1457 - val_loss: 2.2164\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.2709 - val_loss: 2.1041\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0577 - val_loss: 2.7756\n",
      "Iteration number 36 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 31226.0586 - val_loss: 77.0460\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 160349.6094 - val_loss: 84.1261\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 131973.1250 - val_loss: 79.3340\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2153.9724 - val_loss: 74.0621\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 149862.3125 - val_loss: 71.7507\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 139039.8906 - val_loss: 77.3150\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 52267.5938 - val_loss: 82.5420\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 50345.1211 - val_loss: 83.6876\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 47376.1641 - val_loss: 83.1009\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3428.3191 - val_loss: 80.8603\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 64040.2500 - val_loss: 79.6540\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 56709.4922 - val_loss: 81.0134\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10841.7939 - val_loss: 84.4205\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 66610.7734 - val_loss: 86.6829\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 56692.4414 - val_loss: 86.2578\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22376.9629 - val_loss: 84.6159\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16625.0312 - val_loss: 84.5531\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26575.1504 - val_loss: 85.8321\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9632.6641 - val_loss: 86.0106\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10394.4033 - val_loss: 86.0349\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11709.7715 - val_loss: 86.3289\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8557.2344 - val_loss: 86.0941\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3180.1714 - val_loss: 87.5660\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 37951.8320 - val_loss: 87.4710\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 26941.2480 - val_loss: 86.2720\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 23499.9746 - val_loss: 85.7695\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12523.2480 - val_loss: 87.6570\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27756.0156 - val_loss: 87.8680\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31320.9746 - val_loss: 87.6465\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 501.1097 - val_loss: 85.9459\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23619.2012 - val_loss: 85.5258\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 39418.6250 - val_loss: 86.2784\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19210.5391 - val_loss: 87.8478\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13529.6963 - val_loss: 87.9099\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12020.8623 - val_loss: 86.9227\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 26299.2969 - val_loss: 86.8482\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11522.6289 - val_loss: 87.8852\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4870.4097 - val_loss: 88.0392\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6886.7905 - val_loss: 87.1211\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18125.6602 - val_loss: 87.6400\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5128.8784 - val_loss: 88.8056\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 35774.0859 - val_loss: 89.2223\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 24959.9941 - val_loss: 88.5213\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7289.3496 - val_loss: 88.0694\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1711.7935 - val_loss: 89.0001\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 36241.2695 - val_loss: 89.5883\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 27124.4434 - val_loss: 88.2618\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29306.3125 - val_loss: 87.6705\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 16424.5117 - val_loss: 88.7931\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6147.3101 - val_loss: 89.1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 37 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 80.3972 - val_loss: 60.5375\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33.7020 - val_loss: 12.3397\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 24.5539 - val_loss: 8.3108\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16.1635 - val_loss: 25.2467\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17.2891 - val_loss: 24.4149\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.6934 - val_loss: 12.3801\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.4342 - val_loss: 9.5630\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.7376 - val_loss: 14.3259\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.4690 - val_loss: 13.5191\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.5815 - val_loss: 9.8831\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.4002 - val_loss: 9.2381\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.5308 - val_loss: 6.3597\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.6012 - val_loss: 4.9876\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5.7080 - val_loss: 2.6063\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.7866 - val_loss: 2.6666\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.3506 - val_loss: 2.5663\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4300 - val_loss: 2.6197\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.2140 - val_loss: 2.8424\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.1752 - val_loss: 2.3691\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.1832 - val_loss: 2.4274\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.7547 - val_loss: 3.1635\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.7608 - val_loss: 2.4503\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.5550 - val_loss: 2.9508\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.6564 - val_loss: 2.6697\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.4693 - val_loss: 2.1131\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.4791 - val_loss: 2.5576\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.3992 - val_loss: 3.0617\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.6082 - val_loss: 2.2284\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.2435 - val_loss: 2.1534\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.1592 - val_loss: 2.0177\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.1775 - val_loss: 4.3697\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7998 - val_loss: 2.0554\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7718 - val_loss: 2.1647\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1271 - val_loss: 2.9205\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.9823 - val_loss: 2.7216\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9520 - val_loss: 1.8788\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.8841 - val_loss: 1.8413\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.9038 - val_loss: 2.2185\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.6043 - val_loss: 2.2756\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.5472 - val_loss: 2.2122\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 3.4243 - val_loss: 2.3164\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.2905 - val_loss: 1.9618\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.5748 - val_loss: 2.6381\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.5195 - val_loss: 2.4042\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.4682 - val_loss: 1.8999\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.4463 - val_loss: 2.2432\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.2546 - val_loss: 2.4132\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.2975 - val_loss: 1.7592\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.3676 - val_loss: 1.8284\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.3945 - val_loss: 2.9115\n",
      "Iteration number 38 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 76.0872 - val_loss: 56.9167\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40.1973 - val_loss: 21.7992\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 37.9952 - val_loss: 29.7696\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 29.7145 - val_loss: 39.1182\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28.7213 - val_loss: 31.5591\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 23.3968 - val_loss: 17.0834\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21.4083 - val_loss: 18.6352\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19.0689 - val_loss: 20.9886\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15.8328 - val_loss: 11.5602\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.3213 - val_loss: 8.4658\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.8658 - val_loss: 5.2513\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.8777 - val_loss: 2.2829\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.4416 - val_loss: 4.4344\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.0589 - val_loss: 2.1135\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 9.2565 - val_loss: 3.5645\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.7940 - val_loss: 4.8473\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.5070 - val_loss: 2.1030\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.7308 - val_loss: 5.7631\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.3637 - val_loss: 2.6759\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.6766 - val_loss: 3.5038\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.2587 - val_loss: 4.2535\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.8676 - val_loss: 3.8965\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.6244 - val_loss: 3.2353\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3413 - val_loss: 4.4890\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3588 - val_loss: 3.7612\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9726 - val_loss: 3.4253\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.2324 - val_loss: 4.5698\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3238 - val_loss: 3.6497\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.2494 - val_loss: 4.1397\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.3668 - val_loss: 3.7146\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.8381 - val_loss: 3.6421\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.2044 - val_loss: 3.2373\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.3742 - val_loss: 4.3176\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.0969 - val_loss: 2.5961\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2269 - val_loss: 4.7091\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.0474 - val_loss: 3.1392\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9151 - val_loss: 3.0476\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3628 - val_loss: 5.9801\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6927 - val_loss: 2.5449\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.6336 - val_loss: 3.9844\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1977 - val_loss: 3.2977\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0933 - val_loss: 4.1880\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.0200 - val_loss: 3.0613\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9850 - val_loss: 3.9372\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9641 - val_loss: 2.9597\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8822 - val_loss: 4.4123\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7521 - val_loss: 3.1831\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7904 - val_loss: 3.9599\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7355 - val_loss: 3.1755\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.6088 - val_loss: 3.2164\n",
      "Iteration number 39 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 47091.0820 - val_loss: 109.1131\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 368349.8125 - val_loss: 111.5030\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 214011.1719 - val_loss: 104.6370\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21812.4805 - val_loss: 95.3637\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 306832.6875 - val_loss: 92.4755\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 263203.8125 - val_loss: 96.4674\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 104166.2812 - val_loss: 102.1443\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 38844.0508 - val_loss: 103.3928\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 80132.9922 - val_loss: 101.7690\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 37209.3242 - val_loss: 98.9944\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 65642.5391 - val_loss: 99.1756\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7010.6392 - val_loss: 99.1527\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9436.0703 - val_loss: 99.7298\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 16897.8770 - val_loss: 98.6225\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 33460.0898 - val_loss: 99.5731\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16849.0664 - val_loss: 99.0763\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 44562.4258 - val_loss: 98.7085\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5359.7749 - val_loss: 99.4426\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7820.0889 - val_loss: 100.0435\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 25638.4922 - val_loss: 98.5713\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 51792.8047 - val_loss: 98.7738\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18914.5020 - val_loss: 99.8143\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 27057.5234 - val_loss: 97.7956\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 35376.6367 - val_loss: 96.7445\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 34159.4766 - val_loss: 99.6355\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 131884.3750 - val_loss: 100.6445\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 90910.3594 - val_loss: 98.6557\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13613.6279 - val_loss: 97.7320\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5717.2041 - val_loss: 99.4735\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 75030.8125 - val_loss: 99.1965\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 59544.1133 - val_loss: 97.0931\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 31270.0684 - val_loss: 97.1142\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13914.1875 - val_loss: 98.3491\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 48037.7422 - val_loss: 99.0233\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 30809.1406 - val_loss: 97.4504\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 31689.7969 - val_loss: 97.4827\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12776.5469 - val_loss: 98.5544\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4466.3560 - val_loss: 97.8405\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 76641.5156 - val_loss: 97.6548\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27812.7129 - val_loss: 99.6231\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 57813.5391 - val_loss: 100.9938\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 51134.2422 - val_loss: 100.2399\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16338.4502 - val_loss: 99.7341\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6702.9487 - val_loss: 100.0568\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 35313.5547 - val_loss: 99.5427\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19316.4551 - val_loss: 100.1636\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15019.0068 - val_loss: 98.5167\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 37215.1133 - val_loss: 98.7608\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14570.8994 - val_loss: 97.9556\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11501.2617 - val_loss: 98.5303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 40 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 89.2396 - val_loss: 79.9889\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 71.4865 - val_loss: 59.9108\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 54.9297 - val_loss: 39.2189\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 44.1042 - val_loss: 30.0093\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 41.2326 - val_loss: 27.2603\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 36.3825 - val_loss: 27.0408\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 33.3034 - val_loss: 18.6167\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 30.3946 - val_loss: 16.6792\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 29.2299 - val_loss: 13.9191\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 25.7257 - val_loss: 6.0844\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 22.5930 - val_loss: 12.5037\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20.7502 - val_loss: 7.0831\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17.6566 - val_loss: 4.1241\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.8920 - val_loss: 6.7969\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12.0944 - val_loss: 5.6891\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12.9065 - val_loss: 5.2268\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.9080 - val_loss: 4.4171\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.4004 - val_loss: 4.3178\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.6755 - val_loss: 8.8245\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.8357 - val_loss: 4.5937\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16.0122 - val_loss: 9.0582\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.8332 - val_loss: 5.5112\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.9415 - val_loss: 3.9394\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.7217 - val_loss: 3.2471\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.7177 - val_loss: 5.8883\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8089 - val_loss: 3.4642\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.3998 - val_loss: 4.5766\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.3772 - val_loss: 4.4629\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.0192 - val_loss: 3.7519\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.3211 - val_loss: 4.4325\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.3186 - val_loss: 4.3187\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.4019 - val_loss: 4.4748\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.8436 - val_loss: 4.0243\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.0778 - val_loss: 3.6603\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.5455 - val_loss: 4.3422\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.5031 - val_loss: 5.7105\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.8396 - val_loss: 2.5360\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.7833 - val_loss: 5.0142\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 6.9985 - val_loss: 4.4699\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.4731 - val_loss: 3.3860\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.4130 - val_loss: 5.1859\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.5895 - val_loss: 3.3771\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3195 - val_loss: 5.0919\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.2750 - val_loss: 2.9315\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.1466 - val_loss: 4.0958\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.1823 - val_loss: 4.5172\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.4011 - val_loss: 2.7439\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.5102 - val_loss: 5.0346\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.9651 - val_loss: 3.7774\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.2834 - val_loss: 3.0410\n",
      "Iteration number 41 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 160304.8750 - val_loss: 102.2317\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 67399.7109 - val_loss: 113.5374\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 151629.7969 - val_loss: 111.8665\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 127550.1953 - val_loss: 108.6011\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 50341.8633 - val_loss: 103.9577\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 31039.2461 - val_loss: 102.6149\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 34401.3008 - val_loss: 104.0151\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 25226.0410 - val_loss: 104.2103\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8795.7051 - val_loss: 101.6241\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 62094.5117 - val_loss: 100.9566\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 50632.3867 - val_loss: 102.3046\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11786.9434 - val_loss: 103.1132\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3095.9121 - val_loss: 103.2325\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 15524.9141 - val_loss: 102.7831\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 292.3264 - val_loss: 99.9559\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 64747.4414 - val_loss: 99.8420\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 64087.2773 - val_loss: 101.2766\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 15668.1660 - val_loss: 103.1339\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 20879.8887 - val_loss: 103.1489\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 24694.4473 - val_loss: 102.4283\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10909.2275 - val_loss: 101.8525\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6680.4521 - val_loss: 101.8300\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 26431.6406 - val_loss: 100.9738\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17028.3613 - val_loss: 101.8977\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 16966.1699 - val_loss: 102.2347\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8483.9531 - val_loss: 100.9838\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 41677.3281 - val_loss: 100.1946\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 15394.5742 - val_loss: 101.5167\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1040.1788 - val_loss: 104.9740\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 84557.0859 - val_loss: 105.4891\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 104894.1719 - val_loss: 104.8049\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 75378.7344 - val_loss: 103.3057\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 38469.8008 - val_loss: 100.9748\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 26303.3262 - val_loss: 99.7351\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 27221.7168 - val_loss: 100.4467\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5239.9985 - val_loss: 102.0467\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 55884.9883 - val_loss: 102.4874\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 48810.4492 - val_loss: 101.4770\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8205.9785 - val_loss: 100.2279\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3940.0884 - val_loss: 99.4841\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 22274.9844 - val_loss: 99.9478\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4068.0806 - val_loss: 100.8173\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 22784.3594 - val_loss: 100.8608\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 18586.5840 - val_loss: 99.9750\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 16285.5918 - val_loss: 99.7100\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8610.1133 - val_loss: 100.7546\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 19185.6406 - val_loss: 100.6115\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 15803.1689 - val_loss: 99.7811\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 10237.0127 - val_loss: 99.8708\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 2757.4753 - val_loss: 100.9231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 42 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 256592.0781 - val_loss: 88.8412\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 160702.6719 - val_loss: 102.2349\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 91952.8438 - val_loss: 107.7032\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 70800.0078 - val_loss: 104.4489\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 23071.0020 - val_loss: 99.1375\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 84339.9922 - val_loss: 98.3456\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 82554.7891 - val_loss: 100.2561\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 45888.0117 - val_loss: 103.7374\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 52056.9453 - val_loss: 104.9923\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 34591.8242 - val_loss: 102.0916\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19161.9688 - val_loss: 100.9556\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 15514.3633 - val_loss: 101.7643\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 14293.2998 - val_loss: 102.0118\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 1044.5667 - val_loss: 100.9560\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 40201.4414 - val_loss: 99.3428\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 39711.4727 - val_loss: 101.0821\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 2934.2749 - val_loss: 100.9522\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 13663.0820 - val_loss: 101.4311\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5469.1230 - val_loss: 100.8328\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16988.7910 - val_loss: 101.0056\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5901.9644 - val_loss: 103.8079\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 45770.8320 - val_loss: 103.6640\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 42457.8516 - val_loss: 103.0024\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6043.5283 - val_loss: 100.0548\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 43711.1797 - val_loss: 98.5385\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 49900.9688 - val_loss: 99.6149\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 20743.5469 - val_loss: 101.5820\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 15418.1611 - val_loss: 101.9079\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 14222.5889 - val_loss: 100.2346\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 30510.5918 - val_loss: 99.7466\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 12928.8516 - val_loss: 102.0249\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 26962.1250 - val_loss: 102.5841\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 30108.8750 - val_loss: 101.6662\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 12759.3105 - val_loss: 99.1534\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 51420.3828 - val_loss: 98.3422\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 46434.2227 - val_loss: 99.8205\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 5739.2661 - val_loss: 101.5443\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13481.7559 - val_loss: 101.9123\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 22589.3555 - val_loss: 101.2318\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7901.6035 - val_loss: 99.0461\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 54097.1992 - val_loss: 98.1821\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 39106.9766 - val_loss: 99.5914\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 3019.9556 - val_loss: 101.6757\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 42279.9102 - val_loss: 102.9755\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 44157.7852 - val_loss: 102.2272\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 25445.4629 - val_loss: 100.3693\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6222.4468 - val_loss: 100.2601\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 8197.9180 - val_loss: 100.9507\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19751.7754 - val_loss: 101.3746\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 241.1625 - val_loss: 100.1156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 43 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 99.0915 - val_loss: 79.4526\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 51.7472 - val_loss: 42.4144\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 45.1770 - val_loss: 38.6349\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 35.6801 - val_loss: 38.7392\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 30.3373 - val_loss: 31.3958\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 25.3594 - val_loss: 20.4785\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 20.5827 - val_loss: 15.9866\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 16.8204 - val_loss: 7.6393\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 15.5307 - val_loss: 5.0218\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 14.0348 - val_loss: 7.7734\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.3613 - val_loss: 4.4840\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 13.0502 - val_loss: 8.8806\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12.9155 - val_loss: 2.9888\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.7215 - val_loss: 5.8668\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11.1790 - val_loss: 2.7845\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11.2929 - val_loss: 4.0284\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.7324 - val_loss: 3.3229\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6203 - val_loss: 3.6606\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.5051 - val_loss: 2.9550\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.9105 - val_loss: 3.9829\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.6062 - val_loss: 2.5620\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.2450 - val_loss: 3.2575\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.9111 - val_loss: 3.1508\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.6789 - val_loss: 2.2400\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.5015 - val_loss: 2.2693\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.3056 - val_loss: 2.8755\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 7.2382 - val_loss: 1.9304\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.8855 - val_loss: 3.6169\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.8837 - val_loss: 2.2056\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.5643 - val_loss: 2.7066\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6613 - val_loss: 1.7655\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.4619 - val_loss: 4.8799\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.9220 - val_loss: 1.6745\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.7591 - val_loss: 4.6322\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.1220 - val_loss: 1.6543\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.2944 - val_loss: 4.5640\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.0810 - val_loss: 1.6477\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3922 - val_loss: 2.8973\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.4108 - val_loss: 1.5461\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4541 - val_loss: 1.9445\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.1048 - val_loss: 1.6462\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.5718 - val_loss: 4.0300\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0808 - val_loss: 3.3723\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.9586 - val_loss: 4.1592\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0533 - val_loss: 2.1021\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.7614 - val_loss: 1.7894\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9171 - val_loss: 1.9166\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.6482 - val_loss: 1.5818\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4.6713 - val_loss: 1.8836\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.6850 - val_loss: 1.7286\n",
      "Iteration number 44 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 78.5528 - val_loss: 62.7186\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 62.9445 - val_loss: 40.4890\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 51.0645 - val_loss: 44.0326\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 40.0468 - val_loss: 35.9731\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 31.8151 - val_loss: 18.7321\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 27.9383 - val_loss: 17.3860\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 22.8282 - val_loss: 6.2233\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20.3869 - val_loss: 7.6080\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 17.6770 - val_loss: 5.6140\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 15.6989 - val_loss: 6.9225\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 14.2305 - val_loss: 5.6687\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15.0791 - val_loss: 7.4601\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.3964 - val_loss: 4.8894\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17.0048 - val_loss: 4.9811\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13.1342 - val_loss: 7.9296\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13.2677 - val_loss: 4.5467\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.1913 - val_loss: 8.9122\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.8681 - val_loss: 4.4604\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.5051 - val_loss: 7.2864\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10.3201 - val_loss: 4.3534\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.5149 - val_loss: 6.0416\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.4928 - val_loss: 4.8911\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6269 - val_loss: 6.1174\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.7373 - val_loss: 4.4025\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.8988 - val_loss: 7.3654\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.3576 - val_loss: 3.8426\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.3244 - val_loss: 7.8076\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.7782 - val_loss: 3.8115\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.4384 - val_loss: 7.9394\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.8691 - val_loss: 3.5530\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.5912 - val_loss: 5.8506\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.4502 - val_loss: 5.6969\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2198 - val_loss: 3.4750\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.1909 - val_loss: 7.5371\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.6906 - val_loss: 3.4438\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.5172 - val_loss: 5.9915\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.8213 - val_loss: 4.4528\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.5338 - val_loss: 3.9532\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.2189 - val_loss: 5.1413\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.6792 - val_loss: 4.7217\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.4564 - val_loss: 4.6499\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6840 - val_loss: 5.7281\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.4340 - val_loss: 3.6325\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.3022 - val_loss: 5.9315\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.3057 - val_loss: 4.0610\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.0967 - val_loss: 4.4482\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.6047 - val_loss: 4.4603\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.2113 - val_loss: 4.5305\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.8537 - val_loss: 5.5651\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.6671 - val_loss: 3.5560\n",
      "Iteration number 45 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 12163.9287 - val_loss: 119.8618\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 228634.5312 - val_loss: 117.1720\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 136947.3438 - val_loss: 114.5623\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 31408.5918 - val_loss: 108.6054\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 590.6096 - val_loss: 99.2175\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 232872.4688 - val_loss: 95.3660\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 221355.7969 - val_loss: 96.6407\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 147222.1094 - val_loss: 100.6483\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 70685.9062 - val_loss: 105.5363\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 34302.9961 - val_loss: 108.6186\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 29752.5195 - val_loss: 107.6302\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12125.9102 - val_loss: 104.7244\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 78895.2969 - val_loss: 103.5841\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 58520.5977 - val_loss: 104.9300\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34727.4766 - val_loss: 108.1983\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 44091.5312 - val_loss: 109.4071\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 51338.7852 - val_loss: 108.7745\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21017.0410 - val_loss: 107.5526\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18807.5430 - val_loss: 106.1527\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 22023.0703 - val_loss: 106.7623\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5916.9883 - val_loss: 106.9657\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 649.9797 - val_loss: 107.6360\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19200.5195 - val_loss: 107.2203\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6439.9961 - val_loss: 105.3382\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 45743.3281 - val_loss: 105.0099\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40805.7617 - val_loss: 106.4106\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 379.9350 - val_loss: 106.4339\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11018.5264 - val_loss: 106.8316\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5916.5728 - val_loss: 106.4504\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13082.9844 - val_loss: 106.4175\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11998.4160 - val_loss: 107.0519\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8691.9766 - val_loss: 105.5473\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 28331.2500 - val_loss: 105.6647\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16363.2266 - val_loss: 106.4872\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19320.9883 - val_loss: 107.2231\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10639.6445 - val_loss: 105.4065\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 24102.3008 - val_loss: 105.3301\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 25204.8867 - val_loss: 106.0648\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8349.7520 - val_loss: 108.0732\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 53355.8945 - val_loss: 108.6889\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 46387.0781 - val_loss: 107.8277\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 21495.7598 - val_loss: 105.9032\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 22993.0918 - val_loss: 105.1329\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 24203.0312 - val_loss: 105.5864\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3992.0049 - val_loss: 107.1987\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 36322.9961 - val_loss: 107.7637\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 35593.4570 - val_loss: 106.7470\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7311.1479 - val_loss: 105.2326\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 23865.5898 - val_loss: 104.8807\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 24167.7441 - val_loss: 105.7518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 46 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 200494.7500 - val_loss: 86.8451\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3865.9946 - val_loss: 92.5481\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10625.1797 - val_loss: 94.1222\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14306.5293 - val_loss: 92.3947\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32358.6914 - val_loss: 93.3411\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1169.6337 - val_loss: 96.0558\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 62278.9414 - val_loss: 96.8926\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 51797.3164 - val_loss: 94.6548\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6918.2720 - val_loss: 90.8581\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 70756.3125 - val_loss: 90.9279\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 70959.6328 - val_loss: 91.6209\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 25056.7734 - val_loss: 94.7789\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 22998.1973 - val_loss: 96.1580\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 38913.8008 - val_loss: 95.7569\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19952.7559 - val_loss: 93.6365\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 35366.0117 - val_loss: 93.8549\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 32532.2129 - val_loss: 95.3624\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7970.8511 - val_loss: 95.1909\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8882.1211 - val_loss: 95.7116\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 23205.9102 - val_loss: 96.0580\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8316.2705 - val_loss: 95.4454\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7683.1240 - val_loss: 95.5270\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6353.7192 - val_loss: 95.9542\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13004.8896 - val_loss: 95.6410\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 521.4092 - val_loss: 96.4240\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 27930.6973 - val_loss: 96.3660\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17666.5566 - val_loss: 94.6066\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 54787.4883 - val_loss: 93.9554\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 39962.4023 - val_loss: 95.5358\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11981.3340 - val_loss: 96.3617\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8692.8418 - val_loss: 95.4661\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 31611.5723 - val_loss: 95.2547\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 23052.5977 - val_loss: 96.5087\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13705.8486 - val_loss: 96.6064\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5919.8101 - val_loss: 96.2124\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19152.3711 - val_loss: 95.6738\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 20172.1543 - val_loss: 96.0949\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3223.5432 - val_loss: 97.5717\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 47702.2031 - val_loss: 98.0328\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 44607.7188 - val_loss: 97.6419\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 32102.9375 - val_loss: 96.1738\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 14483.7891 - val_loss: 96.1714\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16517.6289 - val_loss: 97.0031\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 15626.8389 - val_loss: 97.0890\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5820.2490 - val_loss: 96.0041\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 41964.2773 - val_loss: 95.5155\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 23592.5234 - val_loss: 96.3181\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3768.6838 - val_loss: 98.3106\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 48010.7773 - val_loss: 98.6694\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 60677.7031 - val_loss: 98.5035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 47 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0000e+00 - val_loss: 96.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 48 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 112121.9453 - val_loss: 115.9916\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 62823.2188 - val_loss: 115.0217\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5932.9595 - val_loss: 110.4111\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 37400.7383 - val_loss: 110.0231\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 34892.8867 - val_loss: 112.2965\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 35556.7344 - val_loss: 113.1559\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 17361.1797 - val_loss: 108.8143\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 83129.5859 - val_loss: 106.7023\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 65746.9453 - val_loss: 110.7621\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9685.8311 - val_loss: 111.0889\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6827.1445 - val_loss: 109.0407\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 43756.4492 - val_loss: 108.0728\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 22176.6250 - val_loss: 109.5979\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6800.5610 - val_loss: 115.2619\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 104066.7656 - val_loss: 116.2699\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 112970.8125 - val_loss: 115.2268\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 75394.5391 - val_loss: 112.3375\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17294.3164 - val_loss: 108.7349\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 61853.8242 - val_loss: 106.2103\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 55456.5859 - val_loss: 107.6870\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2719.3333 - val_loss: 109.3406\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29590.1309 - val_loss: 109.7593\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 32929.9297 - val_loss: 108.4175\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9522.0547 - val_loss: 107.8619\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5023.8921 - val_loss: 107.8308\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4083.5378 - val_loss: 108.2661\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19331.6055 - val_loss: 108.3299\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11298.3047 - val_loss: 106.3798\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 42857.2266 - val_loss: 105.8976\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28584.4473 - val_loss: 107.0316\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11255.4121 - val_loss: 109.7169\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 54206.8906 - val_loss: 110.0444\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 55139.8242 - val_loss: 109.0364\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 22697.9688 - val_loss: 107.2840\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2327.0654 - val_loss: 103.9622\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 91389.1562 - val_loss: 102.7469\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 85951.2734 - val_loss: 104.1420\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 50719.4219 - val_loss: 105.9413\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14778.7793 - val_loss: 108.1088\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 41846.2188 - val_loss: 108.3478\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 42079.1992 - val_loss: 107.7160\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15893.8828 - val_loss: 106.0877\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28433.0059 - val_loss: 105.0193\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23088.4043 - val_loss: 105.5128\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13440.4082 - val_loss: 107.9204\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 51498.9336 - val_loss: 108.2781\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 55895.1914 - val_loss: 107.5912\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 38249.4883 - val_loss: 105.9537\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8415.7236 - val_loss: 105.3053\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5705.9385 - val_loss: 106.5028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 49 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 68.6432 - val_loss: 49.1721\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 43.8220 - val_loss: 25.8666\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 37.8151 - val_loss: 30.8987\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 28.8314 - val_loss: 33.6151\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25.1011 - val_loss: 18.5252\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 22.8744 - val_loss: 11.7562\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18.7063 - val_loss: 17.1232\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15.5748 - val_loss: 3.2887\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.1424 - val_loss: 5.6289\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 13.4538 - val_loss: 2.1991\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.5963 - val_loss: 5.8719\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.9948 - val_loss: 2.0551\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.4865 - val_loss: 4.7734\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.1722 - val_loss: 2.3205\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.5323 - val_loss: 3.7774\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.2426 - val_loss: 2.2092\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.5513 - val_loss: 5.0969\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.2539 - val_loss: 2.4858\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.8885 - val_loss: 2.7338\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.8338 - val_loss: 3.6400\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6144 - val_loss: 2.4610\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.2458 - val_loss: 3.3091\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.0906 - val_loss: 2.8943\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.7063 - val_loss: 2.9194\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.5299 - val_loss: 3.0370\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.1512 - val_loss: 2.8388\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0496 - val_loss: 3.6378\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6494 - val_loss: 2.5055\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4380 - val_loss: 4.3442\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9269 - val_loss: 2.8360\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.4126 - val_loss: 3.0809\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.5052 - val_loss: 3.0304\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4369 - val_loss: 4.2834\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.2754 - val_loss: 2.7377\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.2951 - val_loss: 2.8428\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.4690 - val_loss: 3.2110\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.2076 - val_loss: 2.6404\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.1302 - val_loss: 4.0828\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.2762 - val_loss: 3.0998\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.1103 - val_loss: 3.6843\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.1737 - val_loss: 3.7005\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0491 - val_loss: 4.5215\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7520 - val_loss: 3.7094\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9197 - val_loss: 2.3896\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.0773 - val_loss: 2.5557\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.7917 - val_loss: 2.8027\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.0735 - val_loss: 3.5980\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.7320 - val_loss: 2.7402\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.2543 - val_loss: 4.4926\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6762 - val_loss: 2.5541\n",
      "Iteration number 50 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 65.2903 - val_loss: 48.0871\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 41.1006 - val_loss: 17.1814\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34.9801 - val_loss: 29.0946\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27.6066 - val_loss: 34.0166\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 24.8494 - val_loss: 26.0068\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20.9910 - val_loss: 18.4688\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18.4752 - val_loss: 18.2001\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.8372 - val_loss: 17.1843\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.9496 - val_loss: 6.0434\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 11.5115 - val_loss: 11.3177\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.4762 - val_loss: 3.3023\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6979 - val_loss: 6.0274\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.3715 - val_loss: 3.5639\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.0802 - val_loss: 4.5462\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.2331 - val_loss: 4.7660\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.8552 - val_loss: 5.8866\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.4620 - val_loss: 3.2289\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.8667 - val_loss: 5.9517\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.0400 - val_loss: 3.4571\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.1884 - val_loss: 4.4170\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0127 - val_loss: 3.5888\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.8515 - val_loss: 3.5489\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.6876 - val_loss: 3.5888\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5672 - val_loss: 5.1392\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.5160 - val_loss: 4.6502\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5575 - val_loss: 3.8406\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4707 - val_loss: 3.2633\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.8991 - val_loss: 5.5058\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.5778 - val_loss: 3.2614\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7070 - val_loss: 5.6727\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8276 - val_loss: 3.2862\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8657 - val_loss: 4.4027\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.6509 - val_loss: 3.5704\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5112 - val_loss: 3.4406\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.1547 - val_loss: 3.6172\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1120 - val_loss: 3.1829\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.5366 - val_loss: 5.4240\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3732 - val_loss: 3.1808\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.2184 - val_loss: 3.5394\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5101 - val_loss: 3.9359\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0186 - val_loss: 3.0832\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.0149 - val_loss: 3.5782\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7573 - val_loss: 3.2401\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.6704 - val_loss: 4.0996\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7627 - val_loss: 3.0986\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.7931 - val_loss: 3.0157\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7866 - val_loss: 4.7405\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1034 - val_loss: 3.0280\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.2950 - val_loss: 3.3768\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7892 - val_loss: 3.7991\n",
      "Iteration number 51 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 379673.6875 - val_loss: 95.0587\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14934.9688 - val_loss: 97.1087\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 61707.0664 - val_loss: 100.1751\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 71168.6562 - val_loss: 99.3613\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 16849.4160 - val_loss: 100.2089\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 83906.9219 - val_loss: 100.9599\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 28344.8613 - val_loss: 93.8935\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 137760.7500 - val_loss: 93.1577\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 163414.9375 - val_loss: 95.1306\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 104760.9766 - val_loss: 100.8193\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 70431.6484 - val_loss: 101.3707\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 61445.0391 - val_loss: 98.0835\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 54721.3555 - val_loss: 98.1358\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 28906.8887 - val_loss: 102.0773\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 94458.9844 - val_loss: 101.9750\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 61798.3359 - val_loss: 100.5500\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11983.1348 - val_loss: 98.7618\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 18158.4258 - val_loss: 99.8594\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 62927.5078 - val_loss: 101.0932\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 23421.3125 - val_loss: 97.5039\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 90140.2109 - val_loss: 96.2339\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 84831.5000 - val_loss: 97.1547\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7502.7021 - val_loss: 101.2497\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 50779.4062 - val_loss: 102.5415\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 98856.1641 - val_loss: 102.2365\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 75623.1016 - val_loss: 98.6262\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 66496.3906 - val_loss: 97.5292\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 48487.1992 - val_loss: 99.0400\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10742.2012 - val_loss: 99.6835\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4837.9360 - val_loss: 100.3110\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 38646.0078 - val_loss: 100.4329\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6947.6118 - val_loss: 99.1502\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 67801.0000 - val_loss: 97.3029\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 67942.3047 - val_loss: 98.9952\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3040.9836 - val_loss: 101.6112\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 69352.9062 - val_loss: 102.1740\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 69240.7969 - val_loss: 100.2765\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6016.2368 - val_loss: 97.5907\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 63954.4727 - val_loss: 97.5973\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 64160.7695 - val_loss: 99.4491\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9678.0078 - val_loss: 99.7040\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 24718.4922 - val_loss: 99.4666\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20455.7773 - val_loss: 100.3117\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12678.0098 - val_loss: 98.0980\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 70114.4531 - val_loss: 97.9636\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 44066.3711 - val_loss: 99.0498\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 33041.5703 - val_loss: 100.8813\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18755.6465 - val_loss: 99.1300\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 24316.0098 - val_loss: 99.2086\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 21651.4629 - val_loss: 100.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 52 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 10622.6953 - val_loss: 109.2208\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 169230.2344 - val_loss: 105.7361\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 132609.6250 - val_loss: 112.8179\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14521.8984 - val_loss: 121.0970\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 114422.7500 - val_loss: 120.5193\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 106797.3047 - val_loss: 118.8924\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26359.1113 - val_loss: 113.1940\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 895.5901 - val_loss: 110.0983\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 46424.9688 - val_loss: 111.0575\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16435.9258 - val_loss: 112.7993\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8884.4150 - val_loss: 112.5716\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6938.3848 - val_loss: 111.2549\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 44789.8594 - val_loss: 109.8666\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 36421.1836 - val_loss: 113.7508\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 64254.1758 - val_loss: 114.8460\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 52378.5508 - val_loss: 113.0092\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 30073.0723 - val_loss: 108.4004\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 77545.0547 - val_loss: 106.6618\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 70738.3516 - val_loss: 107.6891\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 51375.7617 - val_loss: 111.2313\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 44834.8438 - val_loss: 112.3501\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 38649.0430 - val_loss: 110.6155\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2057.4302 - val_loss: 110.1690\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8105.0430 - val_loss: 108.9975\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 14708.0381 - val_loss: 109.8050\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7049.3496 - val_loss: 109.0956\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19064.1191 - val_loss: 109.2230\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6780.0664 - val_loss: 111.6101\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 46959.6250 - val_loss: 111.4763\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 36632.5938 - val_loss: 109.9221\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6909.0640 - val_loss: 109.1931\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1070.6350 - val_loss: 111.3647\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 47690.6875 - val_loss: 111.1717\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36137.8789 - val_loss: 109.6543\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11515.5234 - val_loss: 106.5733\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 75331.4062 - val_loss: 105.2482\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 62431.5273 - val_loss: 106.9621\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12274.8330 - val_loss: 109.1076\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11977.7598 - val_loss: 109.6567\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 28123.6875 - val_loss: 109.0170\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9299.7861 - val_loss: 106.9724\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 33441.4609 - val_loss: 106.6826\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 30925.6016 - val_loss: 108.0537\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6612.0488 - val_loss: 107.9673\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4696.8433 - val_loss: 108.5663\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16417.0547 - val_loss: 108.2431\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3577.5813 - val_loss: 108.2031\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 16885.1934 - val_loss: 108.4500\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5442.7134 - val_loss: 107.8540\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4134.3696 - val_loss: 107.5928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 53 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 82.5169 - val_loss: 58.7989\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 33.1647 - val_loss: 9.1541\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30.8248 - val_loss: 19.5209\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 21.8246 - val_loss: 33.5588\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23.6361 - val_loss: 31.4300\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20.3354 - val_loss: 17.4711\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18.7770 - val_loss: 13.0562\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16.5652 - val_loss: 17.6803\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15.4720 - val_loss: 19.4386\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.0119 - val_loss: 10.1643\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.0275 - val_loss: 11.6100\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.9138 - val_loss: 11.4931\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.1071 - val_loss: 6.8345\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.5810 - val_loss: 7.2806\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.0883 - val_loss: 3.1267\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.8031 - val_loss: 3.8124\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.3912 - val_loss: 3.2983\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.7252 - val_loss: 2.7826\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.3895 - val_loss: 3.9947\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5127 - val_loss: 2.6558\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.1723 - val_loss: 2.9031\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.9053 - val_loss: 2.9540\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.6352 - val_loss: 3.0579\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.4060 - val_loss: 2.9869\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2788 - val_loss: 2.7893\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9710 - val_loss: 3.0438\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6112 - val_loss: 3.6192\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7895 - val_loss: 4.3915\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.2533 - val_loss: 4.3756\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4464 - val_loss: 3.9701\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9676 - val_loss: 4.2162\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.1879 - val_loss: 3.6693\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.5774 - val_loss: 3.1101\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.8823 - val_loss: 3.9089\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5799 - val_loss: 2.7609\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7772 - val_loss: 3.0044\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.7034 - val_loss: 4.3414\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.4187 - val_loss: 4.7965\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.9816 - val_loss: 4.2923\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4458 - val_loss: 3.9308\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.6030 - val_loss: 4.3801\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9970 - val_loss: 3.7098\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4504 - val_loss: 4.7805\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7290 - val_loss: 3.2651\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8312 - val_loss: 4.1917\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.6399 - val_loss: 2.9163\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.2599 - val_loss: 3.7520\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1734 - val_loss: 3.2021\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.4360 - val_loss: 4.3493\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.2148 - val_loss: 3.0252\n",
      "Iteration number 54 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 15129.8896 - val_loss: 86.2581\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 459534.4062 - val_loss: 83.8477\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 301444.2500 - val_loss: 90.7137\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 124642.4453 - val_loss: 97.5361\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 120428.7500 - val_loss: 99.7461\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 123609.3125 - val_loss: 98.2472\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 34219.5938 - val_loss: 96.7479\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19759.9941 - val_loss: 98.0418\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 27567.9316 - val_loss: 97.9086\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20944.2832 - val_loss: 97.2583\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32571.9043 - val_loss: 98.7351\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 41712.2461 - val_loss: 98.1869\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12662.2891 - val_loss: 97.7111\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15957.5146 - val_loss: 98.5652\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 25451.8262 - val_loss: 98.5232\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12911.8779 - val_loss: 97.8107\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10251.5713 - val_loss: 98.6295\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 37660.5742 - val_loss: 98.1073\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18604.0332 - val_loss: 97.4727\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14516.7881 - val_loss: 98.2992\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 48128.3477 - val_loss: 98.7725\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22034.8594 - val_loss: 97.4795\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 31119.5820 - val_loss: 97.2904\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25212.3281 - val_loss: 98.1441\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 46849.6953 - val_loss: 98.5214\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36783.4062 - val_loss: 96.5510\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 94763.8672 - val_loss: 96.1931\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 78082.9922 - val_loss: 97.4629\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7782.1802 - val_loss: 97.4978\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19598.6074 - val_loss: 97.6504\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4705.0547 - val_loss: 97.5565\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14201.4180 - val_loss: 97.6097\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15313.3750 - val_loss: 97.7659\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5388.8511 - val_loss: 97.4063\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10486.8467 - val_loss: 98.0423\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 28487.2695 - val_loss: 98.2716\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6506.6768 - val_loss: 97.6446\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8027.7959 - val_loss: 98.7287\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 37100.4805 - val_loss: 98.6237\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12514.4736 - val_loss: 97.7934\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8257.5215 - val_loss: 98.2501\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 37013.0078 - val_loss: 98.4107\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 14368.2812 - val_loss: 96.8520\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 63551.7773 - val_loss: 96.4291\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 64866.4492 - val_loss: 97.6032\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19454.5918 - val_loss: 97.8630\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4157.3628 - val_loss: 97.8168\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6672.1279 - val_loss: 97.5458\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 7722.0742 - val_loss: 97.6925\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 3507.4778 - val_loss: 97.4978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 55 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 49851.9531 - val_loss: 126.5642\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 128560.6953 - val_loss: 122.3995\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 58325.7422 - val_loss: 119.0095\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 14064.6689 - val_loss: 110.6261\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 140257.8281 - val_loss: 106.9037\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 135895.6719 - val_loss: 107.6487\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 87954.9609 - val_loss: 110.2445\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 33434.3633 - val_loss: 114.0679\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 31030.8613 - val_loss: 115.1070\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 47069.5742 - val_loss: 114.6906\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 28024.3750 - val_loss: 110.8772\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 26734.9180 - val_loss: 110.1632\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 39459.2852 - val_loss: 110.2816\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 30079.7070 - val_loss: 113.4514\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 56676.3633 - val_loss: 114.4234\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 54880.1875 - val_loss: 112.3731\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13841.4922 - val_loss: 110.1127\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 43131.4648 - val_loss: 108.7267\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 34132.3242 - val_loss: 110.6634\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 14022.7617 - val_loss: 111.0210\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8123.6431 - val_loss: 110.1257\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 20717.5293 - val_loss: 109.4667\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 10604.8555 - val_loss: 110.1777\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7348.6382 - val_loss: 110.7812\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12144.7080 - val_loss: 109.6448\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 17481.8926 - val_loss: 109.5420\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7032.9185 - val_loss: 111.1874\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 39334.3203 - val_loss: 111.7402\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 30498.6191 - val_loss: 110.9242\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6289.4844 - val_loss: 108.3443\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 36119.0156 - val_loss: 107.3982\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 43447.6094 - val_loss: 108.1528\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 20783.8613 - val_loss: 109.3325\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 19216.7520 - val_loss: 110.0222\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6337.0977 - val_loss: 108.7256\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 15200.1973 - val_loss: 108.2721\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16889.0664 - val_loss: 108.5235\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 9911.1445 - val_loss: 110.7916\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 60935.9922 - val_loss: 111.7395\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 49744.8242 - val_loss: 110.7211\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 20742.3086 - val_loss: 108.5151\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7680.4810 - val_loss: 107.5238\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 16434.4629 - val_loss: 108.0704\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3427.7402 - val_loss: 109.3314\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 27575.7656 - val_loss: 109.2442\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 22341.8379 - val_loss: 108.2230\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 8955.9033 - val_loss: 107.7592\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 4613.0483 - val_loss: 107.9772\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5388.5928 - val_loss: 107.9014\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12051.1230 - val_loss: 108.3347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 56 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 150637.0469 - val_loss: 60.4213\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7843.4517 - val_loss: 78.2517\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 279397.3125 - val_loss: 86.6454\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 261473.6094 - val_loss: 81.3868\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 122426.0781 - val_loss: 76.5772\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9731.7227 - val_loss: 74.9318\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3858.7952 - val_loss: 77.3545\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 55303.1719 - val_loss: 78.6021\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36227.7422 - val_loss: 77.0980\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10772.2002 - val_loss: 72.8374\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 99749.6250 - val_loss: 72.0944\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 94798.7734 - val_loss: 73.4121\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 37484.5898 - val_loss: 77.3111\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 896.1779 - val_loss: 79.9520\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40042.3867 - val_loss: 80.6342\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 16345.6084 - val_loss: 78.1476\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 44849.6875 - val_loss: 77.3968\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 45159.8164 - val_loss: 79.5829\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1411.3341 - val_loss: 82.0379\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 49043.3320 - val_loss: 83.0145\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 46105.3945 - val_loss: 81.4497\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5124.9077 - val_loss: 81.3142\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9759.5859 - val_loss: 81.0556\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12114.8457 - val_loss: 81.4756\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11398.0293 - val_loss: 81.5437\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3551.6985 - val_loss: 82.1167\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13466.7002 - val_loss: 81.3448\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19051.2168 - val_loss: 81.1822\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15372.1396 - val_loss: 82.3213\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4442.9321 - val_loss: 79.9762\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 38880.6172 - val_loss: 80.1066\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40205.1953 - val_loss: 81.3452\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13516.3535 - val_loss: 84.0391\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 30569.6875 - val_loss: 84.3756\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 33835.2070 - val_loss: 83.6012\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 606.9387 - val_loss: 83.3613\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 16812.3496 - val_loss: 83.5672\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1555.7280 - val_loss: 80.9259\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 46403.3008 - val_loss: 80.7822\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 54772.1758 - val_loss: 81.7888\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 23700.8887 - val_loss: 83.6363\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20207.5781 - val_loss: 84.8879\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17181.7012 - val_loss: 83.9814\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9220.3271 - val_loss: 84.1778\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 902.1580 - val_loss: 85.6823\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 37535.0898 - val_loss: 85.9400\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 25089.7129 - val_loss: 85.0505\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9156.7344 - val_loss: 84.4651\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2349.0254 - val_loss: 85.7480\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 40231.5195 - val_loss: 86.5591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 57 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 85.0218 - val_loss: 63.3569\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 35.9170 - val_loss: 15.1152\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 31.5831 - val_loss: 16.9278\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24.0383 - val_loss: 29.0915\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 20.3073 - val_loss: 24.1956\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17.1224 - val_loss: 15.7648\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 15.0179 - val_loss: 14.7492\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.7749 - val_loss: 14.3257\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.2451 - val_loss: 10.6457\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.2244 - val_loss: 4.3161\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.3614 - val_loss: 3.5672\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.0933 - val_loss: 3.2143\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.5843 - val_loss: 3.1966\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.2331 - val_loss: 6.7468\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.5219 - val_loss: 3.0794\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.7918 - val_loss: 5.6883\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.7836 - val_loss: 2.8403\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4805 - val_loss: 3.0726\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.6094 - val_loss: 6.0282\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.9408 - val_loss: 3.2625\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6654 - val_loss: 5.4262\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.1656 - val_loss: 2.7712\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8662 - val_loss: 3.9748\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.7084 - val_loss: 3.1332\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.5641 - val_loss: 3.4557\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.6107 - val_loss: 2.8203\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.4242 - val_loss: 3.5777\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.2218 - val_loss: 3.2884\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.2461 - val_loss: 2.8288\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7653 - val_loss: 2.9338\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.7206 - val_loss: 4.4351\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.2465 - val_loss: 2.8308\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.5544 - val_loss: 2.8125\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8539 - val_loss: 2.8077\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.8028 - val_loss: 3.1894\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7935 - val_loss: 3.4576\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.6481 - val_loss: 2.6617\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.6370 - val_loss: 2.8169\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.6418 - val_loss: 2.8818\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9092 - val_loss: 4.2116\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.9149 - val_loss: 2.9937\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.5846 - val_loss: 2.7351\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.5819 - val_loss: 2.9644\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.4363 - val_loss: 3.1686\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.4516 - val_loss: 2.9046\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.8090 - val_loss: 2.7443\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.0102 - val_loss: 2.9616\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4755 - val_loss: 3.0443\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.4010 - val_loss: 3.0160\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.5008 - val_loss: 3.5227\n",
      "Iteration number 58 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 5182665.5000 - val_loss: 98.9624\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4310527.5000 - val_loss: 97.6350\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3778872.7500 - val_loss: 96.9113\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2219867.2500 - val_loss: 96.1115\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2254320.0000 - val_loss: 95.3471\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1819722.1250 - val_loss: 94.7150\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1227185.2500 - val_loss: 93.9089\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 890387.0625 - val_loss: 93.2578\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 369219.8125 - val_loss: 92.5693\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 167549.5625 - val_loss: 91.9929\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 473681.8438 - val_loss: 91.2386\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 803382.9375 - val_loss: 90.6090\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 325013.2812 - val_loss: 89.9545\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 488839.4062 - val_loss: 89.2880\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 105165.7656 - val_loss: 88.6478\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 315888.8438 - val_loss: 88.0013\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 697024.0000 - val_loss: 87.4450\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 902603.1875 - val_loss: 86.8623\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 950543.3750 - val_loss: 86.3107\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 559213.6250 - val_loss: 85.8506\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 528475.3125 - val_loss: 85.2501\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 559538.4375 - val_loss: 84.7649\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 586367.1875 - val_loss: 84.1492\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 561560.1250 - val_loss: 83.6226\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 492095.5938 - val_loss: 83.1017\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 531332.8125 - val_loss: 82.6128\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 548104.2500 - val_loss: 82.0400\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 596379.3750 - val_loss: 81.4950\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 296038.3750 - val_loss: 81.0405\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 193117.8750 - val_loss: 80.4946\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 487160.9062 - val_loss: 80.0507\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 521842.9375 - val_loss: 79.5582\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 488242.2500 - val_loss: 79.1295\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 471454.0938 - val_loss: 78.6893\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 128557.6484 - val_loss: 78.2627\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 147597.7344 - val_loss: 77.8882\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 180810.3594 - val_loss: 77.3445\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 160890.3281 - val_loss: 76.8560\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 157260.8750 - val_loss: 76.3386\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 154657.0312 - val_loss: 75.7984\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 158970.4688 - val_loss: 75.3355\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 186016.2031 - val_loss: 74.9352\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 378076.7812 - val_loss: 74.4649\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 636270.7500 - val_loss: 74.0925\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1053379.5000 - val_loss: 73.8329\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 312172.8438 - val_loss: 73.5620\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 232736.7344 - val_loss: 73.3324\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 320172.2500 - val_loss: 73.0423\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 113331.8438 - val_loss: 72.7192\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 321028.8750 - val_loss: 72.4733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 59 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 74.1189 - val_loss: 49.7629\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 35.4840 - val_loss: 12.2132\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 31.3260 - val_loss: 25.1044\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 24.8393 - val_loss: 31.8078\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 22.9921 - val_loss: 23.3178\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19.4667 - val_loss: 15.9892\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18.0315 - val_loss: 15.1514\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14.8656 - val_loss: 16.9423\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.4340 - val_loss: 7.1974\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.9612 - val_loss: 8.8777\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 10.7232 - val_loss: 5.8664\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.9344 - val_loss: 6.3418\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.3343 - val_loss: 7.2424\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.0015 - val_loss: 4.2320\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.9464 - val_loss: 9.2884\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.2345 - val_loss: 4.1377\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.4060 - val_loss: 6.0775\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.3460 - val_loss: 5.1988\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.8963 - val_loss: 5.8933\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.7824 - val_loss: 5.6149\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.5402 - val_loss: 4.5231\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.4788 - val_loss: 6.4024\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.3276 - val_loss: 3.8917\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.1897 - val_loss: 7.4156\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.4301 - val_loss: 5.0036\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.2507 - val_loss: 3.8193\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.0021 - val_loss: 5.0760\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.7368 - val_loss: 6.5533\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.6227 - val_loss: 5.1774\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4511 - val_loss: 3.6320\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.4642 - val_loss: 4.6649\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.1632 - val_loss: 4.0956\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.0628 - val_loss: 5.4356\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.9295 - val_loss: 3.7741\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0417 - val_loss: 3.9363\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.7744 - val_loss: 5.2919\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.7468 - val_loss: 4.1608\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.6619 - val_loss: 4.1833\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5306 - val_loss: 5.2604\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.7437 - val_loss: 4.3419\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5998 - val_loss: 2.0275\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9283 - val_loss: 7.1915\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.0657 - val_loss: 3.1087\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.8711 - val_loss: 2.4687\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.2990 - val_loss: 2.9290\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.1874 - val_loss: 6.9642\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.8325 - val_loss: 1.7760\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.4253 - val_loss: 3.7392\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.7002 - val_loss: 4.6808\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4302 - val_loss: 1.6556\n",
      "Iteration number 60 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 87.0844 - val_loss: 67.5338\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 41.6300 - val_loss: 25.6634\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 36.5949 - val_loss: 28.3616\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29.2408 - val_loss: 36.5468\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 26.1975 - val_loss: 32.6163\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 22.8520 - val_loss: 20.4799\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 18.7532 - val_loss: 16.4838\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.6064 - val_loss: 15.1548\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.5198 - val_loss: 10.8874\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.2873 - val_loss: 6.5435\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.1668 - val_loss: 6.9689\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3860 - val_loss: 5.3489\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.1472 - val_loss: 6.6335\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.7979 - val_loss: 4.4526\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.7834 - val_loss: 8.3306\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.5606 - val_loss: 3.0243\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.3104 - val_loss: 4.3769\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.7586 - val_loss: 4.2940\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.2481 - val_loss: 5.9793\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5459 - val_loss: 2.6282\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6160 - val_loss: 5.4666\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.1211 - val_loss: 3.6254\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.7345 - val_loss: 3.4733\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.0936 - val_loss: 4.6085\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.7329 - val_loss: 2.8992\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.2989 - val_loss: 3.4340\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.2901 - val_loss: 4.0304\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.0593 - val_loss: 3.1035\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.8662 - val_loss: 3.5396\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.9716 - val_loss: 2.9890\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9960 - val_loss: 2.6744\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.0902 - val_loss: 5.1479\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.1766 - val_loss: 2.6212\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4715 - val_loss: 2.8430\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.6952 - val_loss: 3.1395\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3645 - val_loss: 2.6489\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.2556 - val_loss: 3.3071\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.7569 - val_loss: 2.6064\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6530 - val_loss: 2.9350\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.6942 - val_loss: 2.8062\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.4776 - val_loss: 2.6322\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.9374 - val_loss: 3.1128\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.4807 - val_loss: 2.4079\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.4810 - val_loss: 2.4915\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7238 - val_loss: 2.3454\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7193 - val_loss: 3.1346\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.3972 - val_loss: 3.6738\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.3065 - val_loss: 3.2768\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.1786 - val_loss: 2.9999\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.7636 - val_loss: 2.2113\n",
      "Iteration number 61 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 18550.9297 - val_loss: 115.0797\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 291241.6250 - val_loss: 110.4249\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 191128.0156 - val_loss: 104.4535\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 53932.0703 - val_loss: 97.2562\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 100454.6797 - val_loss: 92.1032\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 90953.4453 - val_loss: 97.2977\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 36575.2734 - val_loss: 98.6470\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 23638.5254 - val_loss: 95.2406\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 52285.0664 - val_loss: 94.7360\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32695.4180 - val_loss: 96.8013\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5373.5356 - val_loss: 102.2580\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 127341.1875 - val_loss: 104.2369\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 121188.4844 - val_loss: 101.7760\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 52564.8008 - val_loss: 98.8208\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5945.4829 - val_loss: 97.4039\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6877.3843 - val_loss: 99.3461\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 35281.8242 - val_loss: 98.9468\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20387.0254 - val_loss: 96.0910\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 40850.7188 - val_loss: 96.3149\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 29387.7109 - val_loss: 99.0348\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 27968.3379 - val_loss: 98.8209\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7991.4990 - val_loss: 97.6504\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6702.3457 - val_loss: 96.9551\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26743.4473 - val_loss: 97.2268\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4659.2642 - val_loss: 101.2492\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 103027.2812 - val_loss: 103.2967\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 86364.5859 - val_loss: 101.5231\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 36095.6250 - val_loss: 98.6174\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7023.9805 - val_loss: 97.4880\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19738.4395 - val_loss: 98.2896\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 648.3627 - val_loss: 101.0815\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 69620.7266 - val_loss: 101.6010\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 62680.8555 - val_loss: 99.9389\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13123.4160 - val_loss: 97.9490\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 38237.3906 - val_loss: 96.6710\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 36381.5312 - val_loss: 98.1433\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2180.2368 - val_loss: 100.5560\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 49526.0391 - val_loss: 100.9419\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 47572.2734 - val_loss: 100.4410\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28126.3164 - val_loss: 97.4477\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 25840.2441 - val_loss: 96.9269\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 42527.0391 - val_loss: 97.0825\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 33881.4258 - val_loss: 99.9673\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 49855.8867 - val_loss: 101.0236\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 48259.3477 - val_loss: 99.5892\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4516.0166 - val_loss: 98.0956\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 45164.0938 - val_loss: 96.9332\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 40538.0547 - val_loss: 99.0456\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19320.4082 - val_loss: 99.5716\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7243.9761 - val_loss: 98.7417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 62 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 74.7477 - val_loss: 54.1404\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 33.1222 - val_loss: 6.6230\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27.6610 - val_loss: 21.1870\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20.1457 - val_loss: 28.9997\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19.6697 - val_loss: 20.1265\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17.3615 - val_loss: 13.1896\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15.6025 - val_loss: 18.6230\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.8314 - val_loss: 12.7490\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.8602 - val_loss: 13.0877\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.6708 - val_loss: 10.8477\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.9171 - val_loss: 8.2262\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3891 - val_loss: 7.0969\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.7216 - val_loss: 5.5804\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.1698 - val_loss: 6.2455\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5802 - val_loss: 3.4137\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.2383 - val_loss: 5.1367\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.0000 - val_loss: 3.2247\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.4075 - val_loss: 4.3950\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.1826 - val_loss: 3.0780\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.8568 - val_loss: 3.6857\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9061 - val_loss: 2.9836\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.5566 - val_loss: 3.0750\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.5486 - val_loss: 3.9166\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 5.8508 - val_loss: 4.4430\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.9957 - val_loss: 5.3815\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.2164 - val_loss: 3.6175\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.4371 - val_loss: 4.8654\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.9701 - val_loss: 2.7723\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.5089 - val_loss: 2.7907\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3258 - val_loss: 2.8887\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.2211 - val_loss: 2.9026\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3526 - val_loss: 3.2074\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8319 - val_loss: 2.8186\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1587 - val_loss: 2.9346\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.0207 - val_loss: 3.0008\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.8402 - val_loss: 2.5719\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.6631 - val_loss: 2.6479\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.6265 - val_loss: 2.7270\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9074 - val_loss: 3.0359\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7621 - val_loss: 2.6433\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.6381 - val_loss: 2.9109\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8057 - val_loss: 2.4617\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.6020 - val_loss: 2.6504\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.4650 - val_loss: 2.6929\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7181 - val_loss: 2.5202\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.0852 - val_loss: 2.9840\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.3007 - val_loss: 3.9188\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5017 - val_loss: 3.3517\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9671 - val_loss: 2.7832\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.5385 - val_loss: 2.5672\n",
      "Iteration number 63 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 2494805.5000 - val_loss: 100.2677\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4343650.0000 - val_loss: 99.2557\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4516037.5000 - val_loss: 98.5782\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 997127.5000 - val_loss: 97.7368\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 919595.0625 - val_loss: 96.8748\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 320067.4375 - val_loss: 96.1270\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 661921.5625 - val_loss: 95.3337\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 928741.5000 - val_loss: 94.5655\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1085453.6250 - val_loss: 93.9167\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 550441.6875 - val_loss: 93.2631\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 292260.6875 - val_loss: 92.5108\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 628908.5000 - val_loss: 91.8852\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 839545.6875 - val_loss: 91.3718\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 820962.1250 - val_loss: 90.6534\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 448170.5938 - val_loss: 90.0867\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 700980.5625 - val_loss: 89.4969\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1008954.9375 - val_loss: 88.9878\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 996091.6875 - val_loss: 88.4793\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 944104.1875 - val_loss: 88.1783\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 659326.5000 - val_loss: 87.7224\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 427523.9375 - val_loss: 87.2688\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 485016.8438 - val_loss: 86.9076\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 227976.1875 - val_loss: 86.4750\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 120343.0938 - val_loss: 85.9716\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 321602.2500 - val_loss: 85.4712\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 94944.2109 - val_loss: 85.0116\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 127257.6172 - val_loss: 84.5593\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 164036.4531 - val_loss: 84.0691\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 121901.4844 - val_loss: 83.5673\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 164157.6250 - val_loss: 83.0989\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 165100.3906 - val_loss: 82.6745\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 311278.2500 - val_loss: 82.2619\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 562399.1875 - val_loss: 81.8706\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 167890.8281 - val_loss: 81.4662\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 198516.8594 - val_loss: 81.0577\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 460130.2500 - val_loss: 80.6638\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 361037.3750 - val_loss: 80.3332\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 102867.0781 - val_loss: 79.9903\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 143012.2344 - val_loss: 79.6618\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 119470.0625 - val_loss: 79.3624\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 132646.6094 - val_loss: 79.0244\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 125859.2500 - val_loss: 78.6800\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 129262.8984 - val_loss: 78.3831\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 130376.3828 - val_loss: 78.0842\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 120355.1719 - val_loss: 77.7788\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 137197.7969 - val_loss: 77.4458\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 122266.8281 - val_loss: 77.1776\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 137449.9688 - val_loss: 76.8867\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 122923.8672 - val_loss: 76.5977\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 132120.5312 - val_loss: 76.2651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 64 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 28120.7637 - val_loss: 107.1560\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 100352.4844 - val_loss: 104.6785\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 55939.1172 - val_loss: 100.2706\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 52043.3477 - val_loss: 98.8414\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 39130.8008 - val_loss: 102.3630\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 34011.8164 - val_loss: 101.8847\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20719.4844 - val_loss: 100.1943\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 30149.3555 - val_loss: 99.8512\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 14238.3105 - val_loss: 101.5371\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 19596.9395 - val_loss: 101.4087\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13989.4639 - val_loss: 99.9766\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 40485.0273 - val_loss: 99.6211\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20667.9316 - val_loss: 101.0194\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 29266.3555 - val_loss: 101.9505\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25039.8438 - val_loss: 100.1676\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 40604.0781 - val_loss: 99.5727\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 32977.2227 - val_loss: 101.9697\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 51228.9062 - val_loss: 102.6894\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 47079.8828 - val_loss: 101.0056\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13818.7734 - val_loss: 100.4663\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6309.0664 - val_loss: 102.6871\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 63024.8828 - val_loss: 103.0306\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 51468.1602 - val_loss: 102.3185\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8776.9424 - val_loss: 99.5160\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 73808.1328 - val_loss: 97.7976\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 63340.6133 - val_loss: 99.6379\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24089.8105 - val_loss: 101.2219\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 41320.2188 - val_loss: 101.4504\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 35136.7773 - val_loss: 100.3716\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3316.5659 - val_loss: 100.1767\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8296.7266 - val_loss: 100.0913\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5886.5796 - val_loss: 99.9122\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3661.7224 - val_loss: 100.3449\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8675.7158 - val_loss: 100.2262\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3769.1958 - val_loss: 99.6156\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23237.9590 - val_loss: 99.5373\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12072.5088 - val_loss: 100.1392\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12859.1113 - val_loss: 100.4015\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7440.3643 - val_loss: 99.9689\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22653.6133 - val_loss: 99.5597\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18301.7812 - val_loss: 100.7371\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 35527.6875 - val_loss: 101.0935\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 31380.3594 - val_loss: 100.3895\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2704.1489 - val_loss: 100.1683\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6606.3657 - val_loss: 100.0855\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12207.9912 - val_loss: 99.8981\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1231.1805 - val_loss: 100.0062\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5762.1426 - val_loss: 100.2662\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11696.0732 - val_loss: 100.2175\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2639.8323 - val_loss: 100.1749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 65 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 114ms/step - loss: 90.4844 - val_loss: 70.1912\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 42.0088 - val_loss: 19.8513\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32.1752 - val_loss: 22.0390\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 24.9721 - val_loss: 30.4013\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 21.9063 - val_loss: 21.6125\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19.3314 - val_loss: 13.9611\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17.5246 - val_loss: 16.5372\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15.4696 - val_loss: 14.4114\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14.2927 - val_loss: 12.0946\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.3110 - val_loss: 6.9040\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.1257 - val_loss: 7.5069\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.6952 - val_loss: 4.7975\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.9748 - val_loss: 6.1435\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 9.4903 - val_loss: 3.7766\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 9.1847 - val_loss: 5.6721\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.7878 - val_loss: 3.6447\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.4754 - val_loss: 5.7676\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.3352 - val_loss: 3.5204\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.8757 - val_loss: 4.5245\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.8822 - val_loss: 4.0085\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.9305 - val_loss: 3.0018\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.9029 - val_loss: 5.3860\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.5073 - val_loss: 2.3488\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.6933 - val_loss: 3.9033\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.6736 - val_loss: 3.5966\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.0386 - val_loss: 2.6832\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.5370 - val_loss: 5.3838\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.6437 - val_loss: 2.1674\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.6166 - val_loss: 3.7986\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6985 - val_loss: 3.5376\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.2599 - val_loss: 3.1274\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.0601 - val_loss: 4.3154\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.9580 - val_loss: 3.1271\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.0283 - val_loss: 3.5048\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.8339 - val_loss: 3.2457\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.8476 - val_loss: 4.6269\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6161 - val_loss: 3.2877\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.4278 - val_loss: 3.1724\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3422 - val_loss: 3.2268\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.2234 - val_loss: 2.7579\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3110 - val_loss: 2.9243\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8853 - val_loss: 4.5202\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6575 - val_loss: 2.6778\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9261 - val_loss: 3.6849\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.9371 - val_loss: 2.3239\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7467 - val_loss: 4.1665\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.9038 - val_loss: 3.1305\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5.1414 - val_loss: 2.5803\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.5195 - val_loss: 3.6122\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.4421 - val_loss: 2.5364\n",
      "Iteration number 66 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 102.4346 - val_loss: 79.8718\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 45.0232 - val_loss: 28.6956\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 42.5888 - val_loss: 22.0568\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 30.8140 - val_loss: 36.6549\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26.0747 - val_loss: 38.0231\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 23.8256 - val_loss: 28.8384\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20.5573 - val_loss: 17.2658\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 18.9035 - val_loss: 17.3362\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15.6513 - val_loss: 18.4627\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.9111 - val_loss: 13.4784\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.9892 - val_loss: 10.7233\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.5548 - val_loss: 7.5988\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.3234 - val_loss: 9.9259\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.8921 - val_loss: 5.2651\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.1198 - val_loss: 9.6606\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6436 - val_loss: 6.4769\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.3454 - val_loss: 9.5707\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.4000 - val_loss: 6.8349\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.9418 - val_loss: 7.1255\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.7939 - val_loss: 7.3666\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6412 - val_loss: 7.0787\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.4888 - val_loss: 7.3597\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.2715 - val_loss: 6.8246\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.2824 - val_loss: 6.0751\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.2168 - val_loss: 5.5779\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.8614 - val_loss: 7.0656\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.8086 - val_loss: 4.1761\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.7313 - val_loss: 6.5307\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.7705 - val_loss: 6.0937\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.5180 - val_loss: 3.3958\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.3644 - val_loss: 7.1385\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.2115 - val_loss: 4.1883\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.9513 - val_loss: 6.2767\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.1457 - val_loss: 5.1302\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.7880 - val_loss: 5.0118\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.5672 - val_loss: 5.0468\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.4422 - val_loss: 4.3307\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3859 - val_loss: 4.9918\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.1749 - val_loss: 3.7264\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.0207 - val_loss: 5.4882\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.2868 - val_loss: 2.2898\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.0949 - val_loss: 3.6267\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7519 - val_loss: 4.0671\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8807 - val_loss: 3.8992\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.6948 - val_loss: 4.7998\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6159 - val_loss: 1.6711\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8988 - val_loss: 5.8361\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.5554 - val_loss: 1.8993\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7485 - val_loss: 4.6558\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.7909 - val_loss: 4.0008\n",
      "Iteration number 67 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 92.1184 - val_loss: 67.9599\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 40.1310 - val_loss: 15.9844\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 25.7555 - val_loss: 9.4668\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 18.9788 - val_loss: 22.5387\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19.2733 - val_loss: 25.7717\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16.7102 - val_loss: 13.0762\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.7309 - val_loss: 7.6051\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.8808 - val_loss: 11.4479\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.7136 - val_loss: 10.0224\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.0623 - val_loss: 7.0558\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.9304 - val_loss: 6.5452\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.1242 - val_loss: 6.9776\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0611 - val_loss: 6.1749\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.9591 - val_loss: 6.8024\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.6363 - val_loss: 6.2451\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6412 - val_loss: 6.7174\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9434 - val_loss: 6.0463\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4517 - val_loss: 6.6795\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8296 - val_loss: 6.0145\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.0382 - val_loss: 6.4608\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 5.5895 - val_loss: 6.0178\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 5.5141 - val_loss: 7.2971\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 5.7531 - val_loss: 6.1374\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 5.2194 - val_loss: 6.3648\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 5.0423 - val_loss: 6.0361\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 5.1489 - val_loss: 7.1730\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 5.7937 - val_loss: 5.9263\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 5.0448 - val_loss: 6.4023\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.0440 - val_loss: 6.0823\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7003 - val_loss: 5.7870\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 4.6545 - val_loss: 6.1929\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 4.6482 - val_loss: 5.6795\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 4.5630 - val_loss: 7.0444\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9496 - val_loss: 5.7375\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.6474 - val_loss: 5.8206\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4.8509 - val_loss: 6.2280\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 4.5297 - val_loss: 5.5566\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.4343 - val_loss: 7.0969\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 5.1148 - val_loss: 5.4159\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 4.5223 - val_loss: 6.2647\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 4.3552 - val_loss: 5.3554\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 4.4531 - val_loss: 6.6720\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 4.4216 - val_loss: 5.3091\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 4.2568 - val_loss: 5.8999\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 4.2316 - val_loss: 5.2244\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.1329 - val_loss: 5.4869\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.0446 - val_loss: 5.3420\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.9434 - val_loss: 5.1479\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 3.8742 - val_loss: 5.5328\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.7599 - val_loss: 5.0084\n",
      "Iteration number 68 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 80.5226 - val_loss: 58.5061\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 33.1927 - val_loss: 6.8662\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 25.3859 - val_loss: 2.8707\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 18.1645 - val_loss: 19.6879\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 16.6167 - val_loss: 24.4745\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 15.5911 - val_loss: 15.0432\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13.0117 - val_loss: 7.6428\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12.8662 - val_loss: 10.4895\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 10.8517 - val_loss: 13.0362\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.8504 - val_loss: 9.3732\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 8.4600 - val_loss: 8.0352\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.8242 - val_loss: 3.4342\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.5032 - val_loss: 3.0636\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 5.7423 - val_loss: 4.8145\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.9528 - val_loss: 3.3638\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 5.2141 - val_loss: 4.5582\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.3340 - val_loss: 4.7088\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 5.0067 - val_loss: 3.0299\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 4.6479 - val_loss: 3.1219\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 4.5568 - val_loss: 4.1990\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.9717 - val_loss: 4.4653\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 4.9974 - val_loss: 3.4998\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.8279 - val_loss: 3.1763\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 3.8770 - val_loss: 3.2102\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.9043 - val_loss: 3.7227\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 4.1119 - val_loss: 3.1688\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.7131 - val_loss: 3.3083\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 3.7490 - val_loss: 3.4497\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 3.7185 - val_loss: 3.2111\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.5640 - val_loss: 3.1429\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.5826 - val_loss: 3.1342\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.5998 - val_loss: 3.7129\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.0819 - val_loss: 3.0739\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.5017 - val_loss: 3.0806\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.5664 - val_loss: 3.1618\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.4973 - val_loss: 3.1528\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.3829 - val_loss: 3.1238\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.4436 - val_loss: 3.2737\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.5294 - val_loss: 3.1133\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.3371 - val_loss: 3.1062\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.3763 - val_loss: 3.5107\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.5494 - val_loss: 3.0179\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.3802 - val_loss: 3.2734\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.6652 - val_loss: 4.6346\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.0971 - val_loss: 3.4426\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.7203 - val_loss: 3.1480\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.5971 - val_loss: 3.7207\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.9734 - val_loss: 3.8126\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.0369 - val_loss: 4.4965\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.8029 - val_loss: 3.1678\n",
      "Iteration number 69 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 51.4310 - val_loss: 24.8838\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23.4341 - val_loss: 5.6661\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20.0572 - val_loss: 22.9343\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18.6873 - val_loss: 27.0747\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16.7659 - val_loss: 15.0447\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15.0575 - val_loss: 8.1889\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.3201 - val_loss: 14.0643\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 12.5941 - val_loss: 14.6826\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.1396 - val_loss: 8.9053\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6946 - val_loss: 12.5081\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.6926 - val_loss: 4.5469\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.5528 - val_loss: 6.6140\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6660 - val_loss: 4.3637\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.9358 - val_loss: 4.1482\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.3085 - val_loss: 4.0189\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.1816 - val_loss: 3.8816\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.2197 - val_loss: 4.0084\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.3040 - val_loss: 4.4599\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0613 - val_loss: 5.6295\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6823 - val_loss: 4.6639\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0218 - val_loss: 4.1386\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.6354 - val_loss: 4.1282\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.5628 - val_loss: 4.4816\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8880 - val_loss: 4.0876\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.4138 - val_loss: 4.2406\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.3893 - val_loss: 4.1573\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1661 - val_loss: 4.1335\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.1358 - val_loss: 4.1447\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.0917 - val_loss: 4.0645\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1760 - val_loss: 4.0886\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.2060 - val_loss: 4.4011\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.4041 - val_loss: 4.0307\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9195 - val_loss: 3.9130\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.8462 - val_loss: 3.9394\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.1611 - val_loss: 4.4107\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.4505 - val_loss: 4.1144\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8611 - val_loss: 4.2429\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.0910 - val_loss: 4.0709\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8581 - val_loss: 3.9247\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.9590 - val_loss: 4.0309\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.7479 - val_loss: 4.1180\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7852 - val_loss: 4.1729\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.8658 - val_loss: 3.8604\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.5357 - val_loss: 3.9821\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.8020 - val_loss: 3.7244\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.3884 - val_loss: 3.7749\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.4680 - val_loss: 3.8346\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.5499 - val_loss: 4.0115\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.8749 - val_loss: 3.9289\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.4540 - val_loss: 3.9462\n",
      "Iteration number 70 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1868641.8750 - val_loss: 73.7066\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 878342.1875 - val_loss: 89.9292\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 337272.8125 - val_loss: 104.3337\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 392228.4375 - val_loss: 102.8928\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 212276.5781 - val_loss: 99.2928\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 204276.8906 - val_loss: 93.1998\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 341083.7812 - val_loss: 95.3240\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 105049.5469 - val_loss: 100.1793\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 127542.7109 - val_loss: 100.7042\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 136728.5938 - val_loss: 98.0197\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 120972.4062 - val_loss: 97.3055\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 58636.4609 - val_loss: 101.7288\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 269186.4062 - val_loss: 102.4869\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 169753.3906 - val_loss: 99.0593\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 33123.6680 - val_loss: 98.1159\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 52541.4258 - val_loss: 99.2952\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 64196.0117 - val_loss: 97.9120\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 45283.9258 - val_loss: 100.8908\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 193159.3281 - val_loss: 101.7060\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 73791.9531 - val_loss: 97.4417\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 134579.8750 - val_loss: 96.9028\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 105965.8672 - val_loss: 99.1550\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 62788.3281 - val_loss: 100.2807\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 27195.5742 - val_loss: 98.9517\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28323.1113 - val_loss: 101.8159\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 131301.4219 - val_loss: 101.0836\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 42710.6758 - val_loss: 99.3532\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 33771.5352 - val_loss: 102.0983\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 125837.8984 - val_loss: 101.6522\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 62502.9258 - val_loss: 97.8698\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 179487.5156 - val_loss: 97.7502\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 100670.0938 - val_loss: 100.3092\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 134104.3438 - val_loss: 102.5903\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 105375.3438 - val_loss: 99.3046\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 113746.8672 - val_loss: 98.9851\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 69875.0859 - val_loss: 102.2085\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 121057.8672 - val_loss: 101.2012\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19113.3750 - val_loss: 99.4071\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 101672.1875 - val_loss: 98.0434\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 124598.5625 - val_loss: 101.3656\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 75754.6094 - val_loss: 100.5564\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9829.9570 - val_loss: 98.7070\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 226562.3125 - val_loss: 96.9788\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 116207.6328 - val_loss: 100.6909\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 114134.4609 - val_loss: 101.9989\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 105678.4922 - val_loss: 99.5667\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 64674.2617 - val_loss: 99.3001\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 41442.8203 - val_loss: 101.3276\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 59174.0664 - val_loss: 100.0319\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 54060.5625 - val_loss: 100.4075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 71 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 129ms/step - loss: 144143.2656 - val_loss: 93.9451\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 69968.9062 - val_loss: 100.1026\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 26892.1719 - val_loss: 90.8044\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 38868.5898 - val_loss: 91.0355\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 35463.1367 - val_loss: 97.3786\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 46888.4648 - val_loss: 98.4309\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16361.8682 - val_loss: 93.4078\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 28180.8184 - val_loss: 92.2023\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 26305.8691 - val_loss: 95.7460\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21777.1621 - val_loss: 96.2706\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1544.0306 - val_loss: 96.4164\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10898.2002 - val_loss: 96.5786\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12504.8750 - val_loss: 93.5796\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15783.9971 - val_loss: 94.9046\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1993.1581 - val_loss: 99.5482\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 39088.5312 - val_loss: 98.4598\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23361.2031 - val_loss: 96.3783\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 26110.9590 - val_loss: 92.7757\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 24338.9531 - val_loss: 96.6128\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 28625.0781 - val_loss: 97.6461\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9377.6221 - val_loss: 94.1825\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23363.0566 - val_loss: 93.4016\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20764.8887 - val_loss: 96.1899\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 28020.8887 - val_loss: 97.9306\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2334.8843 - val_loss: 96.3382\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22392.9062 - val_loss: 97.4518\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6775.8569 - val_loss: 94.5120\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22057.4941 - val_loss: 93.8894\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19784.5859 - val_loss: 97.5962\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25145.8633 - val_loss: 97.5214\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14335.1416 - val_loss: 92.9834\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 30047.3223 - val_loss: 93.6944\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21060.8359 - val_loss: 95.6289\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18445.2480 - val_loss: 97.4842\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7631.6890 - val_loss: 92.7810\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 55255.9883 - val_loss: 90.9996\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 46615.2031 - val_loss: 92.6657\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 17936.2656 - val_loss: 98.5785\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21535.8496 - val_loss: 100.1125\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 35872.1211 - val_loss: 98.2778\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16906.7754 - val_loss: 94.5882\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20757.1465 - val_loss: 95.0521\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15675.4131 - val_loss: 97.4714\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12860.2939 - val_loss: 97.1990\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2228.4941 - val_loss: 95.0008\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28275.5957 - val_loss: 94.5447\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21791.8301 - val_loss: 97.2370\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20533.9434 - val_loss: 98.4144\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6928.2886 - val_loss: 95.0206\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 37889.7266 - val_loss: 93.3689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 72 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1720354.8750 - val_loss: 82.2321\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 685483.5000 - val_loss: 91.2005\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 212590.4219 - val_loss: 96.3483\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 178496.6094 - val_loss: 92.4116\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 319647.3750 - val_loss: 91.6452\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 186630.5000 - val_loss: 94.6161\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 230631.7344 - val_loss: 96.3033\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 125621.9453 - val_loss: 92.5802\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 309478.0312 - val_loss: 91.6552\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 227604.5312 - val_loss: 92.9609\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 67291.3516 - val_loss: 95.8031\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 171788.8906 - val_loss: 94.7599\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 59760.6641 - val_loss: 92.0868\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 218164.5156 - val_loss: 91.7697\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 146840.2812 - val_loss: 93.4452\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 92502.5391 - val_loss: 94.8660\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 81657.3281 - val_loss: 93.0720\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 96937.7266 - val_loss: 92.9846\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 28991.3457 - val_loss: 94.2676\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 195529.6875 - val_loss: 95.9141\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 215436.7031 - val_loss: 94.3430\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 50423.1094 - val_loss: 92.3448\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 181716.8750 - val_loss: 92.5073\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 38870.0391 - val_loss: 93.9519\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 46725.1406 - val_loss: 92.1386\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 133975.2188 - val_loss: 92.2704\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 75108.9844 - val_loss: 94.6685\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 215529.5312 - val_loss: 94.7309\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 150930.2812 - val_loss: 92.9686\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 25396.3281 - val_loss: 92.9535\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 84614.9297 - val_loss: 94.1256\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 71748.7969 - val_loss: 92.5311\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 183476.0469 - val_loss: 92.1421\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 96018.9453 - val_loss: 94.0214\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34109.2812 - val_loss: 94.6577\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28535.6719 - val_loss: 94.2442\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 116510.0000 - val_loss: 94.2330\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 83249.4844 - val_loss: 96.0307\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 81638.7656 - val_loss: 95.8325\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 71091.2812 - val_loss: 95.1299\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12672.4043 - val_loss: 97.1710\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 239626.1094 - val_loss: 98.0642\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 199991.9531 - val_loss: 96.1049\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 73287.1250 - val_loss: 94.0567\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 168224.5156 - val_loss: 94.5765\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 67623.6953 - val_loss: 96.3850\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 144469.9219 - val_loss: 96.7609\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 130692.8516 - val_loss: 95.7430\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16471.1660 - val_loss: 94.9712\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 71571.8516 - val_loss: 95.3913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 73 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 225389.8750 - val_loss: 71.9609\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 22902.4336 - val_loss: 83.0532\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 101306.1016 - val_loss: 89.0479\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 129553.8281 - val_loss: 87.4517\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 76447.8594 - val_loss: 84.2368\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12305.0674 - val_loss: 82.7449\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5423.2817 - val_loss: 82.5881\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 52382.4336 - val_loss: 80.9033\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 26259.2676 - val_loss: 83.2964\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 37905.2188 - val_loss: 84.8955\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 31251.0332 - val_loss: 83.8548\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7747.8042 - val_loss: 83.5227\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7320.5278 - val_loss: 83.3795\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4788.9482 - val_loss: 83.3001\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9635.0811 - val_loss: 83.8930\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 43523.2695 - val_loss: 85.6475\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28450.9961 - val_loss: 83.5835\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 35522.3086 - val_loss: 82.6516\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 24285.4746 - val_loss: 83.7733\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 22699.0703 - val_loss: 85.5020\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22087.0098 - val_loss: 83.8515\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21429.4492 - val_loss: 84.3831\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4848.1318 - val_loss: 86.4326\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 41177.4258 - val_loss: 86.5489\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33468.7617 - val_loss: 84.7442\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19531.4668 - val_loss: 84.8055\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 419.8939 - val_loss: 84.8894\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 17773.4434 - val_loss: 84.6943\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14561.2549 - val_loss: 86.2230\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36021.3242 - val_loss: 86.8975\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16205.2783 - val_loss: 84.7611\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 39101.1250 - val_loss: 84.0773\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37485.5078 - val_loss: 84.7514\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6667.8618 - val_loss: 86.2662\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 1234.2894 - val_loss: 86.9072\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16808.5352 - val_loss: 86.3587\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6890.1733 - val_loss: 86.7474\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13307.6572 - val_loss: 86.5537\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2833.0579 - val_loss: 87.1435\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12667.1172 - val_loss: 86.3620\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15501.6943 - val_loss: 86.3541\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 242.2429 - val_loss: 86.6201\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4172.6948 - val_loss: 87.1913\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9356.1377 - val_loss: 86.9233\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 620.5816 - val_loss: 87.3921\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 22516.5176 - val_loss: 87.5324\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14272.9160 - val_loss: 85.8555\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25097.5996 - val_loss: 86.2684\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14474.0068 - val_loss: 86.9487\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21176.4805 - val_loss: 88.1185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 74 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 62.9749 - val_loss: 46.3094\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 29.8969 - val_loss: 10.8919\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 33.4016 - val_loss: 16.0011\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23.6175 - val_loss: 26.9077\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20.3578 - val_loss: 19.9647\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16.5587 - val_loss: 7.5501\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.7214 - val_loss: 12.0906\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5421 - val_loss: 4.5994\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.7304 - val_loss: 6.7494\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.9400 - val_loss: 3.3746\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.9519 - val_loss: 5.2557\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.4854 - val_loss: 4.8707\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.2783 - val_loss: 4.7741\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.2874 - val_loss: 5.6457\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7516 - val_loss: 4.0552\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.0833 - val_loss: 5.8486\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.3416 - val_loss: 5.8283\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6453 - val_loss: 3.2463\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.9901 - val_loss: 4.1991\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3951 - val_loss: 5.4770\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.4004 - val_loss: 4.1241\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0549 - val_loss: 4.6276\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.0116 - val_loss: 4.9378\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0029 - val_loss: 3.3259\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1213 - val_loss: 5.5749\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0202 - val_loss: 3.6735\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.6828 - val_loss: 3.9963\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.9708 - val_loss: 4.3622\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.0765 - val_loss: 3.7781\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.6656 - val_loss: 4.1268\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.4841 - val_loss: 3.1181\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.6896 - val_loss: 3.1970\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.4512 - val_loss: 3.1497\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.4625 - val_loss: 3.1796\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.4208 - val_loss: 3.7215\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7246 - val_loss: 4.7964\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.0479 - val_loss: 2.8766\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.1710 - val_loss: 4.0910\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.4949 - val_loss: 3.7911\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.5995 - val_loss: 2.7878\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1077 - val_loss: 4.7739\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.2591 - val_loss: 3.1516\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8321 - val_loss: 2.6697\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.5509 - val_loss: 3.7323\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.3596 - val_loss: 2.9727\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.2713 - val_loss: 2.7200\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1774 - val_loss: 2.8439\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.2063 - val_loss: 3.5657\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.1723 - val_loss: 2.5913\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.9769 - val_loss: 3.5815\n",
      "Iteration number 75 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 69.1234 - val_loss: 52.0821\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 31.7689 - val_loss: 18.2995\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 30.2779 - val_loss: 27.5433\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 22.4042 - val_loss: 33.0661\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21.4018 - val_loss: 24.5635\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18.5233 - val_loss: 13.6635\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17.3337 - val_loss: 19.3233\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15.7570 - val_loss: 16.4115\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14.0178 - val_loss: 9.0129\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12.6868 - val_loss: 13.0355\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.4026 - val_loss: 7.1636\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.2177 - val_loss: 4.1762\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.5070 - val_loss: 4.3867\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.1436 - val_loss: 3.5100\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.2073 - val_loss: 3.1897\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.8439 - val_loss: 3.7460\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.5736 - val_loss: 3.2507\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.6864 - val_loss: 4.2190\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.6809 - val_loss: 3.1454\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.4846 - val_loss: 4.6113\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.4896 - val_loss: 3.7284\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.0593 - val_loss: 3.7727\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.7696 - val_loss: 3.3869\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.8085 - val_loss: 3.4045\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.4624 - val_loss: 3.6217\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.2366 - val_loss: 3.3792\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0502 - val_loss: 3.6386\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9924 - val_loss: 3.3065\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6793 - val_loss: 3.6001\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.5397 - val_loss: 3.3286\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.2878 - val_loss: 3.1128\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.1764 - val_loss: 3.3657\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9855 - val_loss: 3.7169\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.1253 - val_loss: 2.9642\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7496 - val_loss: 3.3172\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5664 - val_loss: 2.8768\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3647 - val_loss: 3.8506\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.6250 - val_loss: 3.3573\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.5271 - val_loss: 3.2359\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0049 - val_loss: 2.7551\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.9263 - val_loss: 3.4534\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.2313 - val_loss: 2.6152\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.2533 - val_loss: 2.7479\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3053 - val_loss: 3.6536\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1028 - val_loss: 3.0216\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.2162 - val_loss: 2.5659\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8630 - val_loss: 2.2911\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.5728 - val_loss: 2.4670\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.7341 - val_loss: 3.2723\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.8701 - val_loss: 2.6105\n",
      "Iteration number 76 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 90937.9453 - val_loss: 93.0035\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 116423.0625 - val_loss: 94.4752\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 60356.2305 - val_loss: 90.9733\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 21121.0312 - val_loss: 83.9647\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 121680.4609 - val_loss: 82.4673\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 114777.5625 - val_loss: 85.2298\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 55661.0312 - val_loss: 88.7617\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12380.9326 - val_loss: 90.0636\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6967.3242 - val_loss: 89.3485\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6135.1724 - val_loss: 88.9501\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19788.2500 - val_loss: 89.3504\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 25021.7891 - val_loss: 90.9770\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7488.6289 - val_loss: 89.2677\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14432.4570 - val_loss: 89.1346\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19958.6094 - val_loss: 90.2223\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 19352.8086 - val_loss: 90.9280\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12440.3516 - val_loss: 88.0528\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 66362.6641 - val_loss: 87.1876\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 62703.2812 - val_loss: 89.3739\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20308.1270 - val_loss: 91.9421\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 28321.0234 - val_loss: 92.4600\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 27635.0762 - val_loss: 91.1098\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12717.0410 - val_loss: 90.9800\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5445.4990 - val_loss: 93.0844\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 50772.1680 - val_loss: 93.7051\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 43711.7383 - val_loss: 92.2428\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 911.7878 - val_loss: 91.7947\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7801.8340 - val_loss: 91.3801\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15067.5166 - val_loss: 91.3088\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7708.5659 - val_loss: 93.2991\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 50707.7266 - val_loss: 94.0761\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 30756.3066 - val_loss: 92.8987\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9805.2686 - val_loss: 89.9990\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 74724.3125 - val_loss: 88.5347\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 69510.8828 - val_loss: 89.8602\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 32099.8613 - val_loss: 91.4060\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21497.4727 - val_loss: 94.4761\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 47355.2109 - val_loss: 95.4273\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 60405.7422 - val_loss: 95.5230\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40371.7109 - val_loss: 94.2416\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16927.4570 - val_loss: 92.2522\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21099.0059 - val_loss: 91.9107\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 35225.9688 - val_loss: 92.0230\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 24890.2559 - val_loss: 93.8333\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 17824.8281 - val_loss: 94.4306\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17476.5059 - val_loss: 93.7479\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 1846.2224 - val_loss: 93.9343\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7105.3921 - val_loss: 93.6417\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9410.5879 - val_loss: 93.5797\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 175.5434 - val_loss: 94.5385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 77 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 81.7749 - val_loss: 64.6692\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 60.8235 - val_loss: 46.4818\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 42.4123 - val_loss: 44.0478\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 34.6311 - val_loss: 25.6920\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 30.5420 - val_loss: 19.9497\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25.0290 - val_loss: 23.4957\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 24.2845 - val_loss: 13.0644\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21.8940 - val_loss: 6.8494\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21.0301 - val_loss: 8.1277\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18.9025 - val_loss: 3.4627\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 16.5495 - val_loss: 7.0260\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15.8420 - val_loss: 3.1460\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14.9814 - val_loss: 4.5043\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.3297 - val_loss: 3.1327\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.7704 - val_loss: 3.3371\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.3141 - val_loss: 3.6064\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.7809 - val_loss: 2.8183\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.0262 - val_loss: 5.1596\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.9042 - val_loss: 3.5552\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.1075 - val_loss: 3.4383\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.4308 - val_loss: 4.1783\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.9352 - val_loss: 3.3452\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.4814 - val_loss: 3.2241\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.8626 - val_loss: 3.1417\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.3458 - val_loss: 3.7250\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.0593 - val_loss: 3.2198\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.7971 - val_loss: 3.7740\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.2525 - val_loss: 3.4198\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6405 - val_loss: 3.3217\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.7294 - val_loss: 3.8359\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.4262 - val_loss: 3.1672\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.7343 - val_loss: 4.0161\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.0549 - val_loss: 3.4535\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 12.1631 - val_loss: 4.3895\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.0739 - val_loss: 4.1697\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.5666 - val_loss: 3.6550\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.4846 - val_loss: 4.6417\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5192 - val_loss: 3.9512\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.8177 - val_loss: 4.1328\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6080 - val_loss: 3.3466\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.9880 - val_loss: 3.8618\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.8678 - val_loss: 3.2746\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.7616 - val_loss: 3.5745\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.6517 - val_loss: 3.4919\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7437 - val_loss: 3.3261\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.7970 - val_loss: 3.5770\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.7057 - val_loss: 3.8175\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.3114 - val_loss: 2.9630\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.3150 - val_loss: 3.6697\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5758 - val_loss: 5.4237\n",
      "Iteration number 78 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 69.5878 - val_loss: 47.2780\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27.0954 - val_loss: 5.1176\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29.0914 - val_loss: 16.2686\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19.4455 - val_loss: 27.9557\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21.1063 - val_loss: 25.9970\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 17.9938 - val_loss: 13.9917\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16.8877 - val_loss: 6.9331\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14.5491 - val_loss: 13.6878\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.8247 - val_loss: 9.4619\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.7976 - val_loss: 4.9681\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.5791 - val_loss: 6.4286\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.1256 - val_loss: 2.5090\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.3984 - val_loss: 3.6753\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.2590 - val_loss: 3.1660\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.8183 - val_loss: 3.5243\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2530 - val_loss: 2.8211\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2064 - val_loss: 3.4768\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.5286 - val_loss: 2.9753\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.6208 - val_loss: 4.3902\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.1779 - val_loss: 2.9483\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.7623 - val_loss: 4.0531\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5211 - val_loss: 2.9751\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.4010 - val_loss: 3.8396\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.2679 - val_loss: 3.2723\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1217 - val_loss: 4.1503\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.9655 - val_loss: 3.9056\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1786 - val_loss: 4.3838\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.0344 - val_loss: 3.3153\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.6877 - val_loss: 4.5579\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.1946 - val_loss: 3.4712\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7228 - val_loss: 3.2887\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7496 - val_loss: 3.3257\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.6738 - val_loss: 3.5504\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.8079 - val_loss: 3.1893\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7133 - val_loss: 4.4532\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.6998 - val_loss: 2.9799\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.5955 - val_loss: 3.6313\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.7699 - val_loss: 3.0928\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7491 - val_loss: 3.2348\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.6511 - val_loss: 4.0420\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.5424 - val_loss: 3.0760\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.2696 - val_loss: 3.1751\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1666 - val_loss: 3.0776\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.1413 - val_loss: 2.9576\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.0967 - val_loss: 3.3277\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1412 - val_loss: 2.9639\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.9943 - val_loss: 2.9249\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.0354 - val_loss: 3.2509\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.2023 - val_loss: 2.9475\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.2988 - val_loss: 3.6745\n",
      "Iteration number 79 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 4749770.0000 - val_loss: 100.4075\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3814828.2500 - val_loss: 98.7893\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3732720.0000 - val_loss: 97.6544\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1310775.6250 - val_loss: 96.1586\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 862910.0000 - val_loss: 95.2983\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 748211.9375 - val_loss: 93.9874\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 778496.0625 - val_loss: 93.0150\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 463411.1562 - val_loss: 92.0694\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 779023.2500 - val_loss: 91.1079\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 218704.2031 - val_loss: 90.0809\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 177394.6250 - val_loss: 89.1606\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 309665.6875 - val_loss: 88.2918\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 692825.8750 - val_loss: 87.3733\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 598825.0625 - val_loss: 86.4821\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 681008.6875 - val_loss: 85.7058\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 279140.5000 - val_loss: 84.8133\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 236293.2969 - val_loss: 84.0805\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 577102.4375 - val_loss: 83.3548\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 665346.3750 - val_loss: 82.5534\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 173797.5625 - val_loss: 81.7675\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 426869.8125 - val_loss: 80.9306\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 673947.9375 - val_loss: 80.3454\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 309868.7500 - val_loss: 79.7085\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 131625.8750 - val_loss: 78.9557\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 398123.6562 - val_loss: 78.2243\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 648727.6875 - val_loss: 77.5977\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 277679.2812 - val_loss: 77.0629\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 143906.6250 - val_loss: 76.4312\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 155327.2812 - val_loss: 75.6702\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 152744.2969 - val_loss: 75.0401\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 195360.7188 - val_loss: 74.2771\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 243140.6562 - val_loss: 73.5394\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 577597.4375 - val_loss: 72.9697\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 460485.2500 - val_loss: 72.4992\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 598356.1875 - val_loss: 72.0453\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 206248.4062 - val_loss: 71.5997\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 124738.6016 - val_loss: 71.0163\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 411713.8438 - val_loss: 70.5126\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 537300.2500 - val_loss: 70.0211\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 198059.9844 - val_loss: 69.4682\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 168706.6562 - val_loss: 68.9978\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 133566.9219 - val_loss: 68.4815\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 170653.3906 - val_loss: 67.9851\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 153477.9688 - val_loss: 67.4754\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 142119.7031 - val_loss: 66.9045\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 131852.4219 - val_loss: 66.3303\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 166262.1875 - val_loss: 65.9289\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 155959.9531 - val_loss: 65.4773\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 137302.1250 - val_loss: 65.0282\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 141847.7969 - val_loss: 64.6346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 80 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 75.0589 - val_loss: 55.3152\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 30.9972 - val_loss: 10.3572\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 32.3190 - val_loss: 20.5226\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 22.6841 - val_loss: 32.5392\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21.5679 - val_loss: 24.0125\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 17.7880 - val_loss: 11.3158\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15.4920 - val_loss: 14.5916\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12.1295 - val_loss: 9.7697\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3779 - val_loss: 5.9077\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10.4164 - val_loss: 3.2144\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.9229 - val_loss: 4.7429\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.0152 - val_loss: 3.2495\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.5866 - val_loss: 4.0534\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.2568 - val_loss: 2.5664\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.9485 - val_loss: 3.0507\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.6396 - val_loss: 3.1705\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.6228 - val_loss: 2.7682\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.0038 - val_loss: 3.3829\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.0764 - val_loss: 3.0489\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.8438 - val_loss: 3.7034\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.0619 - val_loss: 3.0578\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.7756 - val_loss: 3.7645\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.5599 - val_loss: 3.5719\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.3259 - val_loss: 2.6469\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.0923 - val_loss: 3.0063\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.1199 - val_loss: 3.4195\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.0819 - val_loss: 2.5848\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.0038 - val_loss: 2.7470\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 5.9167 - val_loss: 3.9424\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.0941 - val_loss: 2.8219\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.5304 - val_loss: 3.6867\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.9444 - val_loss: 4.2666\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.8937 - val_loss: 3.5189\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 5.6680 - val_loss: 2.9731\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5084 - val_loss: 2.4369\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.4872 - val_loss: 2.7355\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.1052 - val_loss: 2.3651\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9211 - val_loss: 2.9452\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 4.9891 - val_loss: 2.3101\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 5.3147 - val_loss: 2.7945\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5.2574 - val_loss: 3.4266\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7118 - val_loss: 2.3393\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 4.6071 - val_loss: 2.7416\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.6650 - val_loss: 2.9631\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.8566 - val_loss: 2.1251\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4.9889 - val_loss: 2.4344\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9489 - val_loss: 3.4026\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 4.8441 - val_loss: 1.8416\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.5425 - val_loss: 2.1233\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.3290 - val_loss: 3.3179\n",
      "Iteration number 81 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 4802774.5000 - val_loss: 91.6412\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3961739.2500 - val_loss: 90.5651\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3853653.0000 - val_loss: 90.3854\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 1737857.8750 - val_loss: 89.7312\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 740637.8125 - val_loss: 89.0482\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1011321.5625 - val_loss: 88.4389\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 1269717.8750 - val_loss: 87.6789\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 878352.5000 - val_loss: 86.9829\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 492528.5312 - val_loss: 86.2636\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 915144.1875 - val_loss: 85.6245\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 269590.2500 - val_loss: 84.8551\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 223298.2969 - val_loss: 83.9992\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 206139.4531 - val_loss: 83.2452\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 599784.0625 - val_loss: 82.6404\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 585976.1875 - val_loss: 81.9202\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 590975.4375 - val_loss: 81.2016\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 661494.9375 - val_loss: 80.6036\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 650042.8125 - val_loss: 79.9782\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 573503.6250 - val_loss: 79.4692\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 534679.5625 - val_loss: 79.0328\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 653349.5000 - val_loss: 78.5498\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 582864.6250 - val_loss: 78.1100\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 523466.2188 - val_loss: 77.6048\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 547401.8750 - val_loss: 77.1510\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 588875.2500 - val_loss: 76.7682\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 465210.0000 - val_loss: 76.2798\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 141994.3125 - val_loss: 75.7686\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 255660.3125 - val_loss: 75.1973\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 629917.6875 - val_loss: 74.7537\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 384446.6562 - val_loss: 74.2201\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 388629.5312 - val_loss: 73.7555\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 235272.1562 - val_loss: 73.2534\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 290646.2812 - val_loss: 72.8323\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 89480.6562 - val_loss: 72.3694\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 402401.9375 - val_loss: 71.8511\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 614796.6250 - val_loss: 71.3657\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 268480.3125 - val_loss: 70.9565\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 137544.2500 - val_loss: 70.5042\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 164960.8438 - val_loss: 70.0196\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 465755.4062 - val_loss: 69.5604\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 520929.1562 - val_loss: 69.2713\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 230864.8438 - val_loss: 68.9168\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 184688.5781 - val_loss: 68.5507\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 474652.2188 - val_loss: 68.1473\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 531336.1875 - val_loss: 67.8791\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 149264.0469 - val_loss: 67.5053\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 150405.3750 - val_loss: 67.1296\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 135729.2969 - val_loss: 66.7118\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 354100.4062 - val_loss: 66.3600\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 528948.8125 - val_loss: 66.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 82 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 87753.6719 - val_loss: 98.9431\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 77573.6484 - val_loss: 97.6518\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31701.7344 - val_loss: 94.5834\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 61044.6133 - val_loss: 93.2153\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 44093.6406 - val_loss: 96.5359\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 24641.9961 - val_loss: 96.8194\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15495.9229 - val_loss: 94.4543\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 35958.4688 - val_loss: 95.3067\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 22070.8477 - val_loss: 97.8862\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 52157.7852 - val_loss: 98.8112\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 43614.6133 - val_loss: 95.6966\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 51159.6367 - val_loss: 94.7465\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 42240.4883 - val_loss: 96.0132\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 23334.8730 - val_loss: 99.3082\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 68500.4688 - val_loss: 100.5378\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 65383.0000 - val_loss: 99.0397\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27977.8594 - val_loss: 96.8765\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 37248.8867 - val_loss: 96.5602\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 31940.5684 - val_loss: 98.0055\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1865.8008 - val_loss: 97.9214\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9481.3242 - val_loss: 98.5181\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14257.6973 - val_loss: 98.4464\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 1688.7246 - val_loss: 96.9073\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 41317.6953 - val_loss: 96.6362\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 31866.3359 - val_loss: 97.4682\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5301.8789 - val_loss: 99.3693\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 42222.7266 - val_loss: 100.1512\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 42860.3750 - val_loss: 99.5238\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25749.2363 - val_loss: 97.7701\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18035.1309 - val_loss: 97.8546\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18314.1934 - val_loss: 98.9935\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13101.0605 - val_loss: 98.9428\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2205.1914 - val_loss: 97.6589\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 30414.4863 - val_loss: 97.6225\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 25413.1367 - val_loss: 98.6272\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1231.1666 - val_loss: 98.5593\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5436.0137 - val_loss: 99.3365\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21770.1816 - val_loss: 99.4890\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2144.4111 - val_loss: 98.9360\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2021.1123 - val_loss: 98.6284\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9930.8848 - val_loss: 98.7534\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3429.7271 - val_loss: 100.7132\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 49334.6094 - val_loss: 101.1069\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 51709.8477 - val_loss: 100.7818\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 27694.6309 - val_loss: 99.8449\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5047.1582 - val_loss: 98.8487\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8046.3257 - val_loss: 99.4440\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9077.2939 - val_loss: 99.2337\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 592.9227 - val_loss: 98.1539\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36389.2031 - val_loss: 97.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 83 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 68.6353 - val_loss: 45.8238\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 25.2564 - val_loss: 8.6104\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 17.4770 - val_loss: 4.8480\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10.8903 - val_loss: 18.4724\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11.1217 - val_loss: 10.3940\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.5581 - val_loss: 4.4626\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.3042 - val_loss: 10.4599\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.5346 - val_loss: 10.6774\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.1863 - val_loss: 5.5791\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.6161 - val_loss: 6.8564\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.2780 - val_loss: 9.9278\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5.5572 - val_loss: 4.7021\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1281 - val_loss: 7.8114\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.5008 - val_loss: 4.6192\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.8937 - val_loss: 5.0588\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.6964 - val_loss: 3.3453\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.4298 - val_loss: 3.1010\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.4348 - val_loss: 3.7847\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.1941 - val_loss: 3.1850\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.1685 - val_loss: 3.3429\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 3.1322 - val_loss: 3.3884\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.0083 - val_loss: 3.4828\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 2.9769 - val_loss: 4.1125\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.2053 - val_loss: 3.1890\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.9675 - val_loss: 3.5024\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 2.9562 - val_loss: 3.1915\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 2.8391 - val_loss: 4.0655\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 3.0971 - val_loss: 3.0875\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.9468 - val_loss: 3.0213\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.0769 - val_loss: 4.4960\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 3.3245 - val_loss: 3.0493\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.9957 - val_loss: 3.0480\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 2.9434 - val_loss: 3.7029\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 2.7643 - val_loss: 3.1907\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.7049 - val_loss: 3.8088\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 3.1683 - val_loss: 3.1477\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.9195 - val_loss: 3.0018\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.7985 - val_loss: 4.2572\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 3.0962 - val_loss: 3.0466\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.6969 - val_loss: 3.1888\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 2.5470 - val_loss: 3.0591\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.6201 - val_loss: 3.1580\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.5709 - val_loss: 3.7264\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.6177 - val_loss: 3.0466\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.6609 - val_loss: 5.6293\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 3.1564 - val_loss: 3.4365\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 3.5747 - val_loss: 4.3238\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.7488 - val_loss: 2.9518\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 2.7450 - val_loss: 3.4742\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 2.5486 - val_loss: 3.1288\n",
      "Iteration number 84 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 72.2573 - val_loss: 47.9791\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 38.2689 - val_loss: 28.2932\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 31.9743 - val_loss: 32.4904\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 28.5371 - val_loss: 25.0535\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 23.3835 - val_loss: 13.5004\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 19.2446 - val_loss: 17.2707\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 16.8536 - val_loss: 8.9605\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 14.9693 - val_loss: 11.0826\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 15.0679 - val_loss: 8.7516\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 14.6718 - val_loss: 9.5742\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.0408 - val_loss: 9.0219\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.9152 - val_loss: 9.7535\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.9992 - val_loss: 8.5984\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 14.0424 - val_loss: 10.0482\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 14.1986 - val_loss: 8.9013\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.2010 - val_loss: 8.7935\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13.5062 - val_loss: 9.8404\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.9900 - val_loss: 8.7452\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12.9179 - val_loss: 10.1217\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12.9109 - val_loss: 8.9038\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 12.6690 - val_loss: 9.0936\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 12.4765 - val_loss: 9.4930\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.3850 - val_loss: 9.1027\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.4109 - val_loss: 9.2408\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12.1419 - val_loss: 9.2243\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.1381 - val_loss: 9.2796\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.0157 - val_loss: 9.4440\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.8825 - val_loss: 9.0080\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.8589 - val_loss: 9.7187\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.8447 - val_loss: 8.9284\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.6473 - val_loss: 8.5715\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.6121 - val_loss: 8.7940\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11.6197 - val_loss: 9.0275\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.6298 - val_loss: 9.0065\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.3306 - val_loss: 8.9658\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.2317 - val_loss: 8.6054\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.0661 - val_loss: 8.6609\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.0805 - val_loss: 8.5856\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.1432 - val_loss: 8.9185\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.9008 - val_loss: 8.6358\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.8736 - val_loss: 8.6521\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.7463 - val_loss: 8.3223\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.8434 - val_loss: 8.7882\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.5864 - val_loss: 8.4669\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.8603 - val_loss: 8.7017\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.9144 - val_loss: 8.7638\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.4398 - val_loss: 8.5130\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.3389 - val_loss: 8.7472\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10.5456 - val_loss: 8.3214\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.5009 - val_loss: 8.4343\n",
      "Iteration number 85 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 92.0944 - val_loss: 72.3022\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 48.8374 - val_loss: 30.5180\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 23.9352 - val_loss: 8.7195\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21.6231 - val_loss: 20.6585\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17.8485 - val_loss: 23.9333\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15.9680 - val_loss: 15.9437\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13.3348 - val_loss: 11.0229\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.3256 - val_loss: 13.8207\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.5030 - val_loss: 11.8582\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.9126 - val_loss: 7.4279\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.2655 - val_loss: 10.7824\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.4361 - val_loss: 6.5505\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.6955 - val_loss: 6.7571\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.9932 - val_loss: 6.6504\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6305 - val_loss: 6.3735\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.9382 - val_loss: 6.0918\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.3767 - val_loss: 6.0723\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.1834 - val_loss: 6.2480\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.1387 - val_loss: 7.0018\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.3420 - val_loss: 6.7994\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 5.2832 - val_loss: 6.1947\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.7556 - val_loss: 6.3873\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.5736 - val_loss: 6.6333\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.3445 - val_loss: 6.1958\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.1350 - val_loss: 6.1740\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.4657 - val_loss: 6.6766\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.4687 - val_loss: 6.0520\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1527 - val_loss: 6.2365\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.9852 - val_loss: 6.1237\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7229 - val_loss: 6.1676\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7349 - val_loss: 6.3040\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.6847 - val_loss: 6.1393\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.6273 - val_loss: 6.1189\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7747 - val_loss: 6.0657\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.5970 - val_loss: 6.5079\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.3141 - val_loss: 6.2534\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1932 - val_loss: 6.1815\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8546 - val_loss: 6.3549\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7936 - val_loss: 6.1376\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.8416 - val_loss: 6.2947\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.0029 - val_loss: 6.3220\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.5016 - val_loss: 6.2028\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.3610 - val_loss: 6.1161\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.2234 - val_loss: 6.1912\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.1754 - val_loss: 5.9965\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.2458 - val_loss: 6.1134\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.4866 - val_loss: 5.9142\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.2759 - val_loss: 6.0368\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.0615 - val_loss: 6.1611\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0961 - val_loss: 5.7699\n",
      "Iteration number 86 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 87.9591 - val_loss: 68.5423\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 40.8277 - val_loss: 23.7929\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 28.2103 - val_loss: 14.1403\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 22.0472 - val_loss: 26.4425\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19.8618 - val_loss: 24.0904\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 16.6446 - val_loss: 14.7270\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15.1573 - val_loss: 13.5759\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.9743 - val_loss: 14.4066\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.3766 - val_loss: 6.5209\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.1045 - val_loss: 10.4619\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.8227 - val_loss: 6.9801\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3640 - val_loss: 8.2968\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.9786 - val_loss: 6.8198\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.4794 - val_loss: 7.0296\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.3827 - val_loss: 7.1655\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.0980 - val_loss: 7.3106\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.9084 - val_loss: 7.4039\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8470 - val_loss: 7.1938\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.7936 - val_loss: 7.0690\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.7475 - val_loss: 7.0290\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.6463 - val_loss: 7.1701\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.3910 - val_loss: 7.1285\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.2389 - val_loss: 7.1225\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2039 - val_loss: 6.9388\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.3255 - val_loss: 7.4845\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.3039 - val_loss: 7.2449\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 47ms/step - loss: 8.1708 - val_loss: 7.5180\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.4135 - val_loss: 6.9348\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.9756 - val_loss: 6.8656\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.7930 - val_loss: 6.7934\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7592 - val_loss: 6.7821\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.8751 - val_loss: 6.7483\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.3769 - val_loss: 7.1943\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.2512 - val_loss: 6.2240\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.5956 - val_loss: 6.3671\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6354 - val_loss: 6.5472\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4300 - val_loss: 6.3674\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.3047 - val_loss: 6.6485\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4981 - val_loss: 6.4722\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.3105 - val_loss: 6.4560\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.1801 - val_loss: 6.0332\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.6666 - val_loss: 6.0258\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.2087 - val_loss: 6.5399\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.9771 - val_loss: 5.8511\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4324 - val_loss: 5.8613\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.1905 - val_loss: 8.1786\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.7048 - val_loss: 5.8204\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.2820 - val_loss: 6.7009\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.8989 - val_loss: 5.4698\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.8863 - val_loss: 5.5369\n",
      "Iteration number 87 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 54294.4844 - val_loss: 126.8910\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 206543.9531 - val_loss: 132.3884\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 138083.7969 - val_loss: 122.0973\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33064.0117 - val_loss: 112.4515\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 105382.3828 - val_loss: 108.7240\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 105406.1172 - val_loss: 108.5867\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 77459.6250 - val_loss: 112.6749\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5518.9492 - val_loss: 117.5034\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 112518.9453 - val_loss: 118.5027\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 111122.6562 - val_loss: 115.7361\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 56131.7031 - val_loss: 113.2323\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 15025.4277 - val_loss: 109.4034\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 38020.9844 - val_loss: 108.2427\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 48171.8672 - val_loss: 108.8130\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 20412.7734 - val_loss: 111.0328\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10202.6689 - val_loss: 111.7896\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13708.2842 - val_loss: 110.4905\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 17494.7090 - val_loss: 110.5554\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2856.1897 - val_loss: 112.2797\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22152.0566 - val_loss: 112.1598\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19644.1094 - val_loss: 111.5410\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13211.6846 - val_loss: 110.2348\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11201.3311 - val_loss: 111.6815\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 19376.6094 - val_loss: 111.4137\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6236.7568 - val_loss: 110.2398\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15803.2109 - val_loss: 110.0036\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10720.8262 - val_loss: 110.8475\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 14577.6973 - val_loss: 111.1239\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7530.4014 - val_loss: 109.4036\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 25890.2578 - val_loss: 109.4181\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19076.8965 - val_loss: 111.1928\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 22794.4434 - val_loss: 111.1910\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9252.0000 - val_loss: 109.9353\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 13043.2686 - val_loss: 109.6244\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9549.1162 - val_loss: 110.3698\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14319.0771 - val_loss: 110.5961\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8758.5869 - val_loss: 108.7106\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 33054.7148 - val_loss: 108.5374\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20491.9199 - val_loss: 109.5761\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11743.3623 - val_loss: 110.2663\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4459.8052 - val_loss: 109.2175\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 30241.5586 - val_loss: 108.2703\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11936.1123 - val_loss: 109.5291\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1183.7706 - val_loss: 112.9857\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 88576.2578 - val_loss: 114.2869\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 90017.8750 - val_loss: 113.4870\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 73866.7266 - val_loss: 111.0481\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 27172.3203 - val_loss: 108.6318\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9100.8066 - val_loss: 107.7799\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16741.6562 - val_loss: 107.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 88 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 107001.8594 - val_loss: 90.6555\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 105387.3906 - val_loss: 90.9240\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 50552.3906 - val_loss: 87.2553\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 40859.9570 - val_loss: 86.2518\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16587.6484 - val_loss: 87.6554\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33927.6797 - val_loss: 89.3097\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 37852.3164 - val_loss: 88.4736\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 507.6433 - val_loss: 85.4062\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 72444.0859 - val_loss: 84.4380\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 69299.8438 - val_loss: 86.1490\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 18689.3984 - val_loss: 88.7096\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10439.2842 - val_loss: 89.4301\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23455.9043 - val_loss: 88.8128\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 15060.8867 - val_loss: 88.6442\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5217.5171 - val_loss: 88.6891\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6728.9570 - val_loss: 89.4197\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12147.9756 - val_loss: 88.6049\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 15950.3682 - val_loss: 89.1563\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4226.2446 - val_loss: 88.4822\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 27152.7441 - val_loss: 88.5498\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2490.4839 - val_loss: 90.0955\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 20980.1270 - val_loss: 90.3530\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 26336.5762 - val_loss: 89.3314\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 19303.7715 - val_loss: 88.9700\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12793.9961 - val_loss: 89.7784\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6416.4839 - val_loss: 87.5821\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 57809.1562 - val_loss: 87.5092\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 53548.9570 - val_loss: 88.6002\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15086.6729 - val_loss: 90.2456\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7258.2739 - val_loss: 90.6886\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17691.6816 - val_loss: 90.2258\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11727.6670 - val_loss: 90.0962\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4069.5007 - val_loss: 90.1157\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18681.3066 - val_loss: 89.9041\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4770.8315 - val_loss: 90.3928\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3994.0476 - val_loss: 90.8102\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14153.4902 - val_loss: 90.5576\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5522.7095 - val_loss: 90.5771\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8991.0859 - val_loss: 90.7812\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5630.2671 - val_loss: 90.2745\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7859.8882 - val_loss: 90.4560\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 2071.4548 - val_loss: 91.0255\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 27249.3066 - val_loss: 91.4542\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20460.3418 - val_loss: 90.3932\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 27548.4844 - val_loss: 89.9436\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10037.6260 - val_loss: 91.1344\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 16307.8486 - val_loss: 91.5771\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 19736.0156 - val_loss: 91.2837\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4593.2856 - val_loss: 91.0573\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4793.6001 - val_loss: 90.8243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 89 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 103.0272 - val_loss: 85.9301\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 55.6932 - val_loss: 47.9190\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 41.8082 - val_loss: 18.4834\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 35.4281 - val_loss: 33.7702\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 29.9395 - val_loss: 38.0934\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 27.2474 - val_loss: 29.8430\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 22.5658 - val_loss: 16.0237\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 18.9269 - val_loss: 19.1280\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17.0386 - val_loss: 16.4915\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 15.0223 - val_loss: 9.1820\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 13.8197 - val_loss: 8.0391\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.9050 - val_loss: 6.9575\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.4310 - val_loss: 7.3388\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.1103 - val_loss: 6.5842\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.7515 - val_loss: 6.8750\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.2686 - val_loss: 6.1494\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.3506 - val_loss: 8.4272\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.1798 - val_loss: 7.6118\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.0041 - val_loss: 5.9088\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.6783 - val_loss: 8.9296\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.5105 - val_loss: 6.1185\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.1234 - val_loss: 9.9301\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.3192 - val_loss: 5.5984\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.0876 - val_loss: 8.7986\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.5390 - val_loss: 5.4715\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.5538 - val_loss: 7.0291\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 10.0157 - val_loss: 6.0813\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.1607 - val_loss: 6.4220\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.0429 - val_loss: 5.6839\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.9068 - val_loss: 5.9950\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5414 - val_loss: 5.6804\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.8276 - val_loss: 7.2859\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.4781 - val_loss: 5.3656\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.5417 - val_loss: 6.6708\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.4241 - val_loss: 5.3468\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 7.8991 - val_loss: 6.1566\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8544 - val_loss: 5.4292\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.4036 - val_loss: 5.5112\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.2075 - val_loss: 5.4037\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.1460 - val_loss: 5.3502\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0224 - val_loss: 5.7879\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.3994 - val_loss: 5.5340\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.8748 - val_loss: 6.3030\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.8556 - val_loss: 5.1299\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0561 - val_loss: 5.2591\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.7143 - val_loss: 6.5778\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.4265 - val_loss: 5.5179\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.7555 - val_loss: 4.7588\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.5765 - val_loss: 6.3983\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.0276 - val_loss: 4.9659\n",
      "Iteration number 90 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 233667.8438 - val_loss: 86.4621\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 60440.4258 - val_loss: 94.3506\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 20806.5059 - val_loss: 84.4627\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 115181.3047 - val_loss: 83.0247\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 110077.0938 - val_loss: 87.6391\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11124.3467 - val_loss: 91.5178\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 33761.2266 - val_loss: 94.2757\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 62467.9727 - val_loss: 94.1315\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 12171.1064 - val_loss: 89.1174\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 59731.2031 - val_loss: 86.6934\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 63946.3359 - val_loss: 89.4368\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2148.6748 - val_loss: 91.9646\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15734.5645 - val_loss: 93.4351\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 37586.8594 - val_loss: 91.5396\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 162.9455 - val_loss: 92.2126\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 35558.1055 - val_loss: 92.6731\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 25259.2129 - val_loss: 87.5965\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 83458.7031 - val_loss: 85.8639\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 73641.3516 - val_loss: 87.1984\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 59596.5391 - val_loss: 92.9876\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 52161.0000 - val_loss: 94.8001\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 54233.3438 - val_loss: 92.9144\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12036.4766 - val_loss: 90.4052\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 45831.3750 - val_loss: 89.2592\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 34714.8398 - val_loss: 90.5140\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7327.6211 - val_loss: 92.3892\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6824.4150 - val_loss: 90.3966\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 35664.7070 - val_loss: 90.5277\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 22042.2480 - val_loss: 92.8887\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13410.2422 - val_loss: 92.7045\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9603.5020 - val_loss: 90.7290\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 32945.7617 - val_loss: 90.8171\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20118.5840 - val_loss: 93.0787\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 36693.6250 - val_loss: 94.1290\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 17154.1738 - val_loss: 92.1226\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3973.9382 - val_loss: 91.7787\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12262.5469 - val_loss: 92.7445\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 21807.3242 - val_loss: 93.3547\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4189.5425 - val_loss: 90.8419\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 57950.9727 - val_loss: 89.3503\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 47200.8477 - val_loss: 91.4251\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 20477.1621 - val_loss: 94.8449\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 51874.8047 - val_loss: 95.8110\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 51052.1055 - val_loss: 94.5850\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15887.5508 - val_loss: 92.9353\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17345.2207 - val_loss: 92.1696\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 16763.2559 - val_loss: 93.2140\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9628.6904 - val_loss: 93.3712\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 851.5771 - val_loss: 93.9292\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18839.1309 - val_loss: 93.8039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 91 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 81.4377 - val_loss: 64.9638\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 63.2220 - val_loss: 45.1406\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 47.2901 - val_loss: 40.4612\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 33.5635 - val_loss: 29.3958\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 26.5900 - val_loss: 23.9730\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 19.5805 - val_loss: 15.8311\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15.8111 - val_loss: 6.1716\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.7136 - val_loss: 8.0698\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13.2877 - val_loss: 6.0867\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13.0285 - val_loss: 7.4214\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.7933 - val_loss: 5.4612\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.6407 - val_loss: 5.9310\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.3808 - val_loss: 6.0161\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.1859 - val_loss: 5.4810\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.2203 - val_loss: 5.9448\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.9204 - val_loss: 5.3295\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.6823 - val_loss: 6.1221\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.7337 - val_loss: 5.1315\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 11.0601 - val_loss: 6.0994\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.1460 - val_loss: 5.1559\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3087 - val_loss: 5.8298\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.4474 - val_loss: 4.9130\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.9969 - val_loss: 6.2739\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.5531 - val_loss: 5.0146\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.0262 - val_loss: 4.9214\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.0030 - val_loss: 4.8747\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.0617 - val_loss: 5.1850\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.8992 - val_loss: 4.4948\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.0353 - val_loss: 5.5373\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.6407 - val_loss: 4.5279\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3114 - val_loss: 5.2289\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.7120 - val_loss: 4.3911\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.4832 - val_loss: 6.1034\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.1993 - val_loss: 4.4048\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.3346 - val_loss: 5.7801\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.7057 - val_loss: 4.2492\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.2827 - val_loss: 6.1318\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.5472 - val_loss: 4.2199\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.6360 - val_loss: 4.2161\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.8134 - val_loss: 4.7357\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.0686 - val_loss: 4.2676\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.5387 - val_loss: 5.5323\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.5183 - val_loss: 4.0751\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.2446 - val_loss: 4.8095\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.0910 - val_loss: 4.1401\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.9037 - val_loss: 4.2596\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.7130 - val_loss: 4.1805\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.1119 - val_loss: 4.0677\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.9320 - val_loss: 4.0820\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.6598 - val_loss: 4.4690\n",
      "Iteration number 92 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 82.4664 - val_loss: 57.1688\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 33.2848 - val_loss: 4.7387\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 29.1305 - val_loss: 3.4930\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 21.5363 - val_loss: 18.7431\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 18.3185 - val_loss: 21.7279\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 16.4887 - val_loss: 10.2618\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 14.7273 - val_loss: 6.2584\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 14.3712 - val_loss: 9.1731\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 12.7935 - val_loss: 7.7996\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11.8171 - val_loss: 4.2385\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.5737 - val_loss: 4.2670\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.1691 - val_loss: 5.9568\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.9517 - val_loss: 6.4570\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 10.7611 - val_loss: 5.6193\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 10.1360 - val_loss: 4.2447\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.5096 - val_loss: 4.2501\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.1343 - val_loss: 5.1395\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.5790 - val_loss: 4.7539\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.5603 - val_loss: 4.5607\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.2367 - val_loss: 4.2935\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.8510 - val_loss: 4.2941\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.6979 - val_loss: 4.3011\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.5282 - val_loss: 4.2077\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.3620 - val_loss: 4.1864\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.3097 - val_loss: 4.2314\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.0426 - val_loss: 4.0278\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.6142 - val_loss: 3.9643\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.6464 - val_loss: 3.9816\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.8216 - val_loss: 4.0863\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 6.3784 - val_loss: 4.2102\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.7581 - val_loss: 4.0027\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.1105 - val_loss: 3.8028\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.9511 - val_loss: 3.7374\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.9950 - val_loss: 3.6624\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.1315 - val_loss: 3.9771\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.1088 - val_loss: 3.5810\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.5145 - val_loss: 3.6239\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.1756 - val_loss: 3.4279\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.9635 - val_loss: 4.4386\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.0433 - val_loss: 3.6442\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.5155 - val_loss: 3.9089\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.1019 - val_loss: 3.5955\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 4.9412 - val_loss: 3.5155\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0565 - val_loss: 5.0671\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.2137 - val_loss: 4.3043\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.3991 - val_loss: 5.0891\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 4.7799 - val_loss: 3.6689\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0621 - val_loss: 5.1317\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0588 - val_loss: 4.3059\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.0020 - val_loss: 3.8410\n",
      "Iteration number 93 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 78.9525 - val_loss: 62.2533\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 57.1200 - val_loss: 38.8629\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 46.5969 - val_loss: 47.5958\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 39.3895 - val_loss: 48.1978\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 35.3070 - val_loss: 33.3600\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 29.7946 - val_loss: 15.7031\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 25.6277 - val_loss: 19.2102\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 23.5679 - val_loss: 11.3379\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 21.6519 - val_loss: 8.5921\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 18.8893 - val_loss: 13.9124\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18.7425 - val_loss: 8.3624\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 16.6947 - val_loss: 10.3321\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 16.0103 - val_loss: 4.7386\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 15.4264 - val_loss: 5.6733\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14.7802 - val_loss: 3.8450\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 14.3932 - val_loss: 4.4864\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 14.0562 - val_loss: 4.8085\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 14.6229 - val_loss: 3.4949\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 14.1290 - val_loss: 5.0367\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12.4317 - val_loss: 3.5442\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 13.2945 - val_loss: 5.2021\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.1655 - val_loss: 2.9038\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11.6431 - val_loss: 3.1979\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 11.3919 - val_loss: 3.5680\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11.1859 - val_loss: 3.7111\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.9237 - val_loss: 3.1290\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 10.3399 - val_loss: 2.7616\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.2216 - val_loss: 2.9074\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 9.9798 - val_loss: 3.4936\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 10.0190 - val_loss: 2.7368\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10.0638 - val_loss: 2.5153\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.6022 - val_loss: 2.7798\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.4292 - val_loss: 2.8378\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.5586 - val_loss: 4.4120\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.8801 - val_loss: 2.3101\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.4338 - val_loss: 2.9518\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.6258 - val_loss: 2.4030\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.4520 - val_loss: 2.3613\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.1508 - val_loss: 2.3759\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8572 - val_loss: 2.4627\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.6929 - val_loss: 2.3397\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.6538 - val_loss: 2.2509\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.2912 - val_loss: 1.9816\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.4342 - val_loss: 2.0840\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.9189 - val_loss: 2.7172\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.2589 - val_loss: 2.0964\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 8.5102 - val_loss: 3.5603\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 7.3698 - val_loss: 3.4185\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.4998 - val_loss: 2.3979\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 7.1525 - val_loss: 2.5755\n",
      "Iteration number 94 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 82.3479 - val_loss: 52.9908\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 37.3874 - val_loss: 8.1901\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 34.8584 - val_loss: 16.5900\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 25.7291 - val_loss: 27.4907\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 24.4550 - val_loss: 21.9418\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 20.6308 - val_loss: 10.2910\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 17.9991 - val_loss: 6.2277\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16.3309 - val_loss: 7.1255\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.3081 - val_loss: 10.1241\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 12.9089 - val_loss: 6.7238\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.3216 - val_loss: 6.3240\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 10.0580 - val_loss: 6.7395\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.1517 - val_loss: 6.4106\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 8.2936 - val_loss: 6.3964\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.9018 - val_loss: 6.1554\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.5539 - val_loss: 5.9371\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 7.3637 - val_loss: 5.6108\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 7.3903 - val_loss: 5.6784\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.7198 - val_loss: 5.2401\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.2424 - val_loss: 5.4937\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.3111 - val_loss: 5.2811\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.3574 - val_loss: 5.0810\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.9682 - val_loss: 5.3560\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.8024 - val_loss: 4.9551\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.8842 - val_loss: 4.8891\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.7902 - val_loss: 4.8795\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 6.6236 - val_loss: 4.6960\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.9495 - val_loss: 4.6127\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.7602 - val_loss: 5.6121\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.6133 - val_loss: 4.4571\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.7587 - val_loss: 4.4773\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.6080 - val_loss: 4.5615\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.6432 - val_loss: 5.4872\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 7.1419 - val_loss: 4.7445\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 7.0705 - val_loss: 4.9941\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.3235 - val_loss: 4.2989\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.2311 - val_loss: 4.3943\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.0758 - val_loss: 4.3613\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 5.9062 - val_loss: 4.9169\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.3532 - val_loss: 4.0584\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.9922 - val_loss: 4.2947\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 5.8027 - val_loss: 3.9285\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.0395 - val_loss: 4.1605\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 5.8353 - val_loss: 3.8723\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.0792 - val_loss: 4.2578\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 6.4768 - val_loss: 3.6939\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 6.1887 - val_loss: 3.6784\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.8043 - val_loss: 3.9899\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.5550 - val_loss: 3.8862\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.5953 - val_loss: 3.5889\n",
      "Iteration number 95 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 86.8824 - val_loss: 61.9417\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 44.8840 - val_loss: 18.7138\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 44.3336 - val_loss: 25.9627\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 32.6207 - val_loss: 38.7644\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 30.7286 - val_loss: 31.7202\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 24.9110 - val_loss: 17.8881\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 24.3166 - val_loss: 17.6463\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20.9173 - val_loss: 22.0028\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 18.9467 - val_loss: 13.5388\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 16.6908 - val_loss: 10.1817\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 14.6795 - val_loss: 7.9450\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13.7790 - val_loss: 6.8340\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 14.0132 - val_loss: 6.4923\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13.0823 - val_loss: 6.2234\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.7118 - val_loss: 5.7957\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.8278 - val_loss: 6.5898\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.5070 - val_loss: 5.4230\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.9729 - val_loss: 6.3854\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.8397 - val_loss: 5.0101\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.7849 - val_loss: 5.2779\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11.4977 - val_loss: 5.1465\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.6840 - val_loss: 5.0716\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.3718 - val_loss: 5.8916\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.4902 - val_loss: 4.7699\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.1894 - val_loss: 5.3780\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.6113 - val_loss: 4.4317\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.9999 - val_loss: 5.1006\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.6387 - val_loss: 4.4913\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.5329 - val_loss: 4.8454\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.3872 - val_loss: 5.2775\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.0697 - val_loss: 4.4358\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.0761 - val_loss: 5.0371\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.9135 - val_loss: 5.0642\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.8213 - val_loss: 4.7186\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.7575 - val_loss: 4.3265\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.9207 - val_loss: 6.0083\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.3754 - val_loss: 4.0952\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.9881 - val_loss: 6.7345\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.1150 - val_loss: 4.0414\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.0178 - val_loss: 5.0876\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.5267 - val_loss: 4.1850\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.2316 - val_loss: 6.8488\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.1326 - val_loss: 3.9192\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.1410 - val_loss: 6.2194\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.3013 - val_loss: 3.9264\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.9158 - val_loss: 5.1355\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.1528 - val_loss: 3.9820\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.8006 - val_loss: 4.5535\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.8464 - val_loss: 4.2904\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.5568 - val_loss: 4.2012\n",
      "Iteration number 96 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 64.6233 - val_loss: 43.3798\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 21.3654 - val_loss: 2.0146\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 23.6903 - val_loss: 16.2864\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.2499 - val_loss: 26.1644\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 13.4067 - val_loss: 16.6097\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.2644 - val_loss: 13.1373\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 10.6262 - val_loss: 19.0049\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 10.4855 - val_loss: 15.0002\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.2710 - val_loss: 11.0452\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 7.7778 - val_loss: 11.7262\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.5571 - val_loss: 9.7140\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.3952 - val_loss: 5.1319\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.2014 - val_loss: 5.9110\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.8353 - val_loss: 7.0777\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.6239 - val_loss: 6.6326\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.4695 - val_loss: 5.2626\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.5883 - val_loss: 6.4337\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3748 - val_loss: 6.8880\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.3384 - val_loss: 4.2751\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.1411 - val_loss: 4.6652\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.9537 - val_loss: 5.5878\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.9213 - val_loss: 3.1868\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.3811 - val_loss: 6.9770\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.3641 - val_loss: 2.5631\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.8975 - val_loss: 3.3780\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.6990 - val_loss: 3.0315\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.7022 - val_loss: 2.3486\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.5093 - val_loss: 2.4673\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.3698 - val_loss: 3.2326\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.3181 - val_loss: 2.4925\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.2269 - val_loss: 2.3187\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.1124 - val_loss: 2.6052\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.1175 - val_loss: 2.5027\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.0730 - val_loss: 2.5038\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.0373 - val_loss: 2.3769\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.9374 - val_loss: 2.5268\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.0658 - val_loss: 2.7526\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.0122 - val_loss: 2.7151\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.0452 - val_loss: 2.4792\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.3613 - val_loss: 2.6543\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 4.0041 - val_loss: 2.5166\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.7412 - val_loss: 2.8258\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7472 - val_loss: 2.5389\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.2131 - val_loss: 2.2657\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.2935 - val_loss: 3.9414\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.3495 - val_loss: 2.8861\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.4801 - val_loss: 2.1896\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.9015 - val_loss: 2.4268\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.6016 - val_loss: 2.0931\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.7201 - val_loss: 2.7753\n",
      "Iteration number 97 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 59.0739 - val_loss: 27.2121\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 23.6459 - val_loss: 9.5762\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 18.5173 - val_loss: 14.6649\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 16.3942 - val_loss: 21.1252\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 15.7080 - val_loss: 12.8821\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 13.1847 - val_loss: 4.9206\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.7194 - val_loss: 3.2108\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.4724 - val_loss: 8.3614\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 10.5505 - val_loss: 10.0552\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.6913 - val_loss: 4.7475\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.1325 - val_loss: 4.3882\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.4429 - val_loss: 4.9653\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.2386 - val_loss: 2.3698\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.7171 - val_loss: 3.3754\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.3161 - val_loss: 2.2035\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.4063 - val_loss: 2.1469\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.1561 - val_loss: 3.2369\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.7817 - val_loss: 4.2338\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1499 - val_loss: 5.9175\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.9584 - val_loss: 2.9851\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.1599 - val_loss: 2.3788\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.8522 - val_loss: 2.7602\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.6063 - val_loss: 2.4603\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.8341 - val_loss: 4.8271\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.8854 - val_loss: 2.2561\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.7997 - val_loss: 2.6954\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.5971 - val_loss: 3.6565\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.4877 - val_loss: 2.3242\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.5472 - val_loss: 2.8424\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.3378 - val_loss: 3.1067\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.4130 - val_loss: 2.2050\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.6684 - val_loss: 2.3028\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.5235 - val_loss: 4.5974\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.8309 - val_loss: 2.7007\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.9785 - val_loss: 3.7585\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.5642 - val_loss: 3.0377\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.3361 - val_loss: 2.7500\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.2624 - val_loss: 3.3117\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.4070 - val_loss: 3.4228\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.2059 - val_loss: 2.5356\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.0159 - val_loss: 3.0341\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.0649 - val_loss: 3.7579\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.1627 - val_loss: 2.2471\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.5340 - val_loss: 2.2354\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.4245 - val_loss: 4.0707\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.2680 - val_loss: 2.2003\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.2660 - val_loss: 2.4562\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.1194 - val_loss: 4.3516\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.1105 - val_loss: 2.2590\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.9791 - val_loss: 2.8203\n",
      "Iteration number 98 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 79.5139 - val_loss: 55.2971\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 39.2077 - val_loss: 19.0878\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 36.6320 - val_loss: 17.5495\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 25.5340 - val_loss: 28.0909\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 25.8569 - val_loss: 26.9918\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 18.7686 - val_loss: 13.5672\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 18.0734 - val_loss: 13.3827\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 15.4513 - val_loss: 17.3891\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.8438 - val_loss: 10.7091\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 14.1505 - val_loss: 11.2756\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13.7297 - val_loss: 11.8902\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 13.0438 - val_loss: 10.3246\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 13.2701 - val_loss: 11.8762\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 13.1814 - val_loss: 10.4301\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 12.7875 - val_loss: 11.1597\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 12.5616 - val_loss: 10.6484\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.9038 - val_loss: 10.1119\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11.7668 - val_loss: 10.4197\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.9851 - val_loss: 10.0430\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 11.5324 - val_loss: 10.1196\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11.1598 - val_loss: 9.7204\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 11.2242 - val_loss: 9.9828\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 11.2540 - val_loss: 9.8166\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 10.9587 - val_loss: 9.5290\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 10.6398 - val_loss: 10.6183\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 11.5483 - val_loss: 9.4942\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 11.1938 - val_loss: 9.7581\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 11.0511 - val_loss: 9.4673\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 10.2365 - val_loss: 9.4282\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 10.0667 - val_loss: 9.3084\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 9.9667 - val_loss: 9.0660\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 9.8745 - val_loss: 9.0305\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 9.7831 - val_loss: 9.2511\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 9.7860 - val_loss: 9.0786\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.7565 - val_loss: 9.2736\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.8877 - val_loss: 8.8870\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 9.6229 - val_loss: 8.8313\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 9.1782 - val_loss: 9.2000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9.3260 - val_loss: 9.4376\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 9.6522 - val_loss: 8.6333\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 9.4337 - val_loss: 8.7246\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 8.9800 - val_loss: 8.8223\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 8.5078 - val_loss: 8.6251\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8.8349 - val_loss: 8.3485\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8.6514 - val_loss: 9.0915\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.2723 - val_loss: 8.2840\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.5270 - val_loss: 8.5837\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 8.1592 - val_loss: 8.7810\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.9326 - val_loss: 8.3951\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 7.5432 - val_loss: 8.2402\n",
      "Iteration number 99 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 44859.2617 - val_loss: 113.9239\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 314769.7500 - val_loss: 116.8645\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 249520.4688 - val_loss: 108.9129\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 70469.5234 - val_loss: 102.3528\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 35531.4180 - val_loss: 99.4528\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 38686.9414 - val_loss: 101.3177\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 34457.6992 - val_loss: 102.5341\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 23761.2051 - val_loss: 98.1340\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 76219.9062 - val_loss: 97.7250\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 70435.4453 - val_loss: 99.0373\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 37668.1445 - val_loss: 102.9211\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 52693.7773 - val_loss: 104.1043\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 50164.6211 - val_loss: 102.7090\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21420.1133 - val_loss: 99.1738\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 70804.8359 - val_loss: 98.4852\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 62360.5469 - val_loss: 99.7643\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 11826.9619 - val_loss: 102.2336\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 37030.4609 - val_loss: 104.3241\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 52028.9570 - val_loss: 103.6686\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 23566.0762 - val_loss: 101.4544\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 37373.3867 - val_loss: 100.3754\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 31840.9238 - val_loss: 102.2055\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3307.2693 - val_loss: 101.8148\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8996.4785 - val_loss: 102.1404\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 1735.3657 - val_loss: 102.3479\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6983.9185 - val_loss: 101.3363\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 20590.3555 - val_loss: 101.5992\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5481.2202 - val_loss: 103.1621\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 47749.3555 - val_loss: 104.1167\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 30387.3613 - val_loss: 101.8611\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 14004.6562 - val_loss: 101.3700\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 14834.4033 - val_loss: 102.7139\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 26710.1094 - val_loss: 103.0195\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 9334.7959 - val_loss: 101.5930\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 33568.5156 - val_loss: 100.6376\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 25136.8105 - val_loss: 101.4437\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 13841.5186 - val_loss: 105.0096\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 58892.1523 - val_loss: 105.5613\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 69715.9297 - val_loss: 104.3959\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 38552.4609 - val_loss: 102.7778\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 8535.7070 - val_loss: 101.9984\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 183.9397 - val_loss: 101.8403\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 15843.3057 - val_loss: 101.5157\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15098.3008 - val_loss: 102.8868\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 21766.0254 - val_loss: 102.8086\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5222.4556 - val_loss: 101.6904\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 18562.8711 - val_loss: 101.4339\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 16877.5527 - val_loss: 102.8091\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 20300.9531 - val_loss: 102.7542\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6465.9365 - val_loss: 101.4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-250-ff67cb46d433>:76: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 100 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 86.4527 - val_loss: 67.0498\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 45.4784 - val_loss: 23.7865\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 20.5106 - val_loss: 7.8649\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 15.4102 - val_loss: 16.0520\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 12.0854 - val_loss: 22.6734\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 11.7417 - val_loss: 13.8068\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 9.6747 - val_loss: 7.2156\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 9.3905 - val_loss: 11.0047\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.6034 - val_loss: 14.7960\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 8.4803 - val_loss: 10.4247\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8139 - val_loss: 6.7953\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 7.3675 - val_loss: 9.7121\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.9576 - val_loss: 8.6552\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.3563 - val_loss: 6.1750\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.9325 - val_loss: 6.8447\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.1806 - val_loss: 5.9310\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.6551 - val_loss: 5.5650\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.1572 - val_loss: 4.2235\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.8090 - val_loss: 4.4521\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.4730 - val_loss: 3.8965\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 3.8834 - val_loss: 3.9559\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.8282 - val_loss: 3.9052\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.9265 - val_loss: 4.1695\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.8264 - val_loss: 4.0853\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.5955 - val_loss: 4.0645\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.5756 - val_loss: 3.7852\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.3730 - val_loss: 3.7204\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.3207 - val_loss: 3.7427\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 3.2212 - val_loss: 3.7776\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.2724 - val_loss: 3.7766\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.1526 - val_loss: 3.7514\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.0783 - val_loss: 3.7073\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.9954 - val_loss: 3.6980\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3.0401 - val_loss: 3.8431\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.2575 - val_loss: 3.6968\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.6264 - val_loss: 4.1329\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 3.4099 - val_loss: 3.7682\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0481 - val_loss: 3.8796\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0290 - val_loss: 3.5132\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.9226 - val_loss: 3.5365\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0793 - val_loss: 3.5065\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.9908 - val_loss: 3.8154\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 2.8978 - val_loss: 3.6004\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.1036 - val_loss: 3.5368\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 3.0329 - val_loss: 3.6844\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 2.8839 - val_loss: 3.5585\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.9748 - val_loss: 4.1149\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.3842 - val_loss: 4.0823\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.2786 - val_loss: 3.4580\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 3.7837 - val_loss: 4.4859\n",
      "Iteration number 101 finished\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 90.4131 - val_loss: 68.8780\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 41.0040 - val_loss: 29.1619\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 33.4500 - val_loss: 20.4554\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 26.9286 - val_loss: 28.8773\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 23.2213 - val_loss: 32.3073\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 21.5498 - val_loss: 23.9477\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 18.6141 - val_loss: 12.1975\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 17.0971 - val_loss: 14.7351\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 14.7221 - val_loss: 18.3641\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 12.8034 - val_loss: 6.1022\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 12.1468 - val_loss: 10.7718\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.9880 - val_loss: 4.5187\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 10.1527 - val_loss: 7.2414\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.5016 - val_loss: 1.9201\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 9.2688 - val_loss: 6.8725\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.9382 - val_loss: 3.4113\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.3975 - val_loss: 4.5560\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 8.5116 - val_loss: 3.1534\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 8.2735 - val_loss: 2.9224\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.8877 - val_loss: 2.7121\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.7817 - val_loss: 2.5203\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.4659 - val_loss: 3.5087\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.2032 - val_loss: 1.8663\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 7.0252 - val_loss: 3.0787\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 6.8226 - val_loss: 1.7536\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.6476 - val_loss: 2.0165\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 6.5844 - val_loss: 2.9946\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4741 - val_loss: 1.4640\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.4581 - val_loss: 1.9852\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.0710 - val_loss: 2.4100\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 6.3366 - val_loss: 1.9831\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 6.3148 - val_loss: 2.2243\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.7656 - val_loss: 1.4418\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.6915 - val_loss: 1.4956\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.6640 - val_loss: 1.7443\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.5834 - val_loss: 1.6929\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.4359 - val_loss: 1.5012\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.2332 - val_loss: 1.9013\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 5.2658 - val_loss: 1.4804\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0114 - val_loss: 1.3983\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 5.1077 - val_loss: 1.8820\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 5.1390 - val_loss: 2.1386\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 5.0227 - val_loss: 1.7584\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.7834 - val_loss: 2.9629\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 5.0326 - val_loss: 1.5694\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.6020 - val_loss: 1.4432\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 4.9551 - val_loss: 3.0125\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 4.9656 - val_loss: 2.4750\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 4.5766 - val_loss: 2.1038\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 4.8908 - val_loss: 2.0640\n",
      "Iteration number 102 finished\n"
     ]
    }
   ],
   "source": [
    "zipcodes = df_time_series.columns\n",
    "dict_mape_skylar = {}\n",
    "dict_pred_skylar = {}\n",
    "\n",
    "for zipcode in range(len(zipcodes)):\n",
    "\n",
    "    # init a RMM model\n",
    "    rnn_model = Sequential()\n",
    "    # add 4 layers of RNN and a last layer\n",
    "\n",
    "    # we define shape on first layer, (60,1) because we use 60 inputs per prediction\n",
    "    rnn_model.add(LSTM(units= 60, return_sequences = False, input_shape=((60,1))))\n",
    "    #rnn_model.add(Dropout(.1))\n",
    "\n",
    "    # 3 other layers\n",
    "    #rnn_model.add(LSTM(units= 30, return_sequences = True))\n",
    "    #rnn_model.add(Dropout(.1))\n",
    "\n",
    "    # return_sequence is False because we want only 1 output after this layer\n",
    "    #rnn_model.add(LSTM(units= 60, return_sequences = False))\n",
    "    #rnn_model.add(Dropout(.1))\n",
    "\n",
    "    # last layer \n",
    "\n",
    "    rnn_model.add(Dense(units=1))\n",
    "\n",
    "    # compile - because this is a regression model we want to minimize MSE\n",
    "\n",
    "    rnn_model.compile(optimizer='adam', loss='mean_absolute_percentage_error')\n",
    "\n",
    "    # We get only the specific column(Zipcode from our train and test datas)\n",
    "    train_data = train.iloc[:,zipcode:zipcode+1].values.astype(int)\n",
    "    test_data = test.iloc[:,zipcode:zipcode+1].values.astype(int)\n",
    "    \n",
    "    # We are using normalizaion rather than standascaler. \n",
    "    # In a upward trending timeseries it is better to not start from negative\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    train_data_scaled = scaler.fit_transform(train_data)\n",
    "    test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "    # Because we are using 60 previous values to model and predict the next value, \n",
    "    # We set X_train from arrays of 60 for each y_train value\n",
    "    # Same idea for test data sets\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(60,len(train_data_scaled)):\n",
    "        X_train.append(train_data_scaled[i-60:i])\n",
    "        y_train.append(train_data_scaled[i])\n",
    "\n",
    "    data_total = pd.concat((train.iloc[:,zipcode:zipcode+1], test.iloc[:,zipcode:zipcode+1]),axis=0)\n",
    "    inputs = data_total[len(train)-60:].values\n",
    "    inputs = scaler.transform(inputs)\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(60,len(inputs)):\n",
    "        X_test.append(inputs[i-60:i])\n",
    "        y_test.append(inputs[i])\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(test_data)\n",
    "\n",
    "    # We need numpy arrays for our model\n",
    "    X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n",
    "    \n",
    "    # We fit our data to our zipcode specific data\n",
    "    rnn_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, scaler.transform(y_test)))\n",
    "\n",
    "    # Make predictions on the data\n",
    "\n",
    "    y_hat_raw = rnn_model.predict(X_train)\n",
    "    y_hat = scaler.inverse_transform(y_hat_raw)\n",
    "\n",
    "    # Use the score on unseen test data to calculate the MAPE\n",
    "\n",
    "    dict_mape_skylar[zipcodes[zipcode]] = np.mean(np.absolute((y_hat-y_train)/y_train))      \n",
    "\n",
    "    # We get the last 60 values from our test data which is basically last 60 values in the data set\n",
    "    last_60 = df_time_series.iloc[-60:,zipcode:zipcode+1].values.astype(int)\n",
    "    \n",
    "    # Before we use our data we scale it\n",
    "    last_60 = scaler.transform(last_60)\n",
    "    \n",
    "    # Our input should be in (x,60,1) format\n",
    "    x_new_pred = last_60[-60:].reshape(1,60,1)\n",
    "\n",
    "    # make a prediction, add to the last_60 for the next prediction and \n",
    "    y_pred = rnn_model.predict(x_new_pred)\n",
    "\n",
    "    # We add our predition to our list of predictions for zipcode specific predictions list\n",
    "    dict_pred_skylar[zipcodes[zipcode]]=scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    print(f'Iteration number {zipcode} finished')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{89108: array([[89125.06]], dtype=float32),\n",
       " 89121: array([[75994.12]], dtype=float32),\n",
       " 89117: array([[342993.94]], dtype=float32),\n",
       " 89052: array([[421245.44]], dtype=float32),\n",
       " 89123: array([[302137.3]], dtype=float32),\n",
       " 89031: array([[120645.31]], dtype=float32),\n",
       " 89110: array([[86367.11]], dtype=float32),\n",
       " 89074: array([[313313.8]], dtype=float32),\n",
       " 89103: array([[105097.86]], dtype=float32),\n",
       " 89148: array([[304354.1]], dtype=float32),\n",
       " 89147: array([[127211.54]], dtype=float32),\n",
       " 89119: array([[95203.36]], dtype=float32),\n",
       " 89129: array([[136210.56]], dtype=float32),\n",
       " 89122: array([[99995.11]], dtype=float32),\n",
       " 89115: array([[49483.06]], dtype=float32),\n",
       " 89502: array([[103521.21]], dtype=float32),\n",
       " 89014: array([[144321.5]], dtype=float32),\n",
       " 89131: array([[340571.3]], dtype=float32),\n",
       " 89509: array([[442702.12]], dtype=float32),\n",
       " 89436: array([[174319.03]], dtype=float32),\n",
       " 89015: array([[98834.02]], dtype=float32),\n",
       " 89128: array([[138708.28]], dtype=float32),\n",
       " 89523: array([[389421.16]], dtype=float32),\n",
       " 89104: array([[76258.59]], dtype=float32),\n",
       " 89012: array([[345105.72]], dtype=float32),\n",
       " 89030: array([[86262.86]], dtype=float32),\n",
       " 89431: array([[85330.53]], dtype=float32),\n",
       " 89032: array([[86667.86]], dtype=float32),\n",
       " 89506: array([[118694.53]], dtype=float32),\n",
       " 89102: array([[76455.164]], dtype=float32),\n",
       " 89139: array([[142436.7]], dtype=float32),\n",
       " 89149: array([[151599.61]], dtype=float32),\n",
       " 89178: array([[300754.56]], dtype=float32),\n",
       " 89113: array([[304622.88]], dtype=float32),\n",
       " 89521: array([[414650.53]], dtype=float32),\n",
       " 89183: array([[124785.68]], dtype=float32),\n",
       " 89135: array([[421180.38]], dtype=float32),\n",
       " 89107: array([[81638.05]], dtype=float32),\n",
       " 89511: array([[646172.7]], dtype=float32),\n",
       " 89002: array([[308202.84]], dtype=float32),\n",
       " 89130: array([[129408.92]], dtype=float32),\n",
       " 89134: array([[320814.12]], dtype=float32),\n",
       " 89503: array([[119717.45]], dtype=float32),\n",
       " 89081: array([[114632.67]], dtype=float32),\n",
       " 89141: array([[321766.6]], dtype=float32),\n",
       " 89011: array([[272069.34]], dtype=float32),\n",
       " 89142: array([[72033.64]], dtype=float32),\n",
       " 89434: array([[129602.984]], dtype=float32),\n",
       " 89512: array([[115600.35]], dtype=float32),\n",
       " 89145: array([[92809.625]], dtype=float32),\n",
       " 89084: array([[299766.88]], dtype=float32),\n",
       " 89701: array([[265423.44]], dtype=float32),\n",
       " 89801: array([[208460.4]], dtype=float32),\n",
       " 89120: array([[100827.47]], dtype=float32),\n",
       " 89044: array([[350935.9]], dtype=float32),\n",
       " 89156: array([[82416.85]], dtype=float32),\n",
       " 89048: array([[101105.82]], dtype=float32),\n",
       " 89118: array([[148262.86]], dtype=float32),\n",
       " 89706: array([[263521.75]], dtype=float32),\n",
       " 89408: array([[144694.62]], dtype=float32),\n",
       " 89166: array([[292753.38]], dtype=float32),\n",
       " 89144: array([[355562.78]], dtype=float32),\n",
       " 89146: array([[154365.]], dtype=float32),\n",
       " 89027: array([[242222.94]], dtype=float32),\n",
       " 89403: array([[171603.86]], dtype=float32),\n",
       " 89433: array([[102987.836]], dtype=float32),\n",
       " 89005: array([[320821.78]], dtype=float32),\n",
       " 89138: array([[436478.25]], dtype=float32),\n",
       " 89460: array([[326733.5]], dtype=float32),\n",
       " 89423: array([[395983.56]], dtype=float32),\n",
       " 89410: array([[366729.84]], dtype=float32),\n",
       " 89815: array([[215286.89]], dtype=float32),\n",
       " 89029: array([[131214.25]], dtype=float32),\n",
       " 89109: array([[165541.64]], dtype=float32),\n",
       " 89508: array([[150758.38]], dtype=float32),\n",
       " 89703: array([[397177.84]], dtype=float32),\n",
       " 89441: array([[428157.22]], dtype=float32),\n",
       " 89060: array([[78774.56]], dtype=float32),\n",
       " 89143: array([[280067.12]], dtype=float32),\n",
       " 89519: array([[550517.]], dtype=float32),\n",
       " 89447: array([[116557.16]], dtype=float32),\n",
       " 89179: array([[311430.2]], dtype=float32),\n",
       " 89429: array([[109313.07]], dtype=float32),\n",
       " 89061: array([[105543.38]], dtype=float32),\n",
       " 89451: array([[931118.7]], dtype=float32),\n",
       " 89501: array([[343421.28]], dtype=float32),\n",
       " 89705: array([[321901.6]], dtype=float32),\n",
       " 89510: array([[424555.62]], dtype=float32),\n",
       " 89086: array([[120434.48]], dtype=float32),\n",
       " 89448: array([[415196.28]], dtype=float32),\n",
       " 89704: array([[425430.1]], dtype=float32),\n",
       " 89449: array([[256005.6]], dtype=float32),\n",
       " 89040: array([[209113.64]], dtype=float32),\n",
       " 89444: array([[289542.3]], dtype=float32),\n",
       " 89085: array([[331103.47]], dtype=float32),\n",
       " 89034: array([[255054.42]], dtype=float32),\n",
       " 89021: array([[230877.84]], dtype=float32),\n",
       " 89439: array([[458706.2]], dtype=float32),\n",
       " 89411: array([[515886.7]], dtype=float32),\n",
       " 89124: array([[337681.62]], dtype=float32),\n",
       " 89440: array([[99456.234]], dtype=float32),\n",
       " 89413: array([[2040080.5]], dtype=float32),\n",
       " 89155: array([[357517.4]], dtype=float32)}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pred_skylar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{89108: inf,\n",
       " 89121: inf,\n",
       " 89117: 1230066.2112004093,\n",
       " 89052: 849023.4076489556,\n",
       " 89123: 1245902.8314519874,\n",
       " 89031: inf,\n",
       " 89110: inf,\n",
       " 89074: 1235234.9873707858,\n",
       " 89103: inf,\n",
       " 89148: 1564602.609267752,\n",
       " 89147: inf,\n",
       " 89119: inf,\n",
       " 89129: inf,\n",
       " 89122: inf,\n",
       " 89115: inf,\n",
       " 89502: inf,\n",
       " 89014: inf,\n",
       " 89131: 1267790.6867763042,\n",
       " 89509: 861158.6664328449,\n",
       " 89436: inf,\n",
       " 89015: inf,\n",
       " 89128: inf,\n",
       " 89523: 1116613.6573751096,\n",
       " 89104: inf,\n",
       " 89012: 803228.1592199908,\n",
       " 89030: inf,\n",
       " 89431: inf,\n",
       " 89032: inf,\n",
       " 89506: inf,\n",
       " 89102: inf,\n",
       " 89139: inf,\n",
       " 89149: inf,\n",
       " 89178: 1263958.5142729054,\n",
       " 89113: 1231231.4229128067,\n",
       " 89521: 900697.026561388,\n",
       " 89183: inf,\n",
       " 89135: 1030794.2057844118,\n",
       " 89107: inf,\n",
       " 89511: 1099164.863237715,\n",
       " 89002: 874431.9452965424,\n",
       " 89130: inf,\n",
       " 89134: 2055561.3968449957,\n",
       " 89503: inf,\n",
       " 89081: inf,\n",
       " 89141: 1109612.7007757071,\n",
       " 89011: 1274199.9438122606,\n",
       " 89142: inf,\n",
       " 89434: inf,\n",
       " 89512: inf,\n",
       " 89145: inf,\n",
       " 89084: 1126084.1104500473,\n",
       " 89701: 717319.0582727147,\n",
       " 89801: inf,\n",
       " 89120: inf,\n",
       " 89044: 681496.2415705951,\n",
       " 89156: inf,\n",
       " 89048: inf,\n",
       " 89118: inf,\n",
       " 89706: 550594.133251606,\n",
       " 89408: inf,\n",
       " 89166: 853139.2038593336,\n",
       " 89144: 935709.0020983517,\n",
       " 89146: inf,\n",
       " 89027: 626998.6678725225,\n",
       " 89403: inf,\n",
       " 89433: inf,\n",
       " 89005: 737327.2088295636,\n",
       " 89138: 1161405.077756935,\n",
       " 89460: 576897.2225772525,\n",
       " 89423: 704213.0442148822,\n",
       " 89410: 653551.9546868667,\n",
       " 89815: inf,\n",
       " 89029: inf,\n",
       " 89109: inf,\n",
       " 89508: inf,\n",
       " 89703: 990975.8898129726,\n",
       " 89441: 1029440.0940955299,\n",
       " 89060: inf,\n",
       " 89143: 1531513.2771627223,\n",
       " 89519: 1287210.3579656263,\n",
       " 89447: inf,\n",
       " 89179: 812589.4835636253,\n",
       " 89429: inf,\n",
       " 89061: inf,\n",
       " 89451: 1162558.7394056001,\n",
       " 89501: 694543.6927779577,\n",
       " 89705: 666047.7668984331,\n",
       " 89510: 902305.1468193594,\n",
       " 89086: inf,\n",
       " 89448: inf,\n",
       " 89704: 805071.7431239514,\n",
       " 89449: inf,\n",
       " 89040: 1094984.957853918,\n",
       " 89444: 637412.1169201456,\n",
       " 89085: 1484817.5105409324,\n",
       " 89034: 959847.0209837235,\n",
       " 89021: 1018985.4868376467,\n",
       " 89439: 829505.0863944809,\n",
       " 89411: 1195062.9838804866,\n",
       " 89124: 923523.0060611685,\n",
       " 89440: inf,\n",
       " 89413: 2654001.6272303164,\n",
       " 89155: 1066815.2493125491}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_mape_skylar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{95804: [0.5159191513365095, 75412.6796875, 'RNN'],\n",
       " 95817: [0.4806917270953116, 82854.0859375, 'RNN'],\n",
       " 95813: [0.013083894437494591, 341304.875, 'RNN'],\n",
       " 95785: [0.01378739311178748, 422513.25, 'RNN'],\n",
       " 95819: [0.015094171486925623, 301021.0625, 'RNN'],\n",
       " 95770: [0.44194695282497615, 105605.78125, 'RNN'],\n",
       " 95806: [0.5182788597996814, 70026.3359375, 'RNN'],\n",
       " 95790: [0.007388430655138559, 312865.75, 'RNN'],\n",
       " 95799: [0.41473107013635707, 110862.84375, 'RNN'],\n",
       " 95844: [0.014626261526478987, 297665.53125, 'RNN'],\n",
       " 95843: [0.40972854999398295, 126288.140625, 'RNN'],\n",
       " 95815: [0.4957810488444431, 84689.921875, 'RNN'],\n",
       " 95825: [0.38387902090114595, 142043.46875, 'RNN'],\n",
       " 95818: [0.4386295986242681, 94474.0703125, 'RNN'],\n",
       " 95811: [0.46491245082909255, 74676.609375, 'RNN'],\n",
       " 95931: [0.44696939316482326, 114605.4375, 'RNN'],\n",
       " 95753: [0.3965936438523714, 141604.265625, 'RNN'],\n",
       " 95827: [0.007951250432951461, 335392.75, 'RNN'],\n",
       " 95937: [0.010482319800557482, 444971.84375, 'RNN'],\n",
       " 95914: [0.4358761083009109, 168000.984375, 'RNN'],\n",
       " 95754: [0.43916482062443524, 106289.3515625, 'RNN'],\n",
       " 95824: [0.3774964689018774, 143480.625, 'RNN'],\n",
       " 95945: [0.009420181348985288, 393342.9375, 'RNN'],\n",
       " 95800: [0.4718195009651555, 84472.21875, 'RNN'],\n",
       " 95751: [0.011770262489814314, 343317.875, 'RNN'],\n",
       " 95769: [0.2418764663897689, 81770.640625, 'RNN'],\n",
       " 95909: [0.541548922420154, 80201.6171875, 'RNN'],\n",
       " 95771: [0.4449168818684283, 96661.84375, 'RNN'],\n",
       " 95935: [0.468579774527048, 115184.875, 'RNN'],\n",
       " 95798: [0.49470465844168654, 83719.421875, 'RNN'],\n",
       " 95835: [0.3906188879587259, 140822.90625, 'RNN'],\n",
       " 95845: [0.3527866797556514, 166657.90625, 'RNN'],\n",
       " 95865: [0.0060464769748770665, 294301.125, 'RNN'],\n",
       " 95809: [0.009771453341725073, 304432.59375, 'RNN'],\n",
       " 95944: [0.010799909267872594, 409787.03125, 'RNN'],\n",
       " 399671: [0.4116425192150598, 124373.84375, 'RNN'],\n",
       " 95831: [0.013558921555743184, 427525.21875, 'RNN'],\n",
       " 95803: [0.517279830987193, 71889.5390625, 'RNN'],\n",
       " 95939: [0.017354669112205005, 643723.25, 'RNN'],\n",
       " 399665: [0.014149467428044914, 302807.65625, 'RNN'],\n",
       " 95826: [0.3977985782396422, 129545.7578125, 'RNN'],\n",
       " 95830: [0.01499383163019032, 316861.25, 'RNN'],\n",
       " 95932: [0.41069818420484944, 147422.921875, 'RNN'],\n",
       " 95792: [0.41483758989555536, 114382.765625, 'RNN'],\n",
       " 95837: [0.02081529672912315, 327126.6875, 'RNN'],\n",
       " 95750: [0.015961433084506354, 267443.625, 'RNN'],\n",
       " 95838: [0.4022623108188854, 102916.2578125, 'RNN'],\n",
       " 95912: [0.4455681412177846, 132910.53125, 'RNN'],\n",
       " 95940: [0.28050054567188604, 115600.046875, 'RNN'],\n",
       " 95841: [0.3897281475301448, 120660.484375, 'RNN'],\n",
       " 95793: [0.009597087395329643, 296469.84375, 'RNN'],\n",
       " 95952: [0.009495231708585497, 266254.875, 'RNN'],\n",
       " 95963: [0.10176629964979514, 209590.125, 'RNN'],\n",
       " 95816: [0.4315038321521024, 116423.96875, 'RNN'],\n",
       " 95779: [0.011447647262227374, 356830.5, 'RNN'],\n",
       " 95852: [0.46973642009308786, 84205.7265625, 'RNN'],\n",
       " 95783: [0.3317365500787004, 106907.9921875, 'RNN'],\n",
       " 95814: [0.38382792279041755, 134058.5, 'RNN'],\n",
       " 95957: [0.012981924290519522, 267417.28125, 'RNN'],\n",
       " 95888: [0.19147234315544517, 143140.03125, 'RNN'],\n",
       " 95861: [0.014256302542522825, 286464.03125, 'RNN'],\n",
       " 95840: [0.01030218372133625, 347827.71875, 'RNN'],\n",
       " 95842: [0.36788399256165016, 167712.90625, 'RNN'],\n",
       " 95766: [0.00854011861571219, 239586.078125, 'RNN'],\n",
       " 95883: [0.1520333966450159, 175152.21875, 'RNN'],\n",
       " 95911: [0.46489085262274765, 113575.8046875, 'RNN'],\n",
       " 95744: [0.00984138429584488, 318896.78125, 'RNN'],\n",
       " 95834: [0.011436811817719347, 434530.78125, 'RNN'],\n",
       " 95928: [0.021741427708519265, 318965.125, 'RNN'],\n",
       " 95901: [0.015790881595813135, 398780.40625, 'RNN'],\n",
       " 95890: [0.01847956218986767, 367463.875, 'RNN'],\n",
       " 95966: [0.07944586290889326, 216266.828125, 'RNN'],\n",
       " 95768: [0.16659501239545244, 133545.65625, 'RNN'],\n",
       " 95805: [0.3309421671862406, 162035.734375, 'RNN'],\n",
       " 399673: [0.43961954063396863, 131789.515625, 'RNN'],\n",
       " 95954: [0.010090192830097204, 399326.6875, 'RNN'],\n",
       " 399672: [0.02068608427753164, 418189.25, 'RNN'],\n",
       " 95787: [0.32292317749437116, 81608.4609375, 'RNN'],\n",
       " 95839: [0.018493146927664354, 276744.0, 'RNN'],\n",
       " 399674: [0.013792924062759431, 540005.875, 'RNN'],\n",
       " 95922: [0.12094748056449896, 114301.2109375, 'RNN'],\n",
       " 95866: [0.008060034743548408, 306858.6875, 'RNN'],\n",
       " 95907: [0.16924311825053417, 108803.5859375, 'RNN'],\n",
       " 95788: [0.41459605477245626, 98749.265625, 'RNN'],\n",
       " 95926: [0.022131163635094733, 916679.0625, 'RNN'],\n",
       " 95930: [0.03876847845117795, 338159.3125, 'RNN'],\n",
       " 95956: [0.02446487880320502, 315065.6875, 'RNN'],\n",
       " 95938: [0.02889695214142176, 428775.71875, 'RNN'],\n",
       " 95795: [0.3838596467150377, 137223.53125, 'RNN'],\n",
       " 95923: [0.32736431115352693, 409056.8125, 'RNN'],\n",
       " 95955: [0.02124960986573491, 423724.21875, 'RNN'],\n",
       " 95924: [0.23029241142242796, 260255.78125, 'RNN'],\n",
       " 95775: [0.013657195455473416, 207778.890625, 'RNN'],\n",
       " 95919: [0.01986937571165866, 285412.59375, 'RNN'],\n",
       " 95794: [0.009203299905793253, 330588.71875, 'RNN'],\n",
       " 399666: [0.013165532119944867, 323501.25, 'RNN'],\n",
       " 95760: [0.012689090960715351, 309324.90625, 'RNN'],\n",
       " 95916: [0.013263515868673158, 456363.8125, 'RNN'],\n",
       " 95891: [0.01297970825903085, 651686.3125, 'RNN'],\n",
       " 95820: [0.026793016022013318, 333740.6875, 'RNN'],\n",
       " 95917: [0.4154968076834546, 102394.203125, 'RNN'],\n",
       " 95893: [0.026021759253071913, 2057796.75, 'RNN'],\n",
       " 95851: [0.010471334813466493, 355289.84375, 'RNN']}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{89005: [0.022498635550434295, 315770.5862729762, 'FBP_scale=0.5'],\n",
       " 89011: [0.04278110779000683, 268191.1951533978, 'FBP_scale=0.5'],\n",
       " 89012: [0.03408125028112888, 346791.0020159719, 'FBP_scale=0.5'],\n",
       " 89014: [0.02982231867033438, 290775.76827606466, 'FBP_scale=0.5'],\n",
       " 89015: [0.020673062452361255, 255266.9013634051, 'FBP_scale=0.5'],\n",
       " 89021: [0.021685927237354685, 310127.47890419036, 'FBP_scale=0.5'],\n",
       " 89027: [0.023356007127800827, 240189.23249319484, 'FBP_scale=0.5'],\n",
       " 89029: [0.0280397544224704, 182509.39476487262, 'FBP_scale=0.5'],\n",
       " 89030: [0.03602921269009029, 153247.78988254067, 'FBP_scale=0.5'],\n",
       " 89031: [0.03434730300348244, 246300.94475486007, 'FBP_scale=0.5'],\n",
       " 89032: [0.045912141431496335, 234114.60925612983, 'FBP_scale=0.5'],\n",
       " 89040: [0.052278179111354565, 208033.3162393082, 'FBP_scale=0.5'],\n",
       " 89044: [0.026860553573338542, 357295.44824271597, 'FBP_scale=0.5'],\n",
       " 89048: [0.04679328785772354, 200705.8126463924, 'FBP_scale=0.5'],\n",
       " 89052: [0.025238772814148578, 421068.9254136304, 'FBP_scale=0.5'],\n",
       " 89060: [0.05044588420751083, 154590.7171150974, 'FBP_scale=0.5'],\n",
       " 89061: [0.05167060208025199, 219499.46418643033, 'FBP_scale=0.5'],\n",
       " 89074: [0.031614012629685995, 316779.48254232004, 'FBP_scale=0.5'],\n",
       " 89081: [0.04179429582249042, 255788.3319140967, 'FBP_scale=0.5'],\n",
       " 89084: [0.02960237024440737, 299204.5957428111, 'FBP_scale=0.5'],\n",
       " 89085: [0.03774310893184554, 328288.87104909273, 'FBP_scale=0.5'],\n",
       " 89086: [0.022623376705750278, 279634.99856473086, 'FBP_scale=0.5'],\n",
       " 89102: [0.040796909553926605, 220765.39635029866, 'FBP_scale=0.5'],\n",
       " 89103: [0.023349681918025767, 254729.35021436046, 'FBP_scale=0.5'],\n",
       " 89104: [0.03320603155912621, 212632.1144190938, 'FBP_scale=0.5'],\n",
       " 89107: [0.05935506051263894, 198917.1941191062, 'FBP_scale=0.5'],\n",
       " 89108: [0.053552150532358844, 212876.99612153973, 'FBP_scale=0.5'],\n",
       " 89109: [0.07727488712982523, 337940.7916967204, 'FBP_scale=0.5'],\n",
       " 89110: [0.04725188507969076, 200482.01372467002, 'FBP_scale=0.5'],\n",
       " 89113: [0.043625364435827534, 309582.4499731394, 'FBP_scale=0.5'],\n",
       " 89115: [0.04808106594745356, 180066.93932910392, 'FBP_scale=0.5'],\n",
       " 89117: [0.029426083389244503, 344299.1564222108, 'FBP_scale=0.5'],\n",
       " 89118: [0.030271458421119, 271179.49889021384, 'FBP_scale=0.5'],\n",
       " 89119: [0.026782273794856256, 241955.26361875163, 'FBP_scale=0.5'],\n",
       " 89120: [0.04554779842216769, 267252.62681162066, 'FBP_scale=0.5'],\n",
       " 89121: [0.03251929889460807, 212357.25585671476, 'FBP_scale=0.5'],\n",
       " 89122: [0.061933752527614086, 214762.9394826224, 'FBP_scale=0.5'],\n",
       " 89123: [0.04825771073766382, 306773.87555588817, 'FBP_scale=0.5'],\n",
       " 89124: [0.0937953210600353, 334290.0833986482, 'FBP_scale=0.5'],\n",
       " 89128: [0.029905361981916723, 280846.724090128, 'FBP_scale=0.5'],\n",
       " 89129: [0.026517157639345555, 286423.08475005795, 'FBP_scale=0.5'],\n",
       " 89130: [0.025429646834373133, 276657.8056960072, 'FBP_scale=0.5'],\n",
       " 89131: [0.02483231367385673, 339574.2471591072, 'FBP_scale=0.5'],\n",
       " 89134: [0.038341795950534115, 321577.72028144787, 'FBP_scale=0.5'],\n",
       " 89135: [0.024515869611043625, 425834.5057004451, 'FBP_scale=0.5'],\n",
       " 89138: [0.032858074757566254, 442950.4158662549, 'FBP_scale=0.5'],\n",
       " 89139: [0.029566433244114444, 289032.9615452033, 'FBP_scale=0.5'],\n",
       " 89141: [0.034115387972923454, 321001.58057856036, 'FBP_scale=0.5'],\n",
       " 89142: [0.04194677994191061, 213715.51190621968, 'FBP_scale=0.5'],\n",
       " 89143: [0.057232444162401676, 283231.0973787324, 'FBP_scale=0.5'],\n",
       " 89144: [0.024782252424292387, 351505.7533436969, 'FBP_scale=0.5'],\n",
       " 89145: [0.024787502553774866, 246658.82776447045, 'FBP_scale=0.5'],\n",
       " 89146: [0.042324631114278415, 320103.6004428028, 'FBP_scale=0.5'],\n",
       " 89147: [0.014973894613094363, 272818.3010017803, 'FBP_scale=0.5'],\n",
       " 89148: [0.046392384187789994, 305625.52330460254, 'FBP_scale=0.5'],\n",
       " 89149: [0.03378183762916275, 308219.1848478401, 'FBP_scale=0.5'],\n",
       " 89155: [0.01926188734707299, 362032.9962344175, 'FBP_scale=0.5'],\n",
       " 89156: [0.045454014922245524, 207605.68758203776, 'FBP_scale=0.5'],\n",
       " 89166: [0.031835900515247546, 291128.0588082098, 'FBP_scale=0.5'],\n",
       " 89178: [0.03695304263395325, 295479.7674959657, 'FBP_scale=0.5'],\n",
       " 89179: [0.017686717512439786, 305660.96584050445, 'FBP_scale=0.5'],\n",
       " 89403: [0.022850621255721595, 275411.9877160178, 'FBP_scale=0.5'],\n",
       " 89408: [0.0364164969395669, 244086.15490036257, 'FBP_scale=0.5'],\n",
       " 89410: [0.054124594959493774, 373451.8138814837, 'FBP_scale=0.5'],\n",
       " 89411: [0.03320864612697135, 659160.513359919, 'FBP_scale=0.5'],\n",
       " 89413: [0.06451534890433677, 2132853.3588797115, 'FBP_scale=0.5'],\n",
       " 89423: [0.039261552321618015, 403664.2838719558, 'FBP_scale=0.5'],\n",
       " 89429: [0.03392726123407389, 191969.94510094702, 'FBP_scale=0.5'],\n",
       " 89431: [0.040307572198368696, 251931.37670638357, 'FBP_scale=0.5'],\n",
       " 89433: [0.023971527220539444, 274932.279549524, 'FBP_scale=0.5'],\n",
       " 89434: [0.042867507086386175, 303049.82422532566, 'FBP_scale=0.5'],\n",
       " 89436: [0.03748659863446806, 373725.1710211443, 'FBP_scale=0.5'],\n",
       " 89439: [0.05640902283540983, 454322.1839203852, 'FBP_scale=0.5'],\n",
       " 89440: [0.045543683310369336, 218486.83054223805, 'FBP_scale=0.5'],\n",
       " 89444: [0.051584223018417606, 276026.96248109627, 'FBP_scale=0.5'],\n",
       " 89447: [0.03528869275268434, 168143.63871695238, 'FBP_scale=0.5'],\n",
       " 89448: [0.05224819563754419, 736841.7751282966, 'FBP_scale=0.5'],\n",
       " 89449: [0.06992530950915793, 381426.39967530925, 'FBP_scale=0.5'],\n",
       " 89451: [0.04582917758575527, 946704.3623023875, 'FBP_scale=0.5'],\n",
       " 89460: [0.058460290765724114, 324424.9757866733, 'FBP_scale=0.5'],\n",
       " 89501: [0.053482173681949546, 351052.4832722586, 'FBP_scale=0.5'],\n",
       " 89502: [0.020103747985949223, 284076.71654429485, 'FBP_scale=0.5'],\n",
       " 89503: [0.04092247748861434, 314054.913904733, 'FBP_scale=0.5'],\n",
       " 89506: [0.030989881767913532, 280461.8992750567, 'FBP_scale=0.5'],\n",
       " 89509: [0.028004390547727243, 452250.1469241155, 'FBP_scale=0.5'],\n",
       " 89510: [0.03678098305692088, 431762.4565774328, 'FBP_scale=0.5'],\n",
       " 89511: [0.027204512517963773, 666469.6560600224, 'FBP_scale=0.5'],\n",
       " 89512: [0.031919613786859626, 233341.54418264233, 'FBP_scale=0.5'],\n",
       " 89521: [0.02429083202181481, 417320.8573649556, 'FBP_scale=0.5'],\n",
       " 89523: [0.021539231083709576, 397864.98620824347, 'FBP_scale=0.5'],\n",
       " 89701: [0.02731434444478958, 270163.8202057961, 'FBP_scale=0.5'],\n",
       " 89703: [0.024193958362711956, 404803.0094934392, 'FBP_scale=0.5'],\n",
       " 89704: [0.08357990835093837, 423194.0997806724, 'FBP_scale=0.5'],\n",
       " 89705: [0.046550476695143735, 319915.2612081292, 'FBP_scale=0.5'],\n",
       " 89706: [0.027981614194214768, 268942.85774527164, 'FBP_scale=0.5'],\n",
       " 89801: [0.033157206621436924, 241310.69109952607, 'FBP_scale=0.5'],\n",
       " 89815: [0.03217108263103794, 238194.8008724951, 'FBP_scale=0.5'],\n",
       " 89002: [0.03444270744775924, 307046.4987918806, 'FBP_scale=0.5'],\n",
       " 89034: [0.036062482078580683, 332575.9172435973, 'FBP_scale=0.5'],\n",
       " 89183: [0.04578603188943014, 277916.42593488516, 'FBP_scale=0.5'],\n",
       " 89441: [0.0254675908325991, 422765.85601377813, 'FBP_scale=0.5'],\n",
       " 89508: [0.017187629795996175, 301861.70888802106, 'FBP_scale=0.5'],\n",
       " 89519: [0.02247708340155557, 540689.2117415073, 'FBP_scale=0.5']}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fbp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e81d9768a2937af9514d7fa33aa30feb69a40df5b58c34cfa60b871c6c10885"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
